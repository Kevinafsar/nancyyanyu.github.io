<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Nancy&#39;s Notes</title>
  
  <subtitle>Code changes world!</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://nancyyanyu.github.io/"/>
  <updated>2019-08-06T22:41:34.586Z</updated>
  <id>https://nancyyanyu.github.io/</id>
  
  <author>
    <name>Nancy Yan</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Realtime Financial Market Data Visualization and Analysis</title>
    <link href="https://nancyyanyu.github.io/posts/109fc1d1/"/>
    <id>https://nancyyanyu.github.io/posts/109fc1d1/</id>
    <published>2019-08-05T22:36:32.000Z</published>
    <updated>2019-08-06T22:41:34.586Z</updated>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>In this project, I developed a financial data processing and visualization platform using Apache Kafka, Apache Cassandra, and Bokeh. I used Kafka for realtime stock price and market news streaming, Cassandra for historical and realtime stock data warehousing, and Bokeh for visualization on web browsers. I also wrote a web crawler to scrape companys' financial statements and basic information from Yahoo Finance, and played with various economy data APIs.</p><p><em>Please check the <a href="https://github.com/nancyyanyu/kafka_stock" target="_blank" rel="noopener">GitHub repo</a> of this project, and <strong>most importantly, please check this platform's website and play with each plot</strong> <span class="math inline">\(\rightarrow\)</span> <a href="http://magiconch.me/" target="_blank" rel="noopener">magiconch.me</a></em></p><a id="more"></a><p> </p><h1 id="architecture">Architecture</h1><p>There are currently 3 tabs in the webpage:</p><ul><li><strong><em>Stock: Streaming &amp; Fundamental</em></strong><ul><li>Visualize a single stock's candlestick plot, basic company &amp; financial information;</li><li>Plot realtime S&amp;P500 price during trading hours (fake date during non-trading hours)</li></ul></li><li><strong><em>Stock: Comparison</em></strong><ul><li>Plot 2 user-selected stocks' price, and print their statstical summay and correlation</li><li>Visualize the 5,10,30-day moving average of adjusted close price</li></ul></li><li><strong><em>Economy</em></strong><ul><li>Plot a geomap to visualize various economy data by state</li><li>Plot 4 economy indicators nationwide for comparison</li><li>Present the most recent market news</li></ul></li></ul><p> </p><p>Please see the screenshots of each tab below:</p><p><strong>Tab1:</strong></p><p><img src="./tab1.png" width="400"></p><p><strong>Tab2:</strong></p><p><img src="./tab2.png" width="400"></p><p><strong>Tab3:</strong></p><p><img src="./tab3.png" width="400"></p><p>Here is the architecture of the platform, and I will go through the process of :</p><p><img src="./kafka_stock.png" width="700"></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;In this project, I developed a financial data processing and visualization platform using Apache Kafka, Apache Cassandra, and Bokeh. I used Kafka for realtime stock price and market news streaming, Cassandra for historical and realtime stock data warehousing, and Bokeh for visualization on web browsers. I also wrote a web crawler to scrape companys&#39; financial statements and basic information from Yahoo Finance, and played with various economy data APIs.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Please check the &lt;a href=&quot;https://github.com/nancyyanyu/kafka_stock&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub repo&lt;/a&gt; of this project, and &lt;strong&gt;most importantly, please check this platform&#39;s website and play with each plot&lt;/strong&gt; &lt;span class=&quot;math inline&quot;&gt;\(\rightarrow\)&lt;/span&gt; &lt;a href=&quot;http://magiconch.me/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;magiconch.me&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Project" scheme="https://nancyyanyu.github.io/categories/Project/"/>
    
    
      <category term="Bokeh" scheme="https://nancyyanyu.github.io/tags/Bokeh/"/>
    
      <category term="Kafka" scheme="https://nancyyanyu.github.io/tags/Kafka/"/>
    
      <category term="Cassandra" scheme="https://nancyyanyu.github.io/tags/Cassandra/"/>
    
  </entry>
  
  <entry>
    <title>July 2019 | 七月日志</title>
    <link href="https://nancyyanyu.github.io/posts/445d7012/"/>
    <id>https://nancyyanyu.github.io/posts/445d7012/</id>
    <published>2019-08-02T16:38:59.000Z</published>
    <updated>2019-08-02T19:03:41.286Z</updated>
    
    <content type="html"><![CDATA[<p> </p><a id="more"></a><p> </p><center><img src="./allen2.jpg" width="300"></center><p> </p><center><b>❤ 7月手帳 ❤</b></center><table><colgroup><col style="width: 6%"><col style="width: 81%"><col style="width: 12%"></colgroup><thead><tr class="header"><th style="text-align: left;">Date</th><th>Study</th><th>Workout</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">07/01</td><td>Job Hunting</td><td>/</td></tr><tr class="even"><td style="text-align: left;">07/02</td><td>Job Hunting</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">07/03</td><td>Job Hunting</td><td>/</td></tr><tr class="even"><td style="text-align: left;">07/04</td><td>Learn pyspark streaming</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">07/05</td><td>Learn pyspark streaming</td><td>/</td></tr><tr class="even"><td style="text-align: left;">07/06</td><td>(Travel in Portland~~)</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">07/07</td><td>(Travel in Portland~~)</td><td>Hiking</td></tr><tr class="even"><td style="text-align: left;">07/08</td><td>Finish udemy's course <a href="https://www.udemy.com/apache-spark-streaming-with-python-and-pyspark/" target="_blank" rel="noopener">Apache Spark Streaming with Python and PySpark</a></td><td>/</td></tr><tr class="odd"><td style="text-align: left;">07/09</td><td>Write pyspark project code and blog;</td><td>/</td></tr><tr class="even"><td style="text-align: left;">07/10</td><td>Write twitter pyspark streaming project</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">07/11</td><td>Coffee chat and write email all day</td><td>/</td></tr><tr class="even"><td style="text-align: left;">07/12</td><td>Work on new spotify project</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">07/13</td><td>(/<sub>/</sub>/<sub>nothing</sub>~~)</td><td>/</td></tr><tr class="even"><td style="text-align: left;">07/14</td><td>Coding for project; learn AWS DynamoDB, S3</td><td>Ping Pong</td></tr><tr class="odd"><td style="text-align: left;">07/15</td><td>Work on project; learn Apache Airflow, review Linear Regression</td><td>/</td></tr><tr class="even"><td style="text-align: left;">07/16</td><td>Write project report; learn Apache Airflow</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">07/17</td><td>Write project report</td><td>/</td></tr><tr class="even"><td style="text-align: left;">07/18</td><td>Learn Beautifulsoup, crawling and start new project</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">07/19</td><td>Write crawling code</td><td>Ping Pong</td></tr><tr class="even"><td style="text-align: left;">07/20</td><td>Crawling all day, deploy to EC2</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">07/21</td><td>Write feature extraction code; rewrite resume</td><td>/</td></tr><tr class="even"><td style="text-align: left;">07/22</td><td>Revise resume</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">07/23</td><td>Revise resume and review probability theory</td><td>/</td></tr><tr class="even"><td style="text-align: left;">07/24</td><td>Sending applications all day</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">07/25</td><td>Learn Kafka and Spark, start a new project (a full stack data engineering and analysis project with realtime data)</td><td>Ping Pong</td></tr><tr class="even"><td style="text-align: left;">07/26</td><td>Meeting, Research on new project</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">07/27</td><td>Learn Kafka, play with multiple realtime data API</td><td>/</td></tr><tr class="even"><td style="text-align: left;">07/28</td><td>Learn Cassandra and CQL</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">07/29</td><td>Study bokeh, write streaming visualization module</td><td>/</td></tr><tr class="even"><td style="text-align: left;">07/30</td><td>Crawl yahoo finance for realtime data</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">07/31</td><td>Study bokeh, write visualization module for project</td><td>Ping Pong</td></tr></tbody></table><p> </p><p> </p><center><b>❤ Plan ❤</b></center><table><colgroup><col style="width: 12%"><col style="width: 75%"><col style="width: 12%"></colgroup><thead><tr class="header"><th style="text-align: center;">Categories</th><th style="text-align: center;">Content</th><th style="text-align: center;">Progress</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">Project</td><td style="text-align: center;"><a href="https://nancyyanyu.github.io/posts/9fb5a802/">Realtime Twitter Data Analysis using Spark Streaming</a></td><td style="text-align: center;">■■■■■■■■■■</td></tr><tr class="even"><td style="text-align: center;">Project</td><td style="text-align: center;"><a href="https://nancyyanyu.github.io/posts/63adf3bb/">Data Analysis of K-POP: Playing with Spotify API</a></td><td style="text-align: center;">■■■■■■■■■■</td></tr><tr class="odd"><td style="text-align: center;">Project</td><td style="text-align: center;">Realtime financial market data visualization and analysis</td><td style="text-align: center;">■■■■■■□□□□</td></tr><tr class="even"><td style="text-align: center;">Project</td><td style="text-align: center;">Crawling project</td><td style="text-align: center;">■■■■■■■■■■</td></tr><tr class="odd"><td style="text-align: center;">Big Data</td><td style="text-align: center;">Udemy: <a href="https://www.udemy.com/apache-spark-streaming-with-python-and-pyspark/" target="_blank" rel="noopener">Apache Spark Streaming with Python and PySpark</a></td><td style="text-align: center;">■■■■■■■■■■</td></tr></tbody></table><p> </p><p> </p><center><b>❤ Some Spark Resources that I like to share~ ❤</b></center><ul><li>Youtube: <a href="https://www.youtube.com/watch?v=7ooZ4S7Ay6Y" target="_blank" rel="noopener">Advanced Apache Spark Training - Sameer Farooqui (Databricks)</a></li><li>Blog &amp; Github: <a href="%5Bhttps://github.com/NFLX-WIBD/WIBD-Workshops-2018/tree/master/Data%20Engineering%5D(https://github.com/NFLX-WIBD/WIBD-Workshops-2018/tree/master/Data%20Engineering)">Data Engineering Workshop 2018 from Netflix</a> &amp; <a href="https://medium.com/hasbrain/a-typical-data-engineering-project-sharing-from-netflix-data-engineering-team-cc27878fce55" target="_blank" rel="noopener">A Typical Data Engineering Project — Sharing From Netflix Data Engineering Team</a></li></ul><p> </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; &lt;/p&gt;
    
    </summary>
    
      <category term="Journal" scheme="https://nancyyanyu.github.io/categories/Journal/"/>
    
    
      <category term="Journal" scheme="https://nancyyanyu.github.io/tags/Journal/"/>
    
  </entry>
  
  <entry>
    <title>Probability Theory - Random Variables</title>
    <link href="https://nancyyanyu.github.io/posts/ea98dd04/"/>
    <id>https://nancyyanyu.github.io/posts/ea98dd04/</id>
    <published>2019-07-24T21:25:08.000Z</published>
    <updated>2019-08-02T19:23:35.862Z</updated>
    
    <content type="html"><![CDATA[<h1 id="random-variables">Random Variables</h1><p><strong>Discrete random variables</strong>:</p><ul><li><p><span class="math inline">\(X ∼ Bernoulli(p)\)</span> (where 0 ≤ p ≤ 1): one if a coin with heads probability <span class="math inline">\(p\)</span> comes up heads, zero otherwise</p><ul><li><p>PMF: <span class="math inline">\(p(x)=\begin{cases}p &amp; x=1\\1-p &amp; x=0\end{cases}\)</span></p></li><li>Mean: <span class="math inline">\(p\)</span></li><li><p>Variance: <span class="math inline">\(p(1-p)\)</span></p></li></ul></li><li><p>$X ∼ Binomial(n, p) $ (where 0 ≤ p ≤ 1): the number of heads in <span class="math inline">\(n\)</span> independent flips of a coin with heads probability <span class="math inline">\(p\)</span>.</p><ul><li><p>PMF: <span class="math inline">\(p(x)=\left(\begin{array}{c}n\\ x\end{array}\right) p^x(1-p)^{n-x}\)</span></p></li><li>Mean: <span class="math inline">\(np\)</span></li><li><p>Variance: <span class="math inline">\(np(1-p)\)</span></p></li></ul></li><li><p>$X ∼ Geometric(p) $(where p &gt; 0): the number of flips of a coin with heads probability <span class="math inline">\(p\)</span> until the first heads.</p><ul><li><p>PMF: <span class="math inline">\(p(x)=p(1-p)^{x-1}\)</span></p></li><li>Mean: <span class="math inline">\(\frac{1}{p}\)</span></li><li><p>Variance: <span class="math inline">\(\frac{1-p}{p^2}\)</span></p></li></ul></li><li><span class="math inline">\(X ∼ Poisson(λ)\)</span> (where λ &gt; 0): a probability distribution over the nonnegative integers used for modeling the frequency of rare events.<ul><li>PMF: <span class="math inline">\(p(x)=e^{-\lambda}\frac{\lambda^x}{x!}\)</span></li><li>Mean: <span class="math inline">\(\lambda\)</span></li><li>Variance: <span class="math display">\[\lambda\]</span></li><li><strong>Properties</strong>: Poisson random variable may be used to approximate a <strong><em>binomial</em></strong> random variable when the binomial parameter <em>n</em> is large and <em>p</em> is small.</li></ul></li></ul><a id="more"></a><p><strong>Continuous random variables</strong>:</p><ul><li><span class="math inline">\(X ∼ Uniform(a, b)\)</span> (where a &lt; b): equal probability density to every value between a and b on the real line.<ul><li>PDF: <span class="math inline">\(f(x)=\frac{1}{b-a}, a \leq x \leq b\)</span></li><li>CDF: <span class="math inline">\(F(x)=\begin{cases}0 &amp; x \leq a\\ \frac{x-a}{b-a} &amp; a\leq x \leq b \\ 1 &amp; b \leq x \end{cases}\)</span></li><li>Mean: <span class="math inline">\(\frac{a+b}{2}\)</span></li><li>Variance: <span class="math inline">\(\frac{(b-a)^2}{12}\)</span></li></ul></li><li><span class="math inline">\(X ∼ Exponential(λ)\)</span> (where λ &gt; 0): decaying probability density over the nonnegative reals.<ul><li>PDF:<span class="math inline">\(f(x)=\begin{cases}\lambda e^{-\lambda x} &amp; x \geq 0\\0 &amp; otw. \end{cases}\)</span></li><li>CDF: <span class="math inline">\(F(x)=\begin{cases}1- e^{-\lambda x} &amp; x \geq 0\\0 &amp; otw. \end{cases}\)</span></li><li>Mean: <span class="math inline">\(\frac{1}{\lambda}\)</span></li><li>Variance: <span class="math inline">\(\frac{1}{\lambda^2}\)</span></li></ul></li><li><span class="math inline">\(X ∼ Normal(\mu, \sigma^2)\)</span> : also known as the Gaussian distribution<ul><li>PDF: <span class="math inline">\(f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\)</span></li><li>CDF: $F(x)=_{0}<sup>{z}e</sup>{-z^2/2}dx $<br></li><li>Mean: <span class="math inline">\(\mu\)</span></li><li>Variance: <span class="math inline">\(\sigma^2\)</span></li></ul></li></ul><!--more--><p><img src="./1.png" width="600"></p><p><strong>Conditional distributions</strong> <span class="math display">\[p_{Y|X}(y|x)=\frac{p_{XY}(x,y)}{p_X(x)} \\f_{Y|X}(y|x)=\frac{f_{XY}(x,y)}{f_X(x)}\]</span></p><p><strong>Bayes’s rule</strong> <span class="math display">\[\begin{align} P_{Y|X}(y|x)&amp;=\frac{p_{XY}(x,y)}{p_X(x)} \\&amp;=\frac{P_{X|Y}(x|y)P_Y(y)}{P_X(x)} \\&amp;=\frac{P_{X|Y}(x|y)P_Y(y)}{\sum_{y^{&#39;}\in V al(Y)} P_{X|Y}(x|y^{&#39;})P_Y(y^{&#39;})}\end{align}\]</span></p><p><span class="math display">\[\begin{align} f_{Y|X}(y|x)&amp;=\frac{f_{XY}(x,y)}{f_X(x)} \\&amp;=\frac{f_{X|Y}(x|y)f_Y(y)}{\int_{-\infty}^{\infty}f_{X|Y}(x|y^{&#39;})f_Y(y^{&#39;}) dy^{&#39;}}\end{align}\]</span></p><p><strong>Independence</strong></p><h1 id="problems">Problems:</h1><ol type="1"><li>Let <span class="math inline">\(X\)</span> be uniformly distributed over <span class="math inline">\((0, 1)\)</span>. Calculate <span class="math inline">\(E[X^3]\)</span></li></ol><p><span class="math display">\[F_Y(a)=P(Y\leq a)=P(X^3&lt;a)=P(X\leq a^{1/3})=a^{1/3} \\\]</span></p><p>where the last equality follows since <em>X</em> is uniformly distributed over (0, 1). By differentiating <span class="math inline">\(F_Y(a)\)</span>, we obtain the density of <em>Y</em>, namely, <span class="math display">\[f_Y(a)=\frac{1}{3}a^{-2/3} , 0\leq a \leq 1\]</span> Hence, <span class="math display">\[E[X^3]=E(Y)=\int_{-\infty}^\infty af_Y(a)da= \int_{0}^1 \frac{1}{3}a^{1/3}da = \frac{1}{3}\frac{3}{4} a^{4/3} |^1_0=\frac{1}{4}\]</span></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;random-variables&quot;&gt;Random Variables&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Discrete random variables&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&quot;math inline&quot;&gt;\(X ∼ Bernoulli(p)\)&lt;/span&gt; (where 0 ≤ p ≤ 1): one if a coin with heads probability &lt;span class=&quot;math inline&quot;&gt;\(p\)&lt;/span&gt; comes up heads, zero otherwise&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;PMF: &lt;span class=&quot;math inline&quot;&gt;\(p(x)=\begin{cases}p &amp;amp; x=1\\1-p &amp;amp; x=0\end{cases}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Mean: &lt;span class=&quot;math inline&quot;&gt;\(p\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Variance: &lt;span class=&quot;math inline&quot;&gt;\(p(1-p)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;$X ∼ Binomial(n, p) $ (where 0 ≤ p ≤ 1): the number of heads in &lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt; independent flips of a coin with heads probability &lt;span class=&quot;math inline&quot;&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;PMF: &lt;span class=&quot;math inline&quot;&gt;\(p(x)=\left(\begin{array}{c}n\\ x\end{array}\right) p^x(1-p)^{n-x}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Mean: &lt;span class=&quot;math inline&quot;&gt;\(np\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Variance: &lt;span class=&quot;math inline&quot;&gt;\(np(1-p)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;$X ∼ Geometric(p) $(where p &amp;gt; 0): the number of flips of a coin with heads probability &lt;span class=&quot;math inline&quot;&gt;\(p\)&lt;/span&gt; until the first heads.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;PMF: &lt;span class=&quot;math inline&quot;&gt;\(p(x)=p(1-p)^{x-1}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Mean: &lt;span class=&quot;math inline&quot;&gt;\(\frac{1}{p}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Variance: &lt;span class=&quot;math inline&quot;&gt;\(\frac{1-p}{p^2}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(X ∼ Poisson(λ)\)&lt;/span&gt; (where λ &amp;gt; 0): a probability distribution over the nonnegative integers used for modeling the frequency of rare events.
&lt;ul&gt;
&lt;li&gt;PMF: &lt;span class=&quot;math inline&quot;&gt;\(p(x)=e^{-\lambda}\frac{\lambda^x}{x!}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Mean: &lt;span class=&quot;math inline&quot;&gt;\(\lambda\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Variance: &lt;span class=&quot;math display&quot;&gt;\[\lambda\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Properties&lt;/strong&gt;: Poisson random variable may be used to approximate a &lt;strong&gt;&lt;em&gt;binomial&lt;/em&gt;&lt;/strong&gt; random variable when the binomial parameter &lt;em&gt;n&lt;/em&gt; is large and &lt;em&gt;p&lt;/em&gt; is small.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Probability" scheme="https://nancyyanyu.github.io/tags/Probability/"/>
    
  </entry>
  
  <entry>
    <title>Data Analysis of K-POP: Playing with Spotify API</title>
    <link href="https://nancyyanyu.github.io/posts/63adf3bb/"/>
    <id>https://nancyyanyu.github.io/posts/63adf3bb/</id>
    <published>2019-07-16T16:08:09.000Z</published>
    <updated>2019-07-17T21:25:44.358Z</updated>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>K-pop has become a phenomenon in the U.S, as evidenced by bombing number of K-pop shows across the nation. In Spotify's K-pop genre, there are more than 500 K-pop artists. Among them, the wildly popular male group BTS is certainly worth mentioning. According to the data of Top 100 selling artists in the first half of 2019, out of 10 album copies sold out, there are 4 of BTS's albums <a href="https://www.allkpop.com/article/2019/07/bts-occupies-419-of-total-album-sales-of-top-100-kpop-artists" target="_blank" rel="noopener">(Ref)</a>.</p><p>BTS is certainly a reason for the rise of K-pop, but as more and more K-pop artists successfully hit the Billboard charts and took massive world tour, K-pop is definitely a hot topic to dig into.</p><p>This K-Pop sensation draws my attention to perform data analysis using data provided by <strong><em>Spotify API</em></strong> which could be used to answer some interesting questions like:</p><ul><li>Besides BTS, who are the best K-pop artists in the U.S. market?</li><li>How do their popularity change?</li><li>How did the market of K-pop music evolve in size and audio features?</li></ul><a id="more"></a><h1 id="setup">Setup</h1><h3 id="register-an-app"><strong>Register an app</strong>:</h3><p>Just like my last project which used Twitter API, the first step is to register an app on <a href="https://developer.spotify.com/" target="_blank" rel="noopener">Spotify for Developers</a>, then copy <em>Client ID</em> and <em>Client Secret</em>.</p><p><img src="./1.jpg" width="800"></p><p> </p><h3 id="install-spotify-web-api-python-library"><strong>Install Spotify Web API Python library:</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install spotipy</span><br></pre></td></tr></table></figure><p>Please check <strong><em>spotipy</em></strong>'s <a href="https://spotipy.readthedocs.io/en/latest/" target="_blank" rel="noopener">document</a> for more detail.</p><p> </p><h3 id="download-all-relevant-data-to-local-machine">Download all relevant data to local machine</h3><p>I firstly wrote a class <strong><em>MySpotify</em></strong> to grab all relevant data and stored the data under local machine in <em>json</em> format.</p><h4 id="relevant-data"><strong>Relevant data:</strong></h4><ul><li>All artists' information under 'k-pop' genre;</li><li>All albums' information of every k-pop artists</li><li>All tracks' information of all albums of every k-pop artists</li><li>Audio features provided by <em>Spotify</em> of all tracks</li></ul><h4 id="audio-features"><strong>Audio features:</strong></h4><p>There are 8 main audio features provided by <em>Spotify</em>. Here's the explanation from Spotify's <a href="https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/" target="_blank" rel="noopener">document</a>.</p><ul><li><strong><em>Danceability</em></strong> describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity.</li><li><strong>Energy</strong> is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity.</li><li><strong>Instrumentalness</strong> predicts whether a track contains no vocals.<br></li><li><strong>Liveness</strong> detects the presence of an audience in the recording.</li><li><strong>Loudness</strong>: the overall loudness of a track in decibels (dB).</li><li><strong>Speechiness</strong> detects the presence of spoken words in a track.</li><li><strong>Valence:</strong> A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track.</li><li><strong>Tempo:</strong> the overall estimated tempo of a track in beats per minute (BPM).</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpotify</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""This class was to download data from Spotify API.</span></span><br><span class="line"><span class="string">    The download process should be: </span></span><br><span class="line"><span class="string">        artist=artist_genre -&gt; album=album_artists(artist) -&gt; songs=songs_albums(album)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,genre=<span class="string">'k-pop'</span>)</span>:</span></span><br><span class="line">        self.authenticate()</span><br><span class="line">        self.genre=genre</span><br><span class="line">                </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">authenticate</span><span class="params">(self)</span>:</span></span><br><span class="line">        auth=open_json(<span class="string">'auth'</span>)</span><br><span class="line">        my_id = auth[<span class="string">'my_id'</span>]</span><br><span class="line">        secret_key = auth[<span class="string">'secret_key'</span>]</span><br><span class="line">        </span><br><span class="line">        client_credentials_manager = SpotifyClientCredentials(client_id = my_id, client_secret = secret_key)</span><br><span class="line">        self.sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)                </span><br><span class="line">        </span><br><span class="line">......</span><br></pre></td></tr></table></figure><p><a href="https://github.com/nancyyanyu/mini_projects/blob/master/spotify_project/spotify_app.py" target="_blank" rel="noopener">Full code</a></p><p> </p><h1 id="data-analysis">Data Analysis</h1><h3 id="who-are-the-most-popular-k-pop-artists">1. Who are the most popular K-pop artists?</h3><p>Before we jump to the answer, let me take a second to introduce two measures of prevelance in Spotify.</p><ul><li><strong>Followers:</strong> The number of people following the artist.</li><li><strong>Popularity:</strong> The popularity of the artist. The value will be between 0 and 100, with 100 being the most popular. The artist’s popularity is calculated from the popularity of all the artist’s tracks.</li></ul><p>There are cases when some artists have a large amount of followers, but relatively low popularity. That's possibly because they are not active in releasing new songs recently or simply retired from musical life.</p><p>To find the hottest K-pop artists, I extracted all artists' followers and popularity data, ranked them, and visualized top 40 artists in terms of the number of followers.</p><p><img src="./2.png" alt="Fig 1" width="800"></p><p>In <em>Fig 1</em>, for artists with <font color="orange">orange</font> bar higher than <font color="blue">blue</font> bar, some of them are hot, recently debuted K-pop groups, like <strong><em>(G)I-DLE, NCT 127, Stray Kids</em></strong>; most of them released new songs in recent month, e.g. <strong>Jay Park</strong> released his new EP &quot;Nothing Matters&quot; this month.</p><p>For artists with <font color="orange">orange</font> bar lower than <font color="blue">blue</font> bar, some of them are inactive for a long time, such as <strong>G-Dragon</strong> who is serving compulsory military duty; some of them are <em>temporary</em> group or solo which only released one or two albums, like <strong>EXO-CBX, J-hope</strong>.</p><p>Ranking could be sometimes misleading. As 4 out of 10 album copies sold out are BTS's albums, what if this K-pop phenomenon is actually BTS phenomenon? Let's check the numerical number of followers and popularity of these K-pop artists.</p><p><img src="./3.png" alt="Fig 2" width="800"></p><p><img src="./4.png" alt="Fig 3" width="800"></p><p>From <em>Fig 2 &amp; 3</em>, we can see that BTS has more than 1/3 followers than the second hottest group, and more than 1/2 followers than the third one. In addition, BTS has an popularity rate of 99, very close to 100, the highest possible popularity rate.</p><p>For comparison, I listed some top artists in the U.S. market:</p><table><thead><tr class="header"><th>Artists</th><th>Followers</th><th>Popularity</th></tr></thead><tbody><tr class="odd"><td>Ed Sheeran</td><td>47,295,649</td><td>100</td></tr><tr class="even"><td>Drake</td><td>38,673,834</td><td>99</td></tr><tr class="odd"><td>Taylor Swift</td><td>21,198,450</td><td>93</td></tr><tr class="even"><td>Ariana Grande</td><td>32,355,391</td><td>95</td></tr></tbody></table><p>Comparing the top K-pop artists and the top U.S. mainstream artists, except from BTS, other K-pop artists' could hardly be labelled as top stars in the U.S. Even BTS who have amazingly high popularity, they have significantly less followers.</p><p>In my point of view, before BTS rising to fame, K-pop is only a subculture in the U.S., and not usually depicted by western mainstream media. Today, whether K-pop has become a mainstream phenomenon is still open to debate. At least the far less followers of most of the K-pop artists is an evidence against this argument.</p><h3 id="how-does-popularity-change">2. How does popularity change?</h3><p>Since <em>Spotify</em> does not provide historical data of artists' popularity and followers, I write a class <strong><em>Popularity</em></strong> to grab everyday's data, upload to AWS DynamoDB.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Popularity</span><span class="params">(MySpotify)</span>:</span>    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,table_name=<span class="string">'KPOP_POPULARITY'</span>,genre=<span class="string">'k-pop'</span>)</span>:</span></span><br><span class="line">        super().__init__(genre)</span><br><span class="line">        self.table = dynamodb.Table(table_name)</span><br><span class="line">        self.track_table=dynamodb.Table(table_name+<span class="string">'_TRACK'</span>)</span><br><span class="line">        self.data_folder=<span class="string">'%s_artists_info'</span>%self.genre</span><br><span class="line">        self.today=str(dt.datetime.now().date())</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">put_data</span><span class="params">(self)</span>:</span></span><br><span class="line">        artists=open_json(<span class="string">"data/artists/"</span>+self.data_folder)            </span><br><span class="line">        info=&#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> artist <span class="keyword">in</span> artists:</span><br><span class="line">            info[artist[<span class="string">'name'</span>]]=&#123;&#125;</span><br><span class="line">            info[artist[<span class="string">'name'</span>]][<span class="string">'followers'</span>]=artist[<span class="string">'followers'</span>][<span class="string">'total'</span>]</span><br><span class="line">            info[artist[<span class="string">'name'</span>]][<span class="string">'popularity'</span>]=artist[<span class="string">'popularity'</span>]</span><br><span class="line">        </span><br><span class="line">        self.table.put_item(</span><br><span class="line">           Item=&#123;<span class="string">'time'</span>: self.today,</span><br><span class="line">               <span class="string">'info'</span>:info&#125;)</span><br><span class="line">        print(<span class="string">"Update artist data to AWS DynamoDB: &#123;&#125;"</span>.format(self.today))</span><br><span class="line">                </span><br><span class="line">......</span><br></pre></td></tr></table></figure><p><a href="https://github.com/nancyyanyu/mini_projects/blob/master/spotify_project/popularity.py" target="_blank" rel="noopener">Full code</a></p><p>The <strong><em>ETL process</em></strong> works like this:</p><center><img src="./6.png" width="540"></center><ol type="1"><li>Grab artists' data from <em>Spotify</em> API;</li><li>Organize and clean the data;</li><li>Save data to local machine in <em>json</em> format;</li><li>Save data to <strong>AWS DynamoDB</strong>.</li></ol><p>Here is a visualization of the change of BTS' followers &amp; popularity using data collected during writing this report.</p><p><img src="./5.png" alt="Fig 4" width="800"></p><h3 id="how-many-k-pop-albums-released-every-year">3. How many K-pop albums released every year?</h3><p>As an industry, K-pop has experienced significant increase in size over the past two decades. According to each music agent companies' marketing tactics, K-pop artitists will demonstrate certain pattern in terms of the timing of releasing new albums.</p><p>Here is a visualization of the number of albums released by year and by month. We can see a sharp increase from 2016 to 2018. Also artists seem to be more active during October and November.</p><p><img src="./7.png" width="700"></p><h3 id="audios-features-analysis">4. Audios' features analysis</h3><p>To analyze audio features, I wrote a class <strong><em>Analyze</em></strong> to wrap to ETL process and analysis functions.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Analyze</span><span class="params">(object)</span>:</span>       </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,genre=<span class="string">'k-pop'</span>)</span>:</span></span><br><span class="line">        self.features=[<span class="string">'acousticness'</span>,<span class="string">'liveness'</span>,          </span><br><span class="line">                  <span class="string">'instrumentalness'</span>,<span class="string">'speechiness'</span>, </span><br><span class="line">                   <span class="string">'danceability'</span>,<span class="string">'energy'</span>,<span class="string">'valence'</span>,<span class="string">'tempo'</span>]</span><br><span class="line">        self.poplr=Popularity()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">extract_data</span><span class="params">(self,artist)</span>:</span></span><br><span class="line">        df=pd.read_hdf(<span class="string">'./data/analysis/&#123;&#125;_tracks_analysis.h5'</span>.format(artist))</span><br><span class="line">        <span class="keyword">return</span> df</span><br><span class="line">......</span><br></pre></td></tr></table></figure><p><a href="https://github.com/nancyyanyu/mini_projects/blob/master/spotify_project/track_analysis.py" target="_blank" rel="noopener">Full code</a></p><p>The <strong><em>ETL process</em></strong> works like this:</p><center><img src="./10.png" width="540"></center><p>As last chapter says, I extracted 8 audio features of all tracks. I calculated the truncated average of each feature by year, and try to find some changes in the K-pop music itself.</p><p><img src="./8.png" width="800"></p><p>We can see features like <strong><em>acousticness</em></strong>, and <strong><em>instrumentalness</em></strong> have an upward trend; while <strong><em>energy</em></strong>, <strong><em>tempo</em></strong> and <strong><em>valence</em></strong> have un downward trend in recent years.</p><p>The evolution of K-pop music is not smooth. It seems it has experienced a radical shift around 2013 (2012-2014), as most of the features changed their disposition. After 2013 or 2014, the K-pop songs:</p><ul><li>Become more and more acoustic, and instrumental.</li><li>Contain more spoken words.</li><li>Convey less energy and positiveness. In other words, they sound less cheerful but more sad and depressed.</li><li>Become a bit slower in tempo.</li><li>Are still highly danceable, but not as danceable as before 2014.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;K-pop has become a phenomenon in the U.S, as evidenced by bombing number of K-pop shows across the nation. In Spotify&#39;s K-pop genre, there are more than 500 K-pop artists. Among them, the wildly popular male group BTS is certainly worth mentioning. According to the data of Top 100 selling artists in the first half of 2019, out of 10 album copies sold out, there are 4 of BTS&#39;s albums &lt;a href=&quot;https://www.allkpop.com/article/2019/07/bts-occupies-419-of-total-album-sales-of-top-100-kpop-artists&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;(Ref)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;BTS is certainly a reason for the rise of K-pop, but as more and more K-pop artists successfully hit the Billboard charts and took massive world tour, K-pop is definitely a hot topic to dig into.&lt;/p&gt;
&lt;p&gt;This K-Pop sensation draws my attention to perform data analysis using data provided by &lt;strong&gt;&lt;em&gt;Spotify API&lt;/em&gt;&lt;/strong&gt; which could be used to answer some interesting questions like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Besides BTS, who are the best K-pop artists in the U.S. market?&lt;/li&gt;
&lt;li&gt;How do their popularity change?&lt;/li&gt;
&lt;li&gt;How did the market of K-pop music evolve in size and audio features?&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Project" scheme="https://nancyyanyu.github.io/categories/Project/"/>
    
    
      <category term="AWS" scheme="https://nancyyanyu.github.io/tags/AWS/"/>
    
      <category term="Airflow" scheme="https://nancyyanyu.github.io/tags/Airflow/"/>
    
      <category term="Visualization" scheme="https://nancyyanyu.github.io/tags/Visualization/"/>
    
  </entry>
  
  <entry>
    <title>Realtime Twitter Data Analysis using Spark Streaming</title>
    <link href="https://nancyyanyu.github.io/posts/9fb5a802/"/>
    <id>https://nancyyanyu.github.io/posts/9fb5a802/</id>
    <published>2019-07-10T18:23:56.000Z</published>
    <updated>2019-07-16T18:08:21.260Z</updated>
    
    <content type="html"><![CDATA[<p>In this project, I built an application that extract streaming tweets from Twitter, transform the data, and visualize using Apache Sparking Streaming to gain the trending hashtags of a specific topic. In particular, I used a window size of 5 minutes to always get the latest 5 minutes result.</p><a id="more"></a><h1 id="apache-spark-streaming">Apache Spark Streaming</h1><p>Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like <code>map</code>, <code>reduce</code>, <code>join</code> and <code>window</code>.</p><p>I will skip the explaination of how to set up spark in local machine, and the details of Streaming API. Please see the <a href="https://spark.apache.org/docs/latest/api/python/pyspark.streaming.html" target="_blank" rel="noopener">document</a> of Spark.</p><h1 id="part-i.-create-twitter-streaming">Part I. Create Twitter Streaming</h1><h2 id="register-twitter-app">Register Twitter App</h2><p>Before using Twitter's API, I registered an app <a href="https://developer.twitter.com/en/apps" target="_blank" rel="noopener">here</a>, and got the <em>Consumer API keys</em>, <em>Access token &amp; access token secret</em>.</p><p><img src="./2.jpg" width="500"></p><p>These information should be saved, as Twitter needs them to authenticate a user.</p><h2 id="extract-tweets-from-twitter-streaming">Extract Tweets from Twitter Streaming</h2><p>I used Twitter API Python wrapper <a href="https://github.com/bear/python-twitter" target="_blank" rel="noopener">python-twitter</a> to get Tweets stream. Here's a snippet of the code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">twt_app</span><span class="params">(TCP_IP,TCP_PORT,keyword=KEY_WORD)</span>:</span></span><br><span class="line">    consumer_key=<span class="string">''</span></span><br><span class="line">    consumer_secret=<span class="string">''</span></span><br><span class="line">    access_token=<span class="string">''</span></span><br><span class="line">    access_token_secret=<span class="string">''</span></span><br><span class="line">    </span><br><span class="line">    api = twitter.Api(consumer_key=consumer_key,</span><br><span class="line">                      consumer_secret=consumer_secret,</span><br><span class="line">                      access_token_key=access_token,</span><br><span class="line">                      access_token_secret=access_token_secret,</span><br><span class="line">                      sleep_on_rate_limit=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    LANGUAGES = [<span class="string">'en'</span>]</span><br><span class="line">    </span><br><span class="line">    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)</span><br><span class="line">    s.bind((TCP_IP, TCP_PORT))</span><br><span class="line">    s.listen(<span class="number">10</span>)    </span><br><span class="line">    conn, addr = s.accept()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> api.GetStreamFilter(track=[keyword],languages=LANGUAGES):</span><br><span class="line">        conn.send( line[<span class="string">'text'</span>].encode(<span class="string">'utf-8'</span>) )</span><br><span class="line">        print(line[<span class="string">'text'</span>])</span><br><span class="line">        print()</span><br></pre></td></tr></table></figure><p>Basically, I get the streaming of tweets from Twitter API, extract each tweet's text content, and send them to Spark Streaming instance via TCP connection.</p><p>Let's try a topic as <strong>&quot;Trump&quot;</strong>! Here is the result recorded in console:</p><p><img src="./3.gif" width="750"></p><h1 id="part-ii.-set-up-streaming-application">Part II. Set Up Streaming Application</h1><p>Then I set up Spark Streaming App to process tweets text, gain hashtags in every tweet mentioned <strong>&quot;Trump&quot;</strong>, and barplot the top 20 hashtags on the times they appeared in most recent 5 minutes. Here's a snippet of the code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">spark</span><span class="params">(TCP_IP,TCP_PORT,KEY_WORD)</span>:</span></span><br><span class="line">    sc=SparkContext(appName=<span class="string">"TwitterStreamming"</span>)</span><br><span class="line">    sc.setLogLevel(<span class="string">"ERROR"</span>)</span><br><span class="line">    ssc=StreamingContext(sc,<span class="number">5</span>)</span><br><span class="line">    </span><br><span class="line">    socket_stream = ssc.socketTextStream(TCP_IP,TCP_PORT)</span><br><span class="line">    </span><br><span class="line">    lines=socket_stream.window(<span class="number">300</span>)</span><br><span class="line">    df=lines.flatMap(<span class="keyword">lambda</span> x:x.split(<span class="string">" "</span>))  \</span><br><span class="line">            .filter(<span class="keyword">lambda</span> x:x.startswith(<span class="string">"#"</span>))  \</span><br><span class="line">            .filter(<span class="keyword">lambda</span> x:x!=<span class="string">'#%s'</span>%KEY_WORD)  </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(rdd)</span>:</span></span><br><span class="line">        spark=SparkSession \</span><br><span class="line">                .builder \</span><br><span class="line">                .config(conf=rdd.context.getConf()) \</span><br><span class="line">                .getOrCreate()</span><br><span class="line">    </span><br><span class="line">        rowRdd = rdd.map(<span class="keyword">lambda</span> x: Row(word=x))</span><br><span class="line">        wordsDataFrame = spark.createDataFrame(rowRdd)</span><br><span class="line">    </span><br><span class="line">        wordsDataFrame.createOrReplaceTempView(<span class="string">"words"</span>)</span><br><span class="line">        wordCountsDataFrame = spark.sql(<span class="string">"select word, count(*) as total from words group by word order by 2 desc"</span>)       </span><br><span class="line">        pd_df=wordCountsDataFrame.toPandas()</span><br><span class="line">        </span><br><span class="line">        plt.figure( figsize = ( <span class="number">10</span>, <span class="number">8</span> ) )</span><br><span class="line">        sns.barplot( x=<span class="string">"total"</span>, y=<span class="string">"word"</span>, data=pd_df.head(<span class="number">20</span>))</span><br><span class="line">        plt.show()</span><br><span class="line">        </span><br><span class="line">    df.foreachRDD(process)</span><br><span class="line">    </span><br><span class="line">    ssc.start()</span><br><span class="line">    time.sleep(<span class="number">600</span>)</span><br><span class="line">    ssc.stop(stopSparkContext=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>Let's see what I got!</p><p><img src="./4.gif" width="750"></p><h1 id="summary">Summary</h1><p>Thanks to Twitter API and its python Wrapper, I was able to easily get tweets streaming filtered on specified topic. Real-time ETL process could be used to provide instantaneous recommendation, anomaly detection, and etc. There are various projects around Twitter Streaming to be explored. I, in this post, tried a very simple application to find the real-time hashtags trending around a topic.</p><p>As I filtered tweets on topic <strong>&quot;Trump&quot;</strong> , I got <em>#WGDP</em>, <em>#USWNT</em> to be two of the most trending hashtags during the time I ran the application. This application could catch big hot news, and it also serves as a great way to know about what people are talking about in a smaller topic.</p><p>Please check the full code on <a href="https://github.com/nancyyanyu/mini_projects/tree/master/twitter_project" target="_blank" rel="noopener">GitHub</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;In this project, I built an application that extract streaming tweets from Twitter, transform the data, and visualize using Apache Sparking Streaming to gain the trending hashtags of a specific topic. In particular, I used a window size of 5 minutes to always get the latest 5 minutes result.&lt;/p&gt;
    
    </summary>
    
      <category term="Project" scheme="https://nancyyanyu.github.io/categories/Project/"/>
    
    
      <category term="Spark" scheme="https://nancyyanyu.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark SQL &amp; DataFrame, SparkETL</title>
    <link href="https://nancyyanyu.github.io/posts/9a328503/"/>
    <id>https://nancyyanyu.github.io/posts/9a328503/</id>
    <published>2019-07-09T19:49:45.000Z</published>
    <updated>2019-07-09T18:04:06.548Z</updated>
    
    <content type="html"><![CDATA[<h1 id="sql-and-dataframe">SQL and DataFrame</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyspark</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.conf <span class="keyword">import</span> SparkConf</span><br></pre></td></tr></table></figure><p>A <strong><em>SparkSession</em></strong> can be used create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables, and read parquet files. To create a SparkSession, use the following builder pattern:</p><p><strong><em>getOrCreate</em></strong>: Gets an existing SparkSession or, if there is no existing one, creates a new one based on the options set in this builder</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conf=SparkConf().set(<span class="string">"spark.python.profile"</span>, <span class="string">"true"</span>)</span><br><span class="line">spark=SparkSession.builder.master(<span class="string">"local"</span>).appName(<span class="string">"wordcount"</span>).config(conf=SparkConf()).getOrCreate()</span><br></pre></td></tr></table></figure><p><strong><em>createDataFrame</em></strong>(data, schema=None, samplingRatio=None, verifySchema=True)</p><p>Creates a DataFrame from an RDD, a list or a pandas.DataFrame.</p><a id="more"></a><h2 id="creates-a-dataframe-from-a-list">Creates a DataFrame from a list</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">l=[(<span class="string">"alice"</span>,<span class="number">1</span>),(<span class="string">"bob"</span>,<span class="number">2</span>)]</span><br><span class="line">df=spark.createDataFrame(l,[<span class="string">'name'</span>,<span class="string">'age'</span>])</span><br><span class="line">df.collect()</span><br></pre></td></tr></table></figure><pre><code>[Row(name=&#39;alice&#39;, age=1), Row(name=&#39;bob&#39;, age=2)]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.toPandas()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }        .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe"><thead><tr style="text-align: right;"><th></th><th>name</th><th>age</th></tr></thead><tbody><tr><th>0</th><td>alice</td><td>1</td></tr><tr><th>1</th><td>bob</td><td>2</td></tr></tbody></table></div><h2 id="creates-a-dataframe-from-pandas-dataframe-and-use-sql-query">Creates a DataFrame from pandas DataFrame, and use sql query</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df2=spark.createDataFrame(pd.DataFrame(l,columns=[<span class="string">'name'</span>,<span class="string">'age'</span>]))</span><br><span class="line">df2.collect()</span><br></pre></td></tr></table></figure><p>[Row(name='alice', age=1), Row(name='bob', age=2)]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df2.select(<span class="string">"name"</span>).collect()</span><br></pre></td></tr></table></figure><p>[Row(name='alice'), Row(name='bob')]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df2.createOrReplaceTempView(<span class="string">'table1'</span>)</span><br><span class="line">spark.sql(<span class="string">"select name from table1"</span>).collect()</span><br></pre></td></tr></table></figure><p>[Row(name='alice'), Row(name='bob')]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.table(<span class="string">"table1"</span>).collect()</span><br></pre></td></tr></table></figure><p>[Row(name='alice', age=1), Row(name='bob', age=2)]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.stop()</span><br></pre></td></tr></table></figure><h2 id="sqlcontext">SQLContext</h2><p>The entry point for working with structured data (rows and columns) in Spark, in Spark 1.x.</p><p>As of Spark 2.0, this is replaced by SparkSession. However, we are keeping the class here for backward compatibility.</p><p>A SQLContext can be used create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables, and read parquet files.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sc=SparkContext()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Row,SQLContext</span><br><span class="line">sqlContext=SQLContext(sc)</span><br><span class="line">rdd=sc.parallelize(l)</span><br><span class="line">Person=Row(<span class="string">"name"</span>,<span class="string">"age"</span>)</span><br><span class="line">person=rdd.map(<span class="keyword">lambda</span> x: Person(*x))</span><br><span class="line">sqlContext.createDataFrame(person).collect()</span><br></pre></td></tr></table></figure><p>[Row(name='alice', age=1), Row(name='bob', age=2)]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sc.stop()</span><br></pre></td></tr></table></figure><h1 id="sparketl">SparkETL</h1><p>ETL is a type of data integration process referring to three distinct but interrelated steps (Extract, Transform and Load) and is used to synthesize data from multiple sources many times to build a Data Warehouse, Data Hub, or Data Lake.</p><p>Let's write an ETL job on pyspark!</p><p>Reference: (https://github.com/AlexIoannides/pyspark-example-project)</p><p>Before building ETL process, we write a function <em>start_spark</em> to start our sparkSession, update and get our configuration.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> __main__</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> environ,listdir,path</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkFiles</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.conf <span class="keyword">import</span> SparkConf</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_spark</span><span class="params">(app_name=<span class="string">"my_spark_app"</span>,master=<span class="string">"local[*]"</span>,files=[<span class="string">'etl_conf.json'</span>])</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    flag_repl=<span class="keyword">not</span>(hasattr(__main__,<span class="string">'__file__'</span>))</span><br><span class="line">    flag_debug=<span class="string">'DEBUG'</span> <span class="keyword">in</span> environ.keys()</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span>(flag_repl <span class="keyword">or</span> flag_debug):    </span><br><span class="line">        spark_builder=(SparkSession.builder.appName(app_name))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        spark_builder=SparkSession.builder.appName(app_name).master(master)</span><br><span class="line">    </span><br><span class="line">    spark_files=<span class="string">'.'</span>.join(list(files))</span><br><span class="line">    spark_builder.config(<span class="string">'spark.files'</span>,spark_files)</span><br><span class="line">    spark_builder.config(conf=SparkConf())    </span><br><span class="line">    spark_sess=spark_builder.getOrCreate()</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">#spark_logger=logger.Log4j(spark_sess)</span></span><br><span class="line">    spark_files_dir=SparkFiles.getRootDirectory()</span><br><span class="line">    </span><br><span class="line">    config_files=[x <span class="keyword">for</span> x <span class="keyword">in</span> listdir(spark_files_dir) <span class="keyword">if</span> x.endswith(<span class="string">'conf.json'</span>)]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> config_files:</span><br><span class="line">        path_to_config_file=path.join(spark_files_dir,config_files[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">with</span> open(path_to_config_file,<span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            config_dict=json.load(f)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        </span><br><span class="line">        config_dict=<span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> spark_sess,config_dict</span><br></pre></td></tr></table></figure><p>ETL Process contains 3 stages: Extract, Transform, Load. In Spark,</p><ul><li><strong>Extract</strong>: read Parquet format data in local machine</li><li><strong>Transform</strong>: use sparkSQL to manipulate data</li><li><strong>Load</strong>: write to csv</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> spark <span class="keyword">import</span> start_spark</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkFiles</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Row</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    spark,conf=start_spark()</span><br><span class="line">    steps_per_floor_=conf[<span class="string">'steps_per_floor'</span>]</span><br><span class="line">    df=extract(spark)</span><br><span class="line">    df_tf=transform(df,steps_per_floor_,spark)</span><br><span class="line">    load(df_tf)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract</span><span class="params">(spark)</span>:</span></span><br><span class="line">    df=spark.read.parquet(<span class="string">'tests/test_data/employees'</span>)</span><br><span class="line">    <span class="keyword">return</span> df</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(df,steps_per_floor_,spark)</span>:</span></span><br><span class="line">    df.createOrReplaceTempView(<span class="string">"table1"</span>)</span><br><span class="line">    df_transformed=spark.sql(<span class="string">"select id, concat(first_name,' ' , second_name) as name, floor* %s as steps_to_desk from table1"</span>%steps_per_floor_)</span><br><span class="line">    <span class="keyword">return</span> df_transformed</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load</span><span class="params">(df)</span>:</span></span><br><span class="line">    df.coalesce(<span class="number">1</span>).write.csv(<span class="string">'loaded_data'</span>, mode=<span class="string">'overwrite'</span>, header=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_test_data</span><span class="params">(spark,conf)</span>:</span>   </span><br><span class="line">    local_records=[</span><br><span class="line">            Row(id=<span class="number">1</span>, first_name=<span class="string">'nancy'</span>, second_name=<span class="string">"yan"</span>, floor=<span class="number">1</span>),</span><br><span class="line">            Row(id=<span class="number">2</span>, first_name=<span class="string">'Dan'</span>, second_name=<span class="string">'Sommerville'</span>, floor=<span class="number">1</span>),</span><br><span class="line">            Row(id=<span class="number">3</span>, first_name=<span class="string">'Alex'</span>, second_name=<span class="string">'Ioannides'</span>, floor=<span class="number">2</span>),</span><br><span class="line">            Row(id=<span class="number">4</span>, first_name=<span class="string">'Ken'</span>, second_name=<span class="string">'Lai'</span>, floor=<span class="number">2</span>),</span><br><span class="line">            Row(id=<span class="number">5</span>, first_name=<span class="string">'Stu'</span>, second_name=<span class="string">'White'</span>, floor=<span class="number">3</span>),</span><br><span class="line">            Row(id=<span class="number">6</span>, first_name=<span class="string">'Mark'</span>, second_name=<span class="string">'Sweeting'</span>, floor=<span class="number">3</span>),</span><br><span class="line">            Row(id=<span class="number">7</span>, first_name=<span class="string">'Phil'</span>, second_name=<span class="string">'Bird'</span>, floor=<span class="number">4</span>),</span><br><span class="line">            Row(id=<span class="number">8</span>, first_name=<span class="string">'Kim'</span>, second_name=<span class="string">'Suter'</span>, floor=<span class="number">4</span>)</span><br><span class="line">        ]</span><br><span class="line">    </span><br><span class="line">    df=spark.createDataFrame(local_records)</span><br><span class="line">    df_tf=transform(df,conf[<span class="string">'steps_per_floor'</span>],spark)</span><br><span class="line">    df_tf.coalesce(<span class="number">1</span>).write.parquet(<span class="string">'tests/test_data/employees_report'</span>,mode=<span class="string">'overwrite'</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;sql-and-dataframe&quot;&gt;SQL and DataFrame&lt;/h1&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; pyspark&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; pyspark &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; SparkContext&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; pyspark.sql &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; SparkSession&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; pyspark.conf &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; SparkConf&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;A &lt;strong&gt;&lt;em&gt;SparkSession&lt;/em&gt;&lt;/strong&gt; can be used create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables, and read parquet files. To create a SparkSession, use the following builder pattern:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;getOrCreate&lt;/em&gt;&lt;/strong&gt;: Gets an existing SparkSession or, if there is no existing one, creates a new one based on the options set in this builder&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;conf=SparkConf().set(&lt;span class=&quot;string&quot;&gt;&quot;spark.python.profile&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;true&quot;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;spark=SparkSession.builder.master(&lt;span class=&quot;string&quot;&gt;&quot;local&quot;&lt;/span&gt;).appName(&lt;span class=&quot;string&quot;&gt;&quot;wordcount&quot;&lt;/span&gt;).config(conf=SparkConf()).getOrCreate()&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;createDataFrame&lt;/em&gt;&lt;/strong&gt;(data, schema=None, samplingRatio=None, verifySchema=True)&lt;/p&gt;
&lt;p&gt;Creates a DataFrame from an RDD, a list or a pandas.DataFrame.&lt;/p&gt;
    
    </summary>
    
      <category term="Big Data" scheme="https://nancyyanyu.github.io/categories/Big-Data/"/>
    
    
      <category term="Spark" scheme="https://nancyyanyu.github.io/tags/Spark/"/>
    
      <category term="ETL" scheme="https://nancyyanyu.github.io/tags/ETL/"/>
    
      <category term="SQL" scheme="https://nancyyanyu.github.io/tags/SQL/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop MapReduce: Tuning Distributed Storage Platform with File Types</title>
    <link href="https://nancyyanyu.github.io/posts/c490e7f0/"/>
    <id>https://nancyyanyu.github.io/posts/c490e7f0/</id>
    <published>2019-07-09T18:08:35.268Z</published>
    <updated>2019-07-10T23:03:55.904Z</updated>
    
    <content type="html"><![CDATA[<h1 id="data-modeling-and-file-formats">Data modeling and file formats</h1><p>There is a mismatch between terms used to define business tasks and terms used to describe what HDFS is.</p><p>Data modeling and data management are concerned with these issues.</p><a id="more"></a><p><img src="./week_50.png" width="300"></p><h2 id="data-modeling">Data modeling</h2><ul><li><p><strong>Data model</strong> – a way you think about your data elements, what they are, what domain they come from, how different elements relate to each other, what they are composed of</p></li><li>abstract model</li><li>explicitly defines the structure of data</li><li>Makes some things easier to express than others</li><li><p>Will use a relational model</p></li><li><p><strong>Relational data model</strong></p></li><li>a data set is an ordered set called table of tuples called rows.</li><li>Where every tuple is composed of simple values such as numbers or strings.</li><li><p>A position of a value within a tuple is a column. And column defines value semantics. <img src="./week_51.png" width="300"></p></li><li><p><strong>Graph data model</strong></p></li><li>A graph consist of vertices and edges.</li><li>vertices: represent entities. Movies, actors, directors, titles, and so on.</li><li><p>edges: represent relations between entities.</p></li></ul><p><img src="./week_52.png" width="300"></p><ul><li><p><strong>Unstructured data</strong></p></li><li>Technically, all data is structured at least as a byte sequence</li><li>Usually, means “not structured enough for a task</li><li><p>Denote <em>complexity of bringing data to useful form for a particular application</em>.</p></li></ul><p>e.g. Videos are structured as a sequence of frames, where each frame is an ordered set of pixels, where every pixel is just a triple of RGB color intensities.</p><p>However, this structure is useless if you're willing to count people in the video. The hard job is to do the image recognition and bring the appropriate structure to the data so that solving accounting problem would become easier.</p><h2 id="data-management">Data Management</h2><blockquote><p>How to store and how to organize your data physically. In Hadoop, this is a matter of file format or storage format</p></blockquote><p>File format: - Defines (physical) data layout - Different choices of data layout lead to different tradeoffs in application complexity, and thus, affect performance and correctness.</p><p><strong>Primary function</strong>: - <strong>serialization</strong>: a process of converting an object into a sequence of bytes which can be persisted to a disk or database or can be sent through streams. - <strong>deserialization</strong>: creating object from sequence of bytes</p><p><strong>Differ in:</strong> - space efficiency: different formats use different coding schemes which directly affect consumed disk space. - encoding &amp; decoding speed - supported data types - splittable/monolithic structure: this property allows you to extract a subset of data without reading the entire file. We typically expect data to be splittable. This follows from our data model. - extensibility</p><h1 id="text-formats">Text Formats</h1><p>line delimited text files:</p><ul><li><p>pros: human-readable</p></li><li><p>cons: you need to parse it, convert it from the textual form into programmatic data structures.</p></li></ul><h2 id="csv">CSV</h2><blockquote><p>comma-separated values</p></blockquote><p>Criteria: - Space efficiency: BAD - Extensibility: BAD -It is not that easy to remove or reorder fields in these formats and the code is likely to make assumptions about the field indexes. - Splittable: You should not include any headers in your data as it hinders splittability and mergability of your data. - Data types: ONLY STRINGS - Speed: generation and parsing are very efficient</p><h2 id="json">JSON</h2><blockquote><p>JavaScript Object Notation: defines a representation for the primitive values and their combination in the form of lists and maps.</p></blockquote><p>Criteria: - Space efficiency: WORSE THAN CSV -It includes field names in serialized form. As you can see, the strings ticker, date, and others are repeated in every row of the data set. - Extensibility: You can easily add and remove fields from your data items and JSON will remain valid and parsable. - Splittable: - SPLITTABLE IF 1 DOCUMENT PER LINE - Data types: JSON allows you to store strings, numbers, Booleans, maps, lists in the native way. - Speed: GOOD ENOUGH</p><h1 id="binary-formats">Binary formats</h1><ul><li>Text formats are designed with human readability in mind</li><li>Binary formats are designed for machines and they trade readability for efficiency and correctness.</li></ul><h2 id="sequence-file">Sequence File</h2><ul><li>First binary format implemented in Hadoop</li><li>Stores sequence of key-value pairs, where key and value are of arbitrary type with the user defined serialization and deserialization code</li><li>Java-specific serialization/deserialization</li><li>Primary use case: storing the intermediate data in MapReduce computations</li></ul><h3 id="data-layout">Data Layout</h3><p>SequenceFile starts with the header which includes format version, class names for key and value types, flags syndicating compression, metadata, and a sync marker.</p><p><img src="./week_60.png" width="400"></p><p>Uncompressed case for every record:</p><ul><li>Fixed size header with a record key length and value length, followed by the serialized key and the serialized value.</li><li>To decode data, you can read the file linearly and use length to read the key and the value.</li></ul><p>Block compressed case:</p><ul><li>Key-value pairs are grouped in blocks and every block starts with a number of pairs followed by the key lengths and the compressed keys. Then by value lengths and finally by compressed values.</li></ul><p>Record compressed case: - Every value is compressed individually while block compressed case, a set of keys or values are compressed together resulting in better compression</p><h3 id="critera">Critera</h3><ul><li>Space efficiency: MODERATE TO GOOD -the on-disk format closely matches the in-memory format to enable fast encoding and decoding.</li><li>Extensibility: No</li><li>Splittable: splittable via sync markers -Sync markers are unique with the high probability, so you can seek to an arbitrary point in the file and scan for the next occurrence of the sync marker to get the next record.</li><li>Data types: Any type implement in the appropriate interfaces could be used with a format.</li><li>Speed:GOOD</li></ul><h2 id="avro">AVRO</h2><blockquote><p>Avro's design goal was to create an efficient and flexible format which could be used with different programming languages.</p></blockquote><ul><li>Both format &amp; support library</li><li>Stores objects defined by the schema</li><li>specifies field names, types, aliases</li><li>defines serialization/deserialization code</li><li>allows some schema updates</li><li>Interoperability with many languages</li></ul><p><strong>schema</strong>: a description of the fields in data items and their types.</p><ul><li>defines data encoding for every item</li><li>When storing data, the schema is included in the file thus allowing future readers to decode it correctly</li><li>If the read schema does not match the data schema, Avro tries to resolve inconsistencies thus enabling smooth schema migrations</li></ul><h3 id="data-layout-1">Data Layout</h3><p>Every Avro file starts with a header followed by a sequence of blocks containing the number of encoded objects, their sizes, and the actual payload.</p><p><strong>Sync markers</strong> are used to delimit consequent blocks of records.</p><p>What is different in Avro is that the serialization code is defined by the schema and not by the user-provided code. <img src="./week_61.png" width="300"></p><h3 id="critera-1">Critera</h3><ul><li>Space efficiency: MODERATE TO GOOD -The encoding format mostly follows the in-memory format. Space savings could be obtained by using compression.</li><li>Extensibility: field addition, or removal, or renaming, are handled transparently by the framework.</li><li>Splittable: Achieved using the same sync marker technique as in sequence files</li><li>Data types: same types as JSON, plus a few more complex types, like enumerations records.</li><li>Speed: GOOD WITH CODE GENERATION -Avro can generate serialization and deserialization code from a schema. In this case, its performance closely matches sequence files.</li></ul><p>Avro is a popular format now, holding the balance between efficiency and flexibility.</p><h2 id="rcfile">RCFile</h2><h3 id="columnar">Columnar</h3><p>The <strong>execution time</strong> for analytical applications is mostly I/O bound---you could gain more by optimizing input and output, while optimizing the computation has a diminishing effect on performance.</p><p><strong>How to save input and output operations?</strong> - not reading the data necessary for the processing - using superior compression schemes.</p><p>RCFile and Parquet: columnar(relational data model) data formats that exploit outlined optimizations.</p><p><strong>Pros of columnar:</strong> 1. Columnar stores are highly efficient in terms of CPU usage.databases were storing data row by row, linearly. They would completely serialize one row before continuing to the other ---&gt; if you need to read the values from just one particular column, you still need to read the whole table.</p><ol start="2" type="1"><li>Columnar stores transpose data layout and store all the values column by column, enabling two key optimizations:</li></ol><ul><li>you can efficiently scan only the necessary data.</li><li>you can achieve better compression ratios, because column-wise, data is more regular and more repetitive, and hence, more compressible.</li></ul><p><strong>Cons of columnar:</strong> 1. row assembly -To reconstruct the full row, you need to perform lookups from all the columns, which is likely to cause more input and output operations. However, by accessing the subset of columns, you can reduce the number of input and output operations.</p><h3 id="data-layout-2">Data Layout</h3><p>RC: Record Columnar</p><ul><li>First columnar format* in Hadoop()</li><li>one of the most popular storage formats for data warehouses.</li><li>Horizontal/vertical partitioning to layout data</li><li>rows are split into row groups; within each row group, values are encoded column by column.</li><li>transpose values within a row group &gt; The scheme, assuming that the row repeats with a single block managed by a single machine, ensures that the row assembly is a local operation, and hence, does not incur any network accesses.</li></ul><p><img src="./week_70.png" width="300"></p><p>Every row group contains three factions: sync marker, metadata, and column data.</p><ul><li>Metadata:</li><li>the number of rows</li><li>the number of columns</li><li>the total number of bytes</li><li>bytes per column</li><li>bytes per value.</li></ul><p>This information is used by a decoder to read the consequent column data.</p><p><strong>Compression:</strong></p><ul><li><p>Metadata is compressed with the run-length encoding to save on the repeated integers;</p></li><li><p>Column data is compressed with a general-purpose codec such as ZIP.</p></li></ul><p>To produce a block of data, you need to buffer a row group within the main memory and transpose it, and then precompute metadata.</p><h3 id="critera-2">Critera</h3><ul><li>Space efficiency:</li><li>RCFiles save a lot of space by exploiting the columnar layout</li><li>data itself is compressed on the block level, increasing space savings.</li><li>Speed: MODERATE TO GOOD,LESS I/O</li><li>ZIP is not the fastest codec in the world.</li><li>Speed is gained by reducing input and output operations, by not reading columns that are not used in further computation.</li><li>Splittable: SPLITTABLE</li><li>sync markers are used to make a splittable format.</li><li>Data types: BYTE STRINGS</li><li>RCFiles are untyped. And values are treated as bytes.</li><li>The reason for that is because RCFiles are mostly used in conjunction with Hive. And Hive deals with all the serialization and deserialization. So there is no need to offload this functionality to the format.</li><li>Extensibility: NO</li><li>Encoded records have a fixed structure. So you need to deal with schema migration by yourself. Once again, this is mostly because Hive rewrites data on schema change.</li></ul><h2 id="parquet">Parquet</h2><ul><li>The most sophisticated columnar format in Hadoop</li><li>Collaborative effort by Twitter &amp; Cloudera</li><li>Supports nested and repeated data</li><li>Exploits many columnar optimizations (such as predicate pruning,per column codecs)</li><li>Optimizes write path</li></ul><h3 id="data-layout-3">Data Layout</h3><p><img src="./week_71.png" width="300"></p><h2 id="conclusion">Conclusion</h2><ul><li>Binary formats are efficient in coding data</li><li>SequenceFile is a reasonable choice for Java users</li><li>Avro is a good alternative for many use cases</li><li>RCFile/ORC/Parquet are best for “wide” tables and analytical workloads</li></ul><h1 id="compression">Compression</h1><ul><li>Block-level compression</li><li>used in SequenceFiles, RCFiles, Parquet</li><li>RCFiles: sync markers and metadata could be used to devise splitting for the dataset without decompressing the entire file.</li><li>applied within a block of data</li><li><p>pros: compression does not hinder the ability to navigate through the file quickly.</p></li><li>File-level compression</li><li>a file was compressed before being written.</li><li>e.g.zip applied to all files in Unix systems.</li><li>applied to the file as a whole</li><li>hinders an ability to navigate through file</li><li>pros: achieves better compression ratios</li><li><p>cons: breaks the file structure and makes the files lesss splitable.</p></li></ul><h2 id="codecs">Codecs</h2><p>In Hadoop stack, there are few popular codecs that you can use with your data: Gzip, Bzip2, LZO, Snappy.</p><p><img src="./week_80.png" width="400"></p><ul><li>Bzip2 is the slowest yet the most efficient</li><li>Snappy is the fastest, yet not so efficient</li></ul><h2 id="when-to-use-compression">When to use compression?</h2><ol type="1"><li><strong>CPU-bound</strong>: Assume your program can process data at a rate 10 MB per second.</li></ol><ul><li>Providing data at higher rates, has no effect on completion time because there is no capacity to perform more work in a unit of time. (Your program spends more time computing rather than doing I/O operations.)</li><li>CPU is your bottleneck</li><li>Results: Adding compression would put more pressure on CPU and increase the completion time.</li></ul><p><img src="./week_81.png" width="400"></p><ol start="2" type="1"><li>I/O bound: your program can process data at a rate 1000 MB per second, while HDFS can stream data only at rate 100 MB per second</li></ol><ul><li><p>your program spends more time waiting for input and output, rather than doing actual computation. <img src="./week_82.png" width="400"></p></li><li><p>Adding compression here would allow HDFS to stream the compressed data at rate 100 MB per second, which transforms to 500 MB per second of uncompressed data, assuming the compression ratio of five <img src="./week_83.png" width="400"></p></li></ul><h2 id="summary">Summary</h2><ol type="1"><li>Raise awareness about application bottlenecks</li></ol><ul><li>CPU-bound : cannot benefit from the compression</li><li>I/O-bound : can benefit from the compression &gt;trade spare CPU time that was wasted in I/O wait for an extra work for compression which resulted in better utilization of both CPU and I/O.</li></ul><ol start="2" type="1"><li>Codec performance vary depending on data, many options available</li></ol><h1 id="conclusion-1">Conclusion</h1><ul><li>Many applications assume relational data model</li><li>File format defines encoding of your data</li><li>text formats are readable, allow quick prototyping, but inefficient</li><li>binary formats are efficient, but more complicated to use</li><li>File formats vary in terms of space efficiency, encoding &amp; decoding speed, support for data types, extensibility</li><li>When I/O bound, can benefit from compression</li><li>When CPU bound, compression may increase completion time</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;data-modeling-and-file-formats&quot;&gt;Data modeling and file formats&lt;/h1&gt;
&lt;p&gt;There is a mismatch between terms used to define business tasks and terms used to describe what HDFS is.&lt;/p&gt;
&lt;p&gt;Data modeling and data management are concerned with these issues.&lt;/p&gt;
    
    </summary>
    
      <category term="Big Data" scheme="https://nancyyanyu.github.io/categories/Big-Data/"/>
    
    
      <category term="Hadoop" scheme="https://nancyyanyu.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>June 2019 | 六月日志</title>
    <link href="https://nancyyanyu.github.io/posts/4935c427/"/>
    <id>https://nancyyanyu.github.io/posts/4935c427/</id>
    <published>2019-07-09T16:18:24.000Z</published>
    <updated>2019-08-02T19:03:11.019Z</updated>
    
    <content type="html"><![CDATA[<p> </p><a id="more"></a><center><img src="./allen.png" width="400"></center><p> </p><center><b>❤ 6月手帳 ❤</b></center><table><colgroup><col style="width: 5%"><col style="width: 68%"><col style="width: 25%"></colgroup><thead><tr class="header"><th style="text-align: left;">Date</th><th>Study</th><th>Workout</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">06/12</td><td>Finished reading ISLR and writing notes; set up blogs on Hexo, githubPage, and NexT; published notes</td><td>/</td></tr><tr class="even"><td style="text-align: left;">06/13</td><td>Practiced tensorflow; implemented CNN, MLP</td><td>Yoga</td></tr><tr class="odd"><td style="text-align: left;">06/14</td><td>Practiced tensorflow; recap RNN, LSTM; tried implementation on high &amp; low level API; wrote data grabbing module</td><td>/</td></tr><tr class="even"><td style="text-align: left;">06/15</td><td>Answer ML questions</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">06/16</td><td>(Travelling all day)</td><td>/</td></tr><tr class="even"><td style="text-align: left;">06/17</td><td>Answer ML questions</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">06/18</td><td>Answer ML &amp; DL questions</td><td>/</td></tr><tr class="even"><td style="text-align: left;">06/19</td><td>~~nothing accomplished</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">06/20</td><td>Study <a href="https://www.linkedin.com/learning/tableau-10-essential-training" target="_blank" rel="noopener">Tableau</a> , write code on phishing problems using SVM, LR, RF, NN</td><td>/</td></tr><tr class="even"><td style="text-align: left;">06/21</td><td>Study <a href="https://www.linkedin.com/learning/tableau-10-essential-training" target="_blank" rel="noopener">Tableau</a> , <a href="https://mode.com/resources/sql-tutorial" target="_blank" rel="noopener">Mode: Basic SQL</a></td><td>Running, Glutei bridge</td></tr><tr class="odd"><td style="text-align: left;">06/22</td><td><a href="https://mode.com/resources/sql-tutorial" target="_blank" rel="noopener">Mode: Intermediate SQL</a></td><td>Yogo</td></tr><tr class="even"><td style="text-align: left;">06/23</td><td><a href="https://mode.com/resources/sql-tutorial" target="_blank" rel="noopener">Mode: Intermediate &amp; Advanced SQL</a></td><td>/</td></tr><tr class="odd"><td style="text-align: left;">06/24</td><td>Leetcode SQL</td><td>/</td></tr><tr class="even"><td style="text-align: left;">06/25</td><td>Leetcode SQL Premium</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">06/26</td><td>Learn Spark</td><td>/</td></tr><tr class="even"><td style="text-align: left;">06/27</td><td>Learn Spark, pyspark. Setup spark,jupyter on AWS EC2</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">06/28</td><td>Learn pyspark, write project code and work on Tableau</td><td>/</td></tr><tr class="even"><td style="text-align: left;">06/29</td><td>Write project report and work on Tableau; learn Tableau</td><td>Hiking</td></tr><tr class="odd"><td style="text-align: left;">06/30</td><td>Write project report and work on Tableau</td><td>Running</td></tr></tbody></table><p> </p><p> </p><center><b>❤ Plan ❤</b></center><table><thead><tr class="header"><th style="text-align: center;">Categories</th><th style="text-align: center;">Content</th><th style="text-align: center;">Progress</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">Machine Learning</td><td style="text-align: center;">Intro to Statistical Learning</td><td style="text-align: center;">■■■■■■■■■■</td></tr><tr class="even"><td style="text-align: center;">Machine Learning</td><td style="text-align: center;"><a href="https://github.com/Sroy20/machine-learning-interview-questions" target="_blank" rel="noopener">Answer ML Interview Questions</a></td><td style="text-align: center;">■■■■■■■■■■</td></tr><tr class="odd"><td style="text-align: center;">Machine Learning</td><td style="text-align: center;"><a href="%5Bhttp://cs229.stanford.edu/syllabus.html%5D(http://cs229.stanford.edu/syllabus.html)">CS229: Machine Learning</a></td><td style="text-align: center;">■□□□□□□□□□</td></tr><tr class="even"><td style="text-align: center;">Deep Learning</td><td style="text-align: center;">Practice Tensorflow via Implementing DL Models</td><td style="text-align: center;">■■■□□□□□□□</td></tr><tr class="odd"><td style="text-align: center;">Deep Learning</td><td style="text-align: center;"><a href="https://github.com/Sroy20/machine-learning-interview-questions" target="_blank" rel="noopener">Answer DL Interview Questions</a></td><td style="text-align: center;">■■■■□□□□□□</td></tr><tr class="even"><td style="text-align: center;">A/B Testing</td><td style="text-align: center;"><a href="https://classroom.udacity.com/courses/ud257" target="_blank" rel="noopener">Udacity</a></td><td style="text-align: center;">■■■□□□□□□□</td></tr><tr class="odd"><td style="text-align: center;">A/B Testing</td><td style="text-align: center;">Studying Bayesian A/B Testing</td><td style="text-align: center;">■□□□□□□□□□</td></tr><tr class="even"><td style="text-align: center;">Product Sense</td><td style="text-align: center;"><a href="https://www.youtube.com/channel/UC6hlQ0x6kPbAGjYkoz53cvA" target="_blank" rel="noopener">Product School</a> Good Machine Learning PM by Google</td><td style="text-align: center;">■■■■■■■■■■</td></tr><tr class="odd"><td style="text-align: center;">Coding</td><td style="text-align: center;"><a href="https://www.teamblind.com/article/New-Year-Gift---Curated-List-of-Top-75-LeetCode-Questions-to-Save-Your-Time-OaM1orEU" target="_blank" rel="noopener">Top 75 LeetCode Questions</a></td><td style="text-align: center;">■■■□□□□□□□</td></tr><tr class="even"><td style="text-align: center;">SQL</td><td style="text-align: center;">Leetcode SQL easy/medium/hard/premium</td><td style="text-align: center;">■■■■■■■■■■</td></tr><tr class="odd"><td style="text-align: center;">SQL</td><td style="text-align: center;">Mode Analytics SQL Practice</td><td style="text-align: center;">■■■■■■■■■■</td></tr><tr class="even"><td style="text-align: center;">SQL</td><td style="text-align: center;">Hackerrank SQL</td><td style="text-align: center;">■■■■■■■□□□</td></tr><tr class="odd"><td style="text-align: center;">Tableau</td><td style="text-align: center;">LinkedIn Course <a href="https://www.linkedin.com/learning/tableau-10-essential-training" target="_blank" rel="noopener">Tableau 10 Essential Training</a></td><td style="text-align: center;">■■■■□□□□□□</td></tr><tr class="even"><td style="text-align: center;">Big Data</td><td style="text-align: center;"><a href="https://www.coursera.org/learn/big-data-essentials/home/welcome" target="_blank" rel="noopener">Big Data Essentials: HDFS, MapReduce and Spark RDD</a></td><td style="text-align: center;">■■■■■■▥□□□</td></tr><tr class="odd"><td style="text-align: center;">Big Data</td><td style="text-align: center;"><a href="https://www.udemy.com/apache-spark-streaming-with-python-and-pyspark/" target="_blank" rel="noopener">Apache Spark Streaming with Python and PySpark</a></td><td style="text-align: center;">■■□□□□□□□□</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; &lt;/p&gt;
    
    </summary>
    
      <category term="Journal" scheme="https://nancyyanyu.github.io/categories/Journal/"/>
    
    
      <category term="Journal" scheme="https://nancyyanyu.github.io/tags/Journal/"/>
    
  </entry>
  
  <entry>
    <title>Analysis of 2018 H-1B Sponsorship for Data Science Employees</title>
    <link href="https://nancyyanyu.github.io/posts/8b70757d/"/>
    <id>https://nancyyanyu.github.io/posts/8b70757d/</id>
    <published>2019-06-28T18:54:55.000Z</published>
    <updated>2019-07-01T00:07:40.381Z</updated>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>The <strong>H-1B</strong> is a visa in the U.S. that allows U.S. employers to temporarily employ foreign workers in specialty occupations. For international students who are trying to find Data science jobs in the U.S., H-1B visa is the most common working visa. Job hunting is stressful, so the tatics show more importance when selecting the companies to apply. Not saying there are no chances in companies with no H-1B sponsor records in 2018, as policies vary in each company every year. Accessing more information and prioritizing tasks in hand are what new grads need to do.</p><p>I want to dig into the data of H-1B case disclosure file in 2018 a little. The data could be found at U.S. Department of Labor's website: https://www.foreignlaborcert.doleta.gov/performancedata.cfm#dis. I would just focus on data related entry level jobs.</p><p>The <strong><em>Jupyter notebook</em></strong> is <a href="https://github.com/nancyyanyu/mini_projects/blob/master/h1b_analysis/h1b_analysis.ipynb" target="_blank" rel="noopener">here</a>.</p><a id="more"></a><h1 id="key-insights">Key Insights</h1><h4 id="companies-in-california-new-jersey-texas-new-york-illinois-like-to-sponsor-data-science-employees-most-which-takes-up-50-of-that-in-the-u.s.">Companies in California, New Jersey, Texas, New York, Illinois like to sponsor data science employees most, which takes up &gt;50% of that in the U.S.</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#count number of data related job H-1B cases in each state</span></span><br><span class="line">data_geo=df[df.JOB_TITLE.isin(data_job_title)].groupby(<span class="string">'EMPLOYER_STATE'</span>)[<span class="string">'CASE_NUMBER'</span>].count().sort_values(ascending=<span class="literal">False</span>).to_frame()</span><br><span class="line">data_geo.to_csv(<span class="string">'./data_geo.csv'</span>)</span><br></pre></td></tr></table></figure><p>For any job titles containing <em>Data</em>, <em>Machine Learning</em>, <em>Research Scientist</em>, <em>Applied Scientist</em>, <em>SQL</em>, I would include them as data science related jobs. I know there are job titles like <em>Product Analyst</em> which are essentially <em>Data Analyst</em> or <em>Data Scientist</em>, but I'll ignore these cases here.</p><p>We can see the top 5 states sponsor <span class="math inline">\(9699\)</span> data science employees which take up more than <span class="math inline">\(50\%\)</span> of that in the U.S.</p><p>Here is a map in Tableau that demonstrates how many data science employees sponsored in each state.</p><iframe src="https://public.tableau.com/shared/MCQ8F254G?:showVizHome=no&amp;:embed=true" width="900" height="600"></iframe><h4 id="besides-big-tech-companies-iccs-sponsor-a-lot">Besides big tech companies, ICCs sponsor a LOT!</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_company=data_company.groupby(<span class="string">'EMPLOYER_NAME'</span>)[<span class="string">'CASE_NUMBER'</span>].sum().sort_values(ascending=<span class="literal">False</span>).to_frame()</span><br></pre></td></tr></table></figure><p>Here is a visualization of companies who sponsor H-1B (Only shows companies with ≥ 20 cases)</p><iframe src="https://public.tableau.com/views/Book4_15618711097530/NumberofDataScienceEmployeesSponsoredinEachCompany?:showVizHome=no&amp;:embed=true" width="900" height="450"></iframe><h4 id="more-than-140-companies-in-washington-state-sponsor-and-they-locate-not-only-in-washington-state">More than 140 companies in Washington state sponsor, and they locate not only in Washington state</h4><p>Since I'm only interested in companies located near Seattle, I extract the list of companies based in Washington state. Here is a visualization showing that for employers in Wahington state,</p><ul><li>how many data science employees sponsered by each of these companies,</li><li>how many cases in each job titles, and</li><li>how many data science employees sponsored by these companies across the nation (not just in Washington state).</li></ul><p>There are several job hunting insights from these data.</p><ul><li>There are 144 companies in Washington state that sponsor data science employees. Most of these companies only sponsor 1 or 2 data science employees. These companies are mostly small-size companies which are usually ignored by job hunters.</li><li>Job titles like <em>Data Scientist</em>, <em>Applied Scientist</em>, <em>Research Scientist</em> are sponsored most. These job titles mostly required more than 5 years of professional experience or PhD degree. That's because these job titles mostly come from Microsoft and Amazon, and they rarely hire entry level people for data science.</li><li>Companies based in Washington state not only sponsored people in the state, but also in other states, like California, Texas, New Jersey, and etc.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_comp=df[(df.EMPLOYER_STATE==<span class="string">'WA'</span>) &amp; (df.JOB_TITLE.isin(data_job_title))].groupby(<span class="string">'EMPLOYER_NAME'</span>)[<span class="string">'CASE_NUMBER'</span>].count().sort_values(ascending=<span class="literal">False</span>).reset_index()</span><br><span class="line">data_comp_wa=pd.merge(data_comp,df[[<span class="string">'EMPLOYER_CITY'</span>,<span class="string">'EMPLOYER_STATE'</span>,<span class="string">'EMPLOYER_NAME'</span>]].drop_duplicates(),on=[<span class="string">'EMPLOYER_NAME'</span>],how=<span class="string">'left'</span>)</span><br></pre></td></tr></table></figure><iframe src="https://public.tableau.com/shared/82398BJSX?:showVizHome=no&amp;:embed=true" width="1000" height="800"></iframe><p><strong>Ref:</strong></p><p>https://en.wikipedia.org/wiki/H-1B_visa</p><p><a href="https://www.foreignlaborcert.doleta.gov/performancedata.cfm#dis" target="_blank" rel="noopener">U.S. Department of Labor</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The &lt;strong&gt;H-1B&lt;/strong&gt; is a visa in the U.S. that allows U.S. employers to temporarily employ foreign workers in specialty occupations. For international students who are trying to find Data science jobs in the U.S., H-1B visa is the most common working visa. Job hunting is stressful, so the tatics show more importance when selecting the companies to apply. Not saying there are no chances in companies with no H-1B sponsor records in 2018, as policies vary in each company every year. Accessing more information and prioritizing tasks in hand are what new grads need to do.&lt;/p&gt;
&lt;p&gt;I want to dig into the data of H-1B case disclosure file in 2018 a little. The data could be found at U.S. Department of Labor&#39;s website: https://www.foreignlaborcert.doleta.gov/performancedata.cfm#dis. I would just focus on data related entry level jobs.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;&lt;em&gt;Jupyter notebook&lt;/em&gt;&lt;/strong&gt; is &lt;a href=&quot;https://github.com/nancyyanyu/mini_projects/blob/master/h1b_analysis/h1b_analysis.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="Project" scheme="https://nancyyanyu.github.io/categories/Project/"/>
    
    
  </entry>
  
  <entry>
    <title>Hadoop MapReduce: How to Build Reliable System from Unreliable Components</title>
    <link href="https://nancyyanyu.github.io/posts/df5ae10a/"/>
    <id>https://nancyyanyu.github.io/posts/df5ae10a/</id>
    <published>2019-06-27T23:15:02.257Z</published>
    <updated>2019-06-28T21:02:04.140Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>This is course note of <a href="https://www.coursera.org/learn/big-data-essentials" target="_blank" rel="noopener">Big Data Essentials: HDFS, MapReduce and Spark RDD</a></p></blockquote><h1 id="unreliable-components">Unreliable Components</h1><p>3 different unreliable components in distributed systemsnodes: nodes, links and clock. Distributed computational systems: - built from unreliable components - Cluster nodes can break any time because of power supply, disk damages, overheated CPUs, and so on</p><p>3 different unreliable components in distributed systemsnodes: nodes, links and clock.</p><a id="more"></a><h2 id="nodes">Nodes</h2><p><strong>3 types of node failures:</strong></p><ul><li><strong>Fail-Stop</strong>: if machines get out of service during a computation then you have to have an external impact to bring system back to a working state. &gt; A system administrator should either</li><li>fix the node and reboot the whole system or part of it. Or <img src="./week2_0.png" width="550"></li><li>retire the broken machine and reconfigure the distributed system</li></ul><p><img src="./week2_1.png" width="550"></p><ul><li><strong>Fail-Recovery</strong>: during computations, notes can arbitrarily crash and return back to servers.</li><li>doesn't influence correctness and success of computations</li><li>no external impact necessary to reconfiguring the system at such events. &gt; if a hard drive was damaged, then a system administrator can physically change the hard drive.After reconnection, this node will be automatically picked up by a distributed system. And it will even be able to participate in current computations.</li></ul><p><img src="./week2_2.png" width="550"></p><ul><li><strong>Byzantine</strong>: A distributed system is robust Byzantine failures if it can correctly work despite some of the nodes behaving out of protocol. <img src="./week2_3.png" width="550"></li></ul><blockquote><p>If you are developing a financial system, then you are likely required to deal with these types of failures to protect your customers and your business.</p></blockquote><h2 id="links">Links</h2><p><strong>3 types of links:</strong></p><ul><li><p>perfect: all the sent messages must be delivered and received without any modification into the same portal. <img src="./week2_4.png" width="550"></p></li><li><p>fail-loss: some part of the messages can be lost but the probability of message loss does not depend on contents of a message.</p></li></ul><blockquote><p>the well-known TCP/IP protocol tries to solve this problem by re-transmitting messages if they were not received.</p></blockquote><p><img src="./week2_5.png" width="350"> - byzantine: some messages can be filtered according to some rule, some messages can be modified, and some messages could be created out of nowhere</p><h2 id="clocks">Clocks</h2><p><strong>clock synchronization problem: </strong></p><ul><li>clock skew: the time can be different on different machines</li><li>clock drift: there can be a different clock rate <img src="./week2_6.png" width="300"></li></ul><p><strong>Logical clocks</strong> help to track happened before events and therefore, order events to build reliable protocols.</p><h2 id="asynchronous-systems">[A]synchronous systems</h2><p>Systems can be divided into synchronous and asynchronous.</p><p><strong>synchronous</strong>: - every network packet should be delivered within a limited time bound. - Clock drift is limited in size - each CPU instruction is also limited in time.</p><h3 id="examples-of-different-distributed-systems">Examples of different distributed systems</h3><ol type="1"><li>fail-stop, perfect link, and synchronous model</li></ol><blockquote><p>A parallel computational model and widely adopted by supercomputers where many processors connected by a local high speed computer bus.</p></blockquote><ol start="2" type="1"><li><p>fail-recovery, fair-loss link, and asynchronous model (this course focus)</p></li><li><p>byzantine-failure, byzantine link, and asynchronous model</p></li></ol><blockquote><p>computational components spread across the globe of unreliable and untrusted network connections. The common representative of this model is grid computing.</p></blockquote><h1 id="mapreduce">MapReduce</h1><p>There are two phases during computation, map and reduce:</p><p><strong>map</strong>:apply the same function to each element of your collection. <img src="./week2_8.png" width="550"></p><p><strong>reduce</strong>:Reduce operator causes a sequence of elements by applying the following procedure iteratively. - As soon as you have more than one element in the sequence, then you get the first two and combine them to one element by applying the provided function. - Reduce function computes the value from left to right. &gt;Be careful about reducing functions that are not associative.</p><p><img src="./week2_9.png" width="550"></p><p><strong>MapReduce</strong>:the class of problems that you can solve with arbitrary map and reduce functions is quite big. <img src="./week2_10.png" width="550"></p><h1 id="distributed-shell">Distributed Shell</h1><ul><li>run a distributed <strong>grep</strong> as a MapReduce job. <img src="./week2_12.png" width="550"></li><li>Map will be equivalent to grep</li><li>Reduce will be None.</li></ul><p>In MapReduce applications, you don't always need map or reduce function.</p><ul><li><p>run a distributed <strong>head</strong> as a MapReduce job. <img src="./week2_13.png" width="550"> You can just retrieve the necessary data with HDFS client. To get these data with MapReduce job, get actual information such as, block index and size in lines on map phase to complete the task correctly --&gt; <strong>head and tweaks</strong>.</p></li><li>run a distributed <strong>wc</strong> as a MapReduce job. <img src="./week2_14.png" width="550"></li><li>The output from the map will be a tuple of the size 3: number of lines, words, and bytes.</li><li><p>Sum the items by components, so the reduced function will be an extended add operator.</p></li></ul><h2 id="wordcound-example">WordCound Example</h2><p><em>find the most popular words in the Wikipedia with MapReduce</em></p><ol type="1"><li>count how many times each word appear in a data set &gt; wikipedia.dump | tr ' ' '' | sort | uniq -c</li></ol><p>Map--&gt;Shuffle&amp;Sort--&gt;Reduce - Map: the text is split in two words - tr - Shuffle&amp;Sort: words are distributed to a reduce phase in a way that reduce functions can be executed independently on different machines - sort - Reduce: uniq</p><p><img src="./mr2.png" width="600"> <strong>external sorting</strong>:If the data is sorted, and can be read as a stream, then uniq-c will be working correctly. To make data sorted, you only need to have enough disk space. The algorithm for this is called, external sorting.</p><p>All input and output of map and reduce functions should be a key value pair.</p><p><strong>MapReduce Formal Model</strong>:</p><p>map: (key, value) → (key, value)</p><p>reduce: (key, value) → (key, value)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ cat -n wikipedia.dump | tr <span class="string">' '</span> <span class="string">'\n'</span>| sort | uniq -c</span><br></pre></td></tr></table></figure><ul><li>read data and get pairs with a line number, and line content.</li></ul><blockquote><p>cat -n wikipedia.dump: [(line_no, line), …]</p></blockquote><ul><li><p>on a map phase,you ignore line numbers and split lines into words. &gt; tr ' ' '': (-, line) —&gt; [ (word, 1), … ]</p></li><li><p>You can add value one to each output it worked. So, it means that you have seen this word once by reading a line from left to right.</p></li><li><p>a shuffle and sort phase where you spread the words by the hashes.So, you can process them on independent reducers. &gt; sort: Shuffle &amp; Sort</p></li><li><p>count how many figures of 1 you have for each word, and sum them up to get an answer. &gt; uniq -c: (word, [1, …]) —&gt; (word, count)</p></li></ul><p><img src="./mr4.png" width="600"></p><p><strong>3 types of key value pairs</strong>: - Key value pairs for the input data <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat -n wikipedia.dump: [(line_no, line), …]</span><br><span class="line"><span class="built_in">read</span>: [(k_in, v_in), …]</span><br></pre></td></tr></table></figure></p><ul><li><p>key value pairs for the intermediate data <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tr ‘ ‘ ‘\n’: (-, line) —&gt; [ (word, 1), … ]</span><br><span class="line">map: (k_in, v_in) —&gt; [(k_interm, v_interm), …]</span><br></pre></td></tr></table></figure></p></li><li><p>key value pairs for the output data <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Shuffle &amp; Sort: sort and group by k_interm</span><br><span class="line">uniq -c: (word, [1, …]) —&gt; (word, count)</span><br><span class="line">reduce: (k_interm, [(v_interm, …)] ) —&gt; [(k_out, v_out), …]</span><br></pre></td></tr></table></figure></p></li></ul><p><img src="./mr6.png" width="600"></p><h2 id="fault-tolerence">Fault Tolerence</h2><p>In a distributed file system, you store information with duplication in order to overcome node failures. MapReduce framework should also provide robustness against node failures during the job execution.</p><p>In a MapReduce job, you will have: 1. Master program: control the execution <img src="./ft1.png" width="600"></p><ol start="2" type="1"><li><p>Master program will launch mappers to process input blocks or splits of data. <img src="./ft2.png" width="600"></p></li><li><p>To overcome the issues of correction execution mappers, there is no harm in re-executing mapper against the same data because you expect map function to be deterministic. As soon as you work on top of HDFS, you have a replica of this data on other nodes. So, you could assign another worker to a execute mapper against these data, and application master will do all this magic for you. <img src="./ft3.png" width="600"></p></li><li><p>If a worker running reducer dies, you can shuffle and sort data for this particular reducer to another worker. <img src="./ft4.png" width="600"></p></li><li><p>Shuffled and sorted data are stored on local disks instead of the distributed file system <img src="./ft5.png" width="600"></p></li></ol><blockquote><p><em>You only need to provide deterministic map and reduce function</em></p></blockquote><h2 id="hadoop-mapreduce-framework">Hadoop MapReduce framework</h2><p><strong>Job</strong> One MapReduce application is a job.</p><p><strong>Task</strong> a task can be either mapper or reducer.</p><h3 id="first-version">First version</h3><p><img src="./ft7.png" width="600"></p><ul><li>JobTracker: one global JobTracker to direct execution of MapReduce jobs.</li><li>located on one high-cost and high-performance node with HDFS namenode.</li><li>a single point failure</li><li>TaskTrackers</li><li>located once per every node where you store data, or where datanode daemon is working</li><li>spawns workers from mapper or reducer</li></ul><h3 id="yarn">YARN</h3><blockquote><p>Yet Another Resource Negotiation</p></blockquote><p><img src="./ft8.png" width="600"></p><ul><li>TaskTracker is subtituted by NodeManagers who can provide a layer of CPU and RAM containers.</li><li>ResourceManager overseas NodeManagers, and client request resources for execution</li><li>MapReduce applications can work on top of this resource layer.</li><li>There is no concept such as a global JobTracker because application master can start on any node.</li><li>All of these enable Hadoop to share resources dynamically between MapReduce and other parallel processing frameworks.</li></ul><p><img src="./mrf.png" width="600"></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;This is course note of &lt;a href=&quot;https://www.coursera.org/learn/big-data-essentials&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Big Data Essentials: HDFS, MapReduce and Spark RDD&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;unreliable-components&quot;&gt;Unreliable Components&lt;/h1&gt;
&lt;p&gt;3 different unreliable components in distributed systemsnodes: nodes, links and clock. Distributed computational systems: - built from unreliable components - Cluster nodes can break any time because of power supply, disk damages, overheated CPUs, and so on&lt;/p&gt;
&lt;p&gt;3 different unreliable components in distributed systemsnodes: nodes, links and clock.&lt;/p&gt;
    
    </summary>
    
      <category term="Big Data" scheme="https://nancyyanyu.github.io/categories/Big-Data/"/>
    
    
      <category term="Hadoop" scheme="https://nancyyanyu.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop MapReduce Streaming Application in Python</title>
    <link href="https://nancyyanyu.github.io/posts/f53c188b/"/>
    <id>https://nancyyanyu.github.io/posts/f53c188b/</id>
    <published>2019-06-27T23:13:45.786Z</published>
    <updated>2019-06-28T21:12:05.710Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>This is course note of <a href="https://www.coursera.org/learn/big-data-essentials" target="_blank" rel="noopener">Big Data Essentials: HDFS, MapReduce and Spark RDD</a></p></blockquote><h1 id="streaming">Streaming</h1><p>In a Hadoop MapReduce application:</p><a id="more"></a><ol type="1"><li><p>you have a stream of input key value pairs. <img src="./mrs1.png" width="600"></p></li><li>you process this data with a map function, and transform this data to a list of intermediate key value pairs. <img src="./mrs2.png" width="600"></li><li>This data is aggregated by keys during shuffle and sort phase. <img src="./mrs3.png" width="600"></li><li><p>you process data provided in reduce function <img src="./mrs4.png" width="600"></p></li></ol><p>If you want to plug in an external program, then it is natural to communicate via standard input and output channels. You have to <em>implement your own mappers and reducers</em> instead of just map and reduce functions. <img src="./mrs5.png" width="600"></p><h2 id="what-is-hadoop-streaming">What is Hadoop Streaming?</h2><p>Hadoop streaming is a utility that comes with the Hadoop distribution. The utility allows you to create and run Map/Reduce jobs with any executable or script as the mapper and/or the reducer. For example: <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$HADOOP_HOME</span>/bin/hadoop  jar <span class="variable">$HADOOP_HOME</span>/hadoop-streaming.jar \</span><br><span class="line">    -input myInputDirs \</span><br><span class="line">    -output myOutputDir \</span><br><span class="line">    -mapper /bin/cat \</span><br><span class="line">    -reducer /bin/wc</span><br></pre></td></tr></table></figure></p><h2 id="cli-command">CLI command</h2><h3 id="execute-the-map-phase">Execute the map phase</h3><ol type="1"><li>define the path to the streaming jar</li></ol><ul><li>Where this jar is located depends on your cluster installation</li></ul><ol start="2" type="1"><li>execute yarn application</li><li>provide a number of arguments</li></ol><ul><li>mapper: bash command wc-l.</li><li>numReducesTasks: For now, you don't want to execute any reducers,</li><li>input: specify what HDFS folder or file you are going to process</li><li>output: specify an HDFS folder for output.</li></ul><p><img src="./s1.png" width="600"></p><p>If you already have this HDFS folder, then you have to remove it beforehand. Otherwise, you wouldn't be able to launch mapreduce jar.</p><p><img src="./s2.png" width="600"> <img src="./s3.png" width="600"></p><p><strong>HDFS folder internals after a successful execution</strong> - only two mappers were executed - These two files contain information about the processed articles. - If you sum them up, then you get 4,100 Wikipedia articles in our assemble. <img src="./s4.png" width="600"></p><h3 id="execute-the-reduce-phase">Execute the reduce phase</h3><ol type="1"><li>For each input line, sum it up to the variable--&gt;line count</li><li>provide reducer which aggregates the number of articles from all the mappers--&gt;awk</li><li>specified exactly one reducer--&gt;guarantee that the only reducer will be working, you have exactly one value in the output.</li><li>print it out</li></ol><p><img src="./s5.png" width="600"></p><p>Wrap special characters in the Shell map and reduce commands into as Shell script.</p><p><img src="./s6.png" width="600"></p><p>Summary: write and call MapReduce Shell streaming application. <img src="./s7.png" width="600"></p><h1 id="streaming-in-python">Streaming in Python</h1><ul><li>If you have more than one file to distribute over the workers, then you can specify them as a comma separated list.</li><li><p><img src="./sp1.png" width="500"></p></li><li><p>I have also flipped the order of arguments.The order of some arguments is important. They are the so-called <strong>generic options</strong>.</p></li></ul><p><img src="./sp2.png" width="600"></p><ul><li>When released an HDFS folder with output, you see only one file.</li><li><p>it is only natural to see an empty output by running this streaming mapReduce job. <img src="./sp3.png" width="600"></p></li><li><p>double check it with an hdfs minus text command. <img src="./sp4.png" width="600"></p></li></ul><p><strong>mapper.py</strong> <img src="./sp5.png" width="600"></p><p><strong>reducer.py</strong> <img src="./sp6.png" width="600"> <img src="./sp9.png" width="600"></p><h1 id="wordcount-in-python">WordCount in Python</h1><p>learn how to define key value pairs for the input and output streams.</p><h2 id="map">Map</h2><p><img src="./wc1.png" width="600"> 1. For each input line, you split it into key and value where the article ID is a key, and the article content is a value.</p><ol start="2" type="1"><li><p>Then you split the content into words, and finally output intermediate key value pairs. <img src="./wc2.png" width="600"></p></li><li>validate mapper against a small dataset.</li></ol><ul><li>I don't want to execute any reducer. So, I set minus numReduceTasks argument to zero.</li><li><p>In the output folder, you'll see several map output files. According to the random nature, you don't know which of the mappers processed the first split of data. <img src="./wc3.png" width="600"></p></li><li>As you can see the first chunk of data was processed by the second mapper.</li><li><p>And there is no article ID in the output. <img src="./wc4.png" width="600"></p></li></ul><ol start="4" type="1"><li>If you see one reducer with default implementation which does nothing, then shuffle and sort phase will be executed. And you should see the sorted output.</li></ol><ul><li>Be cautious to use one reducer with big datasets <img src="./wc5.png" width="600"></li><li>To get rid of all of the configuration characters. You can use Python regular expression module. I use here Capital W, which helps to ignore all bold characters. <img src="./wc6.png" width="600"> <img src="./wc7.png" width="600"></li></ul><h2 id="reduce">Reduce</h2><blockquote><p>have data aggregated by key. But how does reading the input streaming data on the reducer side look like.</p></blockquote><p><img src="./wc8.png" width="600"></p><p>On reduce phase, you have sorted stream of key value pairs - stream is sorted by keys--&gt;you can iterate line by line, and keep track of the current keys to aggregate values. <img src="./wc9.png" width="600"></p><h3 id="code-snippet">Code Snippet</h3><p><img src="./mrs10.png" width="600"></p><p>If you execute the whole mapper reduce job with one reducer, then you get only one file in the output. <img src="./mrs11.png" width="600"></p><p>Take a look at the content of this file, then you see the data sorted by keys, and there is only one value per each key. <img src="./mrs12.png" width="600"></p><p><strong>Remove <em>numReduceTasks</em> from the argument list</strong> - my previous job will have an arbitrary number of reducers - several files in the output HDFS folder. - In each file, the data is sorted by keys, but the keys are not globally sorted as I have shuffled between reducers.</p><p><img src="./mrs13.png" width="600"></p><h1 id="distributed-cache">Distributed Cache</h1><p><strong>Distributed Cache</strong> is a facility provided by the Hadoop MapReduce framework. It cache files when needed by the applications. It can cache read only text files, archives, jar files etc. Once we have cached a file for our job, Hadoop will make it available on each datanodes where map/reduce tasks are running.</p><p>When you call MapReduce application, NodeManagers provide containers for execution. And there can be several containers on each NodeManager. If you provide files then each of this files will be copied once by each node before any task execution. <img src="./dc2.png" width="600"></p><p>So each container can access this data locally via created synlinks. - distributed cache file should not be modified by the application while the job being executed. <img src="./dc3.png" width="600"></p><h2 id="ways-to-distribute-files">3 ways to distribute files</h2><p><img src="./dc4.png" width="600"></p><ul><li>-files</li><li>-archives: provides the ability to better utilize network profile transmission</li><li>all archives will be un-parked on worker nodes. So, you will be able to work with profiles from mapper or reducer's grid.</li><li>-libjars: distribute files in JARs(Java Archive). You will not pay attention to this option at all, as this course is about Python development, not Java.</li></ul><h2 id="archives">Archives</h2><ol type="1"><li>create two files - The text files will contain female and male names.</li><li><p>create a tar archive with the following CLI comment. <img src="./dv6.png" width="600"></p></li><li><p>execute the following MapReduce application <img src="./dc7.png" width="600"></p></li></ol><p>Python code:</p><ul><li><p>mapper.py <img src="./dc8.png" width="600"></p></li><li><p>reducer.py <img src="./dc10.png" width="600"></p></li></ul><h1 id="environments-counters">Environments, Counters</h1><p>You can get job configuration options through environment variables. When you launch MapReduce application, hadoop framework will assign splits of data to available workers. you can access this data from your scripts. <img src="./e1.png" width="600"> <img src="./e2.png" width="600"> You can also access task id within map, or reduce phase with the following environment variables. 1. get an absolute task id, and is usually available as a task name on job tracker ui. 2. access the relative order of the task within map or reduced pace. In this example, it would be 1 and 8. <img src="./e3.png" width="600"></p><p><img src="./e4.png" width="600"></p><p>In addition to the existing environment variables, you can provide your own: - Write a generic mapper for word cound problem - use regular expression to parse words. <img src="./e5.png" width="600"></p><ul><li>use -D flag to provide arbitrary environment variables</li><li>Here, I provided a regular expression to parse words ending with numbers <img src="./e6.png" width="600"></li></ul><h2 id="communication">Communication</h2><ol type="1"><li>Reading environment variables is a one-way communication between Hadoop MapReduce and scripts.</li><li>A backward communication channel between framework--provide information about your task progress <img src="./e7.png" width="600"></li></ol><p>2 types of information: - status - provide an aribtrary message in status for each task execution - normally used to inform a user about the processing stage(startup, run, or cleanup) <img src="./e8.png" width="600"></p><ul><li><strong>counters</strong></li><li>accumulate statstics of a map and the reduced executions</li></ul><p>In word count example, you can easily count the number of words, providing 1. a counter family name--&gt;group 2. a counter name 3. the value you'd like to add to the counter</p><p><img src="./e9.png" width="600"> <img src="./e10.png" width="600"></p><h1 id="testing">Testing</h1><h2 id="unit-testing">Unit Testing</h2><p>Python Testing Tools: Taxonomy, pytest</p><h3 id="example-of-unit-testing">Example of unit testing</h3><p><img src="./t1.png" width="600"></p><p>Aim of testing: to validate your scripts in a reproducible environment as close as possible to the production of one.</p><h2 id="integration-testing">Integration Testing</h2><p>You can validate the whole pipeline with mapper and reducer, or independent mapper or reducer with the hand-crafted input. This type of testing is referred to as an integration testing, because you validate how our mappers and reducers scripts are integrated with Hadoop MapReduce streaming API.</p><p><img src="./t2.png" width="600"></p><p>Drawback: it will be working out of the box if your scripts rely on MapReduce job configuration options</p><h2 id="system-testing">System Testing</h2><p>System testing: execute the whole pipeline end to end</p><p>Hadoop MapReduce framework provides an empty config which you can use for HDFS, MapReduce and the Yarn clients.</p><blockquote><p>see <em>locate</em> CLI to <em>find</em> a path to an empty config on your Hadoop installation.</p></blockquote><p><img src="./t3.png" width="600"></p><p>If you provide a Hadoop empty config, then you execute the whole MapReduce application in a <strong>standalone mode</strong>. - an HDFS client points out to a local file system - a node manager is working on the same node - your streaming scripts will be able to communicate with MapReduce framework via environment variables. - You will be able to read configuration of variables and validate counters correctness.</p><p><img src="./t4.png" width="600"></p><h2 id="acceptance-testing">Acceptance Testing</h2><ul><li>Validation of your streaming scripts against a sample dataset to</li><li>find bugs early without wasting your time and CPU cycles.</li><li>Validation against big datasets and measuring performance or efficiency of your solution <img src="./t5.png" width="600"></li></ul><h1 id="summary">Summary</h1><ul><li>you know how to use Python unit testing</li><li>you know how to emulate MapReduce locally with <code>(cat | map | sort | reduce)</code></li><li>you know how to run MapReduce in a standalone mode ( <code>hadoop/conf.empty</code>)</li><li>you know why you need to execute MapReduce against sample datasets</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;This is course note of &lt;a href=&quot;https://www.coursera.org/learn/big-data-essentials&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Big Data Essentials: HDFS, MapReduce and Spark RDD&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;streaming&quot;&gt;Streaming&lt;/h1&gt;
&lt;p&gt;In a Hadoop MapReduce application:&lt;/p&gt;
    
    </summary>
    
      <category term="Big Data" scheme="https://nancyyanyu.github.io/categories/Big-Data/"/>
    
    
      <category term="Hadoop" scheme="https://nancyyanyu.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop MapReduce Application Tuning</title>
    <link href="https://nancyyanyu.github.io/posts/bbe483af/"/>
    <id>https://nancyyanyu.github.io/posts/bbe483af/</id>
    <published>2019-06-27T23:12:33.793Z</published>
    <updated>2019-06-28T21:20:14.467Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>This is course note of <a href="https://www.coursera.org/learn/big-data-essentials" target="_blank" rel="noopener">Big Data Essentials: HDFS, MapReduce and Spark RDD</a></p></blockquote><p><strong>The world of the efficient MapReduce is based on three whales. Combiner, Partitioner, and Comparator.</strong></p><h1 id="combiner">Combiner</h1><p>To change the usage of these IO operations and network bandwidth, you can use <strong>combiner</strong> to squash several items into one.</p><a id="more"></a><p><img src="./cb1.png" width="600"></p><p>Combiner: - expects an input in the form of the reducer input and it has the same output signature as a mapper. - can be applied arbitrarily number of times between map and reduce phases. - A combiner should not change a type and format of a key and value</p><p><img src="./cb2.png" width="600"></p><p>In the word count application, there is no difference between the combiner and the reducer. So you can easily call it with the following arguments.</p><p><img src="./cb3.png" width="600"></p><h2 id="example-mean-value">Example: Mean Value</h2><blockquote><p>count how many times on average you see a word in an article</p></blockquote><ul><li>Mapper.py: print a pair containing the number of articles processed and the cumulative amount of words</li></ul><p><img src="./cb4.png" width="600"></p><ul><li>Reducer: memorize not only the number of occurrences but also the number of articles.</li></ul><p><img src="./cb5.png" width="600"></p><ul><li>Combiner: It could help us to speed up calculations, for the whole MapReduce job, as you will use less IO resources <img src="./cb61.png" width="600"></li></ul><h2 id="example-median">Example: Median</h2><p>you have to get the whole dataset in place. So, the combiner is out of help in this case.</p><p>it's not always possible to speed up calculations with the combiner. <img src="./cb7.png" width="600"></p><h1 id="partitioner">Partitioner</h1><p>Partitioner is used to calculate a reducer index for each (key, value) pair</p><p>Collocation 搭配字詞</p><blockquote><p>To find collocations of size two in a data sets, you need to count Bigrams.</p></blockquote><p><strong>Mapper:</strong> The following mapper will emit a sequence of bigrams followed aggregation during their use phase.</p><p><img src="./pt1.png" width="600"></p><p>Hadoop MapReduce frame work will distribute and sort data by the first word. Because everything before the first tab character is considered a key.</p><p><strong>Reducer:</strong> To sort data by the second word, you can update reducer.py to count all bigrams for the first corresponding word in memory--&gt;memory consuming. <img src="./pt2.png" width="600"></p><p>output of these MapReduce application which validates that New York bigram is a collocation <img src="./pt3.png" width="600"></p><p>In addition to the unnecessary memory consumption there would be uneven lot on the reducers.</p><p>The benefit of MapReduce: it provides functionality to <strong>parallelized work.</strong></p><p>e.g. In a default scenario you will have the far more lot on the reducer that will be busy processing this article <code>The</code>. But you have no need to send all of the bigrams starting with <code>The</code> to one reducer as you do calculations for each pair of words independently. <img src="./pt4.png" width="600"></p><p>** partitioner**: &gt; In this case you would like to split the line into key value pairs by the second tab character.</p><p><img src="./pt5.png" width="600"></p><ul><li>complete this MapReduce job faster due to better parallelism.</li><li>bigrams starting with any arbitrary word allocated in different files.</li></ul><h2 id="example-ipv4-network-addresses">Example: IPv4 network addresses</h2><p>IPv4 address contains four numbers called Octets delimited by dots.</p><p><img src="./pt6.png" width="600"></p><p>You can specify what a delimiter is and set number of fields related to a key. MapReduce framework will substitute this particular delimiter between num and num+1 fields to a tab character without any changes in your streaming scripts.</p><p>In this example, - I would like to split the output from the streaming mapper by the first dot. - And from the reducers stream and output, I substituted the next but one dot with a key value MapReduce delimiter, which is a tab character.</p><p><img src="./pt7.png" width="600"></p><blockquote><p>To partition IPv4 addresses by the second character of a first octet</p></blockquote><ul><li>specify the field index and the starting character index in the start position</li><li>specify the field index and the character index in the end position.</li></ul><p><img src="./pt8.png" width="600"></p><p>I have to set a special partitioner called KeyFieldBasedPartitioner.</p><p><em>Bigger picture</em>: the whole pipeline of MapReduce application execution</p><p><img src="./pt9.png" width="400"></p><p>You have mappers at the top. Then the data goes through combiners, then it is distributed by the partitioner. Finally there is a reduced phase.</p><h1 id="comparator">Comparator</h1><p>All the keys in MapReduce implement writable comparable interface.</p><p><strong>Comparable</strong> means that you can specify the rule according to which one key is bigger than another.</p><p>By default, you have the keys sorted by increasing order. For some applications, you would like to store them in a reverse order.</p><p>comparator compares records for sorting, it is not an optimization</p><h2 id="example-ipv4-network-addresses-1">Example: IPv4 network addresses</h2><blockquote><p>To sort octets of IPV4 address by the second octet in an increasing order, and by the third octate in a reverse order.</p></blockquote><p><img src="./cp1.png" width="450"></p><h1 id="speculative-execution-backup-tasks">Speculative Execution / Backup Tasks</h1><blockquote><p>reduce your total waiting time by a factor of two</p></blockquote><p>One of the most common problems that causes a MapReduce application to wait longer for a job completion is a <strong>straggler</strong>--a machine that takes an unusually long time to complete one of the last few tasks in the computation. <img src="./se2.png" width="450"></p><p>The solution of straggler: <strong>Backup Tasks</strong></p><p><img src="./se1.png" width="450"> Due to the deterministic behavior of the Mapper and Reducer, you can easily re-execute straggler body of work on other node.</p><p>In this case, the worker which processes data, they first outputs data to a distributed file system. All the other concurrent executions will be killed.</p><p>Of course, the MapReduce framework is not going to have a copy for each running task. It is only used when a MapReducer application is close to completion.</p><h2 id="tuning">Tuning:</h2><p><img src="./se3.png" width="450"></p><ul><li>Speculative Execution is set by default to true. set these flags to false if you don't allow multiple instances of some map or reduce task to be executed in parallel.</li></ul><p><img src="./se4.png" width="450"></p><ul><li>These two flags can be used to specify the allowed number of running backup tasks at each point in the stream of the time and overall.</li></ul><p><img src="./se5.png" width="450"></p><ul><li>you can tune timeouts in milliseconds that will limit the time of your waiting till the next round of speculation.</li></ul><p>If you have successfully managed to speed up the process with speculation, then you should be able to find concurrent tasks killed by speculation on job trigger. <img src="./se7.png" width="550"></p><h1 id="compression">Compression</h1><p>You can balance the process and capacity by the data <strong>compression</strong>.</p><p>Data compression is essentially a trade-off between - the disk I/O required to read and write data - The network bandwidth required to send data across the network - the in-memory calculation capacity(speed and usage of CPU and RAM). - ability of archives to be splitted by Hadoop</p><p>The correct balance of these factors depends on the characteristics of your cluster, your data, your applications, or usage patterns, and the weather forecast.</p><blockquote><p>Data located in HDFS can be compressed. There is a shuffle and sort phase between map and the reduce where you can compress the <em>intermediate data.</em>--&gt;optimization</p></blockquote><p><img src="./comp1.png" width="600"></p><ul><li>Splittable column means that you can cut a file at any place and find the location for the next or the previous valid record.</li><li>Native libraries that provide implementation of compression and decompression functionality, usually also support an option to choose a trade-off between speed or space optimization.</li></ul><p><strong>Pros &amp; Cons:</strong> - gzip file is a deflate file with extra headers and a footer. - bzip is more aggressive for space requirements, but consequently, it's slower during the compression. - lzo files can be used where you read data far more frequently than write. - You can provide index files for lzo files to make them splittable. - Snappy,even more faster decompression, but you will only be able to split this file records.</p><p>A <strong>Hadoop codec</strong> is an implementation of a compression, decompression algorithm.</p><p><img src="./comp2.png" width="600"></p><p>You can specify the compression parameters for intermediate data for output or for both</p><p><img src="./comp3.png" width="600"></p><p><strong>Rules of thumb</strong>: 1. <em>gzip</em> or <em>bzip</em> are a good choice for <strong>cold data</strong>, which is accessed infrequently. 2. <em>bzip</em> produce more compression than <em>gzip</em> for some kinds of files at the cost of some speed when compressing and decompressing. 3. <em>Snappy</em> or <em>lzo</em> are a better choice for <strong>hot data</strong>, which is accessed frequently. 4. <em>Snappy</em> often performs better than <em>lzo</em>. 5. For MapReduce, we can use bzip and lzo formats, if you would like to have your data splittable. 6. <em>Snappy</em> and <em>gzip</em> formats are not splittable at file level compression. But you can use block level compression and splittable container formats such as <em>Avro</em> or <em>SequenceFile</em>--&gt; process the blocks in parallel using MapReduce</p><p><img src="./comp4.png" width="600"></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;This is course note of &lt;a href=&quot;https://www.coursera.org/learn/big-data-essentials&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Big Data Essentials: HDFS, MapReduce and Spark RDD&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;The world of the efficient MapReduce is based on three whales. Combiner, Partitioner, and Comparator.&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&quot;combiner&quot;&gt;Combiner&lt;/h1&gt;
&lt;p&gt;To change the usage of these IO operations and network bandwidth, you can use &lt;strong&gt;combiner&lt;/strong&gt; to squash several items into one.&lt;/p&gt;
    
    </summary>
    
      <category term="Big Data" scheme="https://nancyyanyu.github.io/categories/Big-Data/"/>
    
    
      <category term="Hadoop" scheme="https://nancyyanyu.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Apache Spark: Advanced Topics</title>
    <link href="https://nancyyanyu.github.io/posts/6d5a8880/"/>
    <id>https://nancyyanyu.github.io/posts/6d5a8880/</id>
    <published>2019-06-27T23:09:33.264Z</published>
    <updated>2019-06-28T21:37:11.123Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>This is course note of <a href="https://www.coursera.org/learn/big-data-essentials" target="_blank" rel="noopener">Big Data Essentials: HDFS, MapReduce and Spark RDD</a></p></blockquote><h1 id="execution-scheduling">Execution &amp; Scheduling</h1><p><strong>SparkContext</strong> - When creating a Spark application, the first thing you do is create a SparkContext object, which tells Sparks how to access a cluster. - The context, living in your driver program, coordinates sets of processes on the cluster to run your application.</p><a id="more"></a><p><img src="./exe1.png" width="600"></p><ul><li>The SparkContext object communicates the Cluster Manager to allocate executors.</li><li>The Cluster Manager is an external service for acquiring resources on a cluster. For example, YARN, Mesos or a standalone Spark cluster.</li><li>once the context has allocated the executors, it communicates directly with them and schedules tasks to be done.</li></ul><h2 id="jobs-stages-tasks">Jobs, stages, tasks</h2><ul><li><strong>Task</strong> is a unit of work to be done</li><li><strong>Tasks</strong> are created by a <strong>job scheduler</strong> during the scheduling of a job for every job stage. And every task belongs to the job stage.</li><li><strong>Job</strong> is spawned in response to a Spark action</li><li><strong>Job</strong> is divided in smaller sets of tasks called <strong>stages</strong></li></ul><h3 id="example">Example</h3><p>Z = X .map(lambda x: (x % 10, x / 10)) .reduceByKey(lambda x, y: x + y) .collect()</p><ol type="1"><li><p>Whenever you invoke an action, the job gets spawned in the driver program. <img src="./exe2.png" width="600"></p></li><li><p>Then the driver runs a job scheduler to divide the job into smaller stages. <img src="./exe3.png" width="600"></p></li><li>Then tasks are created for every job stage.</li><li><p>tasks are delegated to the executors, which perform the actual work. <img src="./exe4.png" width="600"></p></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bash</span><br><span class="line">All this machinery exists within the SparkContext object. It keeps track of the executors, it spawns jobs, and it runs the scheduler.</span><br></pre></td></tr></table></figure><p><strong>Difference between job stage and task:</strong> - <strong>Job stage</strong> is a pipelined computation spanning between materialization boundaries - job stages are defined on RDD level, thus not immediately executable - <strong>Task</strong> is a job stage bound to particular partitions - bound to a particular partitions, thus immediately executable - <strong>Materialization</strong> happens when reading, shuffling or passing data to an action - narrow dependencies allow pipelining - wide dependencies forbid it</p><p><strong>SparkContext – other functions</strong>: - Tracks liveness of the executors by sending heartbeat messages periodically. - required to provide fault-tolerance - Schedules multiple concurrent jobs - to control the resource allocation within the application - Performs dynamic resource allocation if the cluster manager permits. - increases cluster utilization in shared environments by proper scheduling of multiple applications according to their resource demands</p><h2 id="summary">Summary</h2><ol type="1"><li>The SparkContext is the core of your application</li></ol><ul><li>allows your application to connect to a cluster and allocate resources and executors.</li><li>whenever you invoke an action, the SparkContext spawns a job and runs the job scheduler to divide it into stages--&gt;<strong>pipelineable</strong></li><li>tasks are created for every job stage and scheduled to the executors.</li></ul><ol start="2" type="1"><li>The driver communicates directly with the executors</li><li><p>Execution goes as follows: <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Action -&gt; Job -&gt; Job Stages -&gt; Tasks</span><br></pre></td></tr></table></figure></p></li><li><p>Transformations with narrow dependencies allow pipelining</p></li></ol><h1 id="caching-persistence">Caching &amp; Persistence</h1><ul><li>RDDs are partitioned</li><li>Execution is build around the partitions</li><li>Each task processes a small number of partitions at a time, and the shuffle globally redistributes data items between the partitions, when required.</li><li>Spark transfers data over the network and the IO unit here is not a partition but a block.</li><li>Block is a unit of input and output in Spark</li></ul><h2 id="example-1">Example</h2><blockquote><p>Motivating example: load a wikipedia dump from HDFS and see how many articles there contain the words Spark</p></blockquote><p>You need to create the RDD, apply the filter transformation, and invoke the count action. <img src="./exe5.png" width="600"></p><blockquote><p>Motivating example: among those articles with the Spark word, you would like to see how many of them contain the word, Hadoop and how many the word MapReduce.</p></blockquote><p><img src="./exe6.png" width="600"></p><p><strong><em>Perform worse!</em></strong></p><ul><li><p>Reason: after completing the computation, Spark disposes intermediate data and those intermediate RDDs. That means, it will reload the Wikipedia dump two more times incurring extra input and output operations.</p></li><li><p>A better strategy:</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cache the preloaded dump in the memory and reuse it until you end your session.</span><br></pre></td></tr></table></figure><p>Spark allows you to hint which RDDs are better to be kept in memory or even on the disk. Spark does so by <strong><em>caching the blocks comprising your dataset. </em></strong></p><h2 id="controlling-persistence-level">Controlling persistence level</h2><p>Cache: mark the data set as cached by invoking a cache method on it - The cache method is just a shortcut for the memory-only persistence.</p><p>Persist: allows you to set RDDs storage to persist across operations after the first time it is computed. - parameterized by a storage level <img src="./exe7.png" width="600"></p><h2 id="best-practices">Best practices</h2><p>When running an <strong>interactive shell</strong>, cache your dataset after you've done all the necessary preprocessing. - by keeping your work inside in the memory, you would get a more responsive experience.</p><p>When running a <strong>batch computation</strong>, cache dictionaries that you join with your data. - Join dictionaries are often reshuffled, so it would be helpful to speed up their read times.</p><p>When running an <strong>iterative computation</strong>, cache static data like dictionaries or input datasets - avoid reloading the data from the ground up on every iteration. - The static data tends to get evicted due to the memory pressure from the intermediate data.</p><h2 id="summary-1">Summary</h2><ul><li>Performance may be improved by persisting data across operations</li><li>in interactive sessions, iterative computations and hot datasets</li><li>You can control the persistence of a dataset</li><li>whether to store in the memory or on the disk</li><li>how many replicas to create</li></ul><h1 id="broadcast-variable">Broadcast Variable</h1><p><strong>shared memory</strong> is a powerful abstraction, but often misused. - It can make the developer's life easier - It can make the application performance deteriorate because of extra synchronization.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">This is why <span class="keyword">in</span> spark there are restricted forms of the shared memory.</span><br></pre></td></tr></table></figure><p><strong>Broadcast variable</strong> is a read-only variable that is efficiently shared among tasks</p><p><strong>one to many communication:</strong> When it captures a variable into the closure, it is sent to an executor together with a task specification.</p><p><strong>many to many communication protocol</strong>: torrent</p><ul><li>Distribution is done by a torrent-like protocol (extremely fast!)</li><li>Distributed efficiently compared to captured variables</li></ul><h2 id="example-2">Example</h2><blockquote><p>Motivating example: resolve IP addresses to countries from 1 terabyte access log for your website</p></blockquote><p>Idea: map-side join--distribute the database to every mapper and query it locally.</p><p>Distributing the database via a broadcast variable, we take slightly more than 1 gigabyte of outgoing traffic at the driver node</p><p>&lt;img src=&quot;./br1.png&quot; width=&quot;&quot;600&quot; /&gt;</p><blockquote><p>Motivating example:</p></blockquote><ol type="1"><li>setup a transformation graph to compute a dictionary</li><li>invoke the <em>collect</em> action to load it into the driver's memory</li><li>put it into the broadcast variable to use in further computations.</li></ol><p>Idea: upload computations to spark executors and use the driver program as the coordinator. &lt;img src=&quot;./br2.png&quot; width=&quot;&quot;600&quot; /&gt;</p><h2 id="summary-2">Summary</h2><ul><li>Broadcast variables are read-only shared variables with effective sharing mechanism</li><li>Useful to share dictionaries, models</li></ul><h1 id="accumulator-variable">Accumulator Variable</h1><p><strong>Accumulator variable</strong> is a read-write variable that is shared among tasks - Writes are restricted to increments! - i. e.: var += delta - addition may be replaced by any associate, commutative operation</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Restricting the right operations allows the framework to avoid complex synchronization thus making the accumulators efficient.</span><br></pre></td></tr></table></figure><ul><li>Accumulator variable could be read only by the <strong><em>driver</em></strong> program and not by the executors.</li><li>cannot read the accumulated value from within a task</li></ul><h2 id="example-3">Example</h2><p><img src="./acc1.png" width="600"> <img src="./acc2.png" width="200"> <img src="./acc3.png" width="200"> <img src="./acc4.png" width="200"></p><h2 id="guarantees-on-the-updates">Guarantees on the updates</h2><ul><li>Updates generated in actions: guaranteed to be applied only once to the accumulator.</li><li>This is because successful actions are never re-executed and Spark can conditionally apply the update.</li><li>Updates generated in transformations: no guarantees when they accumulate updates. - - Transformations can be recomputed on a failure, on the memory pressure, or in another unspecified codes like a preemption.</li><li>Spark provides no guarantees on how many times transformation code maybe re-executed.</li></ul><h2 id="use-cases">Use cases</h2><ol type="1"><li>Performance counters</li></ol><ul><li>number of processed records, total elapsed time, total error and so on and so forth</li></ul><ol start="2" type="1"><li>Simple control flow</li></ol><ul><li>conditionals: stop on reaching a threshold for corrupted records</li><li>loops: decide whether to run the next iteration of an algorithm or not</li></ul><ol start="3" type="1"><li>Monitoring</li></ol><ul><li>export values to the monitoring system</li></ul><ol start="4" type="1"><li>Profiling &amp; debugging</li></ol><h2 id="summary-3">Summary</h2><ul><li>Accumulators are shared read-write variables with de-coupled read and write sides</li><li>could be updated from actions and transformations by using an increment.</li><li>can use custom associative, commutative operation for the updates</li><li>can read the total value only in the driver</li><li>Useful for the control flow, monitoring, profiling &amp; debugging</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;This is course note of &lt;a href=&quot;https://www.coursera.org/learn/big-data-essentials&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Big Data Essentials: HDFS, MapReduce and Spark RDD&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;execution-scheduling&quot;&gt;Execution &amp;amp; Scheduling&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;SparkContext&lt;/strong&gt; - When creating a Spark application, the first thing you do is create a SparkContext object, which tells Sparks how to access a cluster. - The context, living in your driver program, coordinates sets of processes on the cluster to run your application.&lt;/p&gt;
    
    </summary>
    
      <category term="Big Data" scheme="https://nancyyanyu.github.io/categories/Big-Data/"/>
    
    
      <category term="Spark" scheme="https://nancyyanyu.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Apache Spark: Basic Concepts</title>
    <link href="https://nancyyanyu.github.io/posts/4fe20312/"/>
    <id>https://nancyyanyu.github.io/posts/4fe20312/</id>
    <published>2019-06-27T23:08:12.765Z</published>
    <updated>2019-06-28T21:29:05.924Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>This is course note of <a href="https://www.coursera.org/learn/big-data-essentials" target="_blank" rel="noopener">Big Data Essentials: HDFS, MapReduce and Spark RDD</a></p></blockquote><h1 id="apache-spark">Apache Spark</h1><p><strong>Apache Spark</strong>:a modern distributed fault tolerant computation platform.</p><p><strong>History of Apache Spark</strong>:</p><a id="more"></a><p><img src="./week4_1.png" width="600"> <img src="./week4_2.png" width="250"> <img src="./week4_3.png" width="600"> <img src="./week4_4.png" width="600"></p><h1 id="rdds">RDDs</h1><p><strong>RDD(Resilient Distributed Dataset)</strong>: a core abstraction enabling both an <em>efficient execution for a computation</em>, and a <em>flexible and convenient formalism to define computations</em>.</p><ul><li><strong>Resilient</strong> — able to withstand failures</li><li><strong>Distributed</strong> — spanning across multiple machines</li><li>Formally, RDD is a read-only, partitioned collection of records</li><li>To say that the dataset is an RDD, the dataset must adhere to the RDD interface.</li></ul><p><strong>The dataset must be:</strong> - able to enumerate its partitions by implementing the partition's function.</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">partitions() -&gt; Array[Partition]</span><br></pre></td></tr></table></figure></p><ul><li>The partition is an opaque object for the framework. It is passed back to the iterator function of the RDD, when the framework needs to read the data from the partition.</li></ul><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iterator(p: Partition, parents: Array[Iterator]) -&gt; Iterator</span><br></pre></td></tr></table></figure></p><ul><li>able to enumerate its dependencies and provide an array of dependency objects.</li></ul><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dependencies() -&gt; Array[Dependency]</span><br></pre></td></tr></table></figure></p><ul><li>The dependency object maps partitions of the dataset to the dependencies that are partitions of the parent dataset.</li><li><p>Those parent partitions are injected into the iterator call when creating a reader.</p></li><li><strong>Typed.</strong> every item in RDD has the same, known type.</li><li>typedness is an important property to catch bugs early on before the actual execution</li><li><p>e.g. RDD[strings], or an RDD[integers]. <img src="./rdd1.png" width="600"></p></li></ul><h2 id="why-do-we-need-a-new-abstraction">Why do we need a new abstraction?</h2><p><strong>Example</strong>: iterative computations (K-means, PageRank, …) - relation between consequent steps is known only to the user code not the framework - framework has no capabilities to optimize the whole computation - framework must reliably persist data between steps thus generating excessive I/O (even if it is temporary data)</p><blockquote><p>In this scenario,Spark is trying to keep the data in the memory, effectively eliminates an intermediate disk persistence, and thus improving the completion time.</p></blockquote><p><strong>Example</strong>: joins - join operation is used in many MapReduce applications - not-so-easy to reuse code</p><h2 id="example-a-binary-file-in-hdfs">Example: a binary file in HDFS</h2><blockquote><p>Implement the necessary functions to make the binary file in RDD</p></blockquote><p><img src="./rdd2.png" width="600"></p><ul><li>To implement the <strong>partition</strong>'s function: lookup the blocks for NameNode, create a partition for every block, and return the partitions.</li><li>To implement the <strong>iterator</strong>'s function: extract the block information from the partition, and use it to create a reader from HDFS.</li><li>To implement the <strong>dependencies</strong>' function: File reading does not depend on any other RDD, nor on any other partition. So implementing the dependencies function is trivial. It returns an empty array</li></ul><h2 id="example-a-data-file-in-hdfs">Example: a data* file in HDFS</h2><blockquote><p>reading records from the file rather than a raw bytes</p></blockquote><p><img src="./rdd3.png" width="600"></p><h2 id="example-an-in-memory-array">Example: an in-memory array</h2><p>The simplest way to make the array in RDD is to pretend that there is a single partition with the whole array. In this case, the partition object keeps a <strong><em>reference</em></strong> to the array, and the iterator function uses this reference to create an iterator.</p><p><img src="./rdd4.png" width="600"></p><h2 id="example-a-sliced-in-memory-array">Example: a sliced* in-memory array</h2><blockquote><p>slice the array into chunks to gain parallelism</p></blockquote><p>The partition corresponds to the chunk of the source array.</p><p><em>partitions are handled in parallel.</em></p><p><img src="./rdd5.png" width="600"></p><h2 id="summary">Summary</h2><ul><li><strong>RDD is a read-only, partitioned collection of records</strong></li><li>The collection of the dataset must provide information about its partitions, and provide means to create an iterator over the partition.</li><li>a developer can access the partitions and create iterators over them</li><li>RDD tracks dependencies (to be explained in the next video)</li><li>Examples of RDDs</li><li>Hadoop files with the proper file format</li><li>In-memory arrays</li></ul><h1 id="transformation">Transformation</h1><p><strong>Two ways to construct RDDs:</strong> - Data in a stable storage - Example: files in HDFS, objects in Amazon S3 bucket, lines in a text file, … - RDD for data in a stable storage has no dependencies - From existing RDDs by applying a transformation - Example: filtered file, grouped records, … - RDD for a transformed data depends on the source data</p><p><strong>Transformation</strong>: - Allow you to create new RDDs from the existing RDDs by specifying how to obtain new items from the existing items - The transformed RDD depends implicitly on the source RDD</p><p>Note: Datasets are immutable in Spark, and you cannot modify data in-place.</p><h2 id="example-map-flatmap">Example: map, flatMap</h2><p><strong>map</strong>: - Def: map(f: T -&gt; U): RDD[T] -&gt; RDD[U] - returns a mapped RDD with items f(x) for every x in the source RDD <img src="./trans2.png" width="600"></p><p><strong>flatMap</strong>: - Def: flatMap(f: T -&gt; Array[U]): RDD[T] -&gt; RDD[U] - same as map but flattens the result of <em>f</em> - generalizes map and filter</p><h2 id="example-filter">Example: filter</h2><p><strong>filter</strong>: - Def: filter(p: T -&gt; Boolean): RDD[T] -&gt; RDD[T] - returns a filtered RDD with items satisfying the predicate <em>p</em> <img src="./trans1.png" width="600"></p><p><strong>Filtered RDD</strong>: - <strong>partitions</strong>: transformed RDD's mostly the same as source one--&gt;<em>reuse partitions of the source RDD</em> as there is no need to change the partitioning. - <strong>dependencies</strong>: every field of partition depends on the source partition. You can establish this relation by providing a dependency object that establishes one-to-one correspondence between the filtered and the source partitions. - <strong>iterator</strong>: when creating an iterator over the filter partition: - Spark would inject an iterator of the source partition into the iterator function called - reusing the parent iterator. - When requested for the next value, you can pull values from the parent iterator until it returns you an item that satisfies the predicate. <img src="./trans3.png" width="600"></p><h3 id="lazy-iterator">Lazy iterator</h3><p>Actual filtering happens not at the creation time of Y, but at the access time to the iterator over a partition of Y.The filtering starts to happen only when you start to pull items from the iterator.</p><p>Same holds for other transformations – they are lazy,i.e. they compute the result only when accessed.</p><h3 id="on-closures">On closures</h3><p><img src="./trans4.png" width="600"></p><h3 id="partitiondependency-graph">(Partition)Dependency graph</h3><p>Whenever you apply a transformation to the RDD, you implicitly construct a dependency graph on the RDDs - This graph is used by the framework to schedule jobs</p><p><img src="./trans5.png" width="600"></p><h2 id="keyed-transformation">Keyed Transformation</h2><p><img src="./trans6.png" width="600"></p><h3 id="grouped-rdd">Grouped RDD</h3><ul><li>partition the key space</li><li><p>using the hash partition then to compute the values in the resulting partition</p></li><li><strong>Shuffle</strong>: redistribute all the values between all the partitions.</li><li><p>scan over the entire source RDD to select only the pairs that belong to the output partition.</p></li></ul><blockquote><p>Differnt from flatMap like transformation, a single output partition depends on all the input partitions.</p></blockquote><p><img src="./trans9.png" width="600"></p><h4 id="narrow-wide-dependencies">Narrow &amp; Wide dependencies</h4><p><img src="./trans10.png" width="600"></p><h2 id="cogroup-transformation">Cogroup Transformation</h2><p><img src="./trans7.png" width="600"></p><p>The cogroup transformation allows you to compute any kind of a join between two data sets.</p><h3 id="inner-join">Inner join</h3><p>That is all triples (k, x, y) where (k, x) is in X and (k, y) is in Y</p><p>--&gt; apply the <strong>`flatMap</strong> transformation on top of the result of the cogroup transformation.</p><h2 id="joins-transformation">Joins Transformation</h2><p>The join transformation produces the inner join of two data sets.</p><p><strong>inner join:</strong> If the key is present only in the one side of the join that is in one data set. Then it is omitted from the result</p><p><strong>outer join:</strong> one-sided keys are added to the result with appropriate null values.</p><p><img src="./trans8.png" width="600"></p><h2 id="mapreduce-in-spark">MapReduce in Spark</h2><p>You can express any MapReduce computation in Spark as the <strong>flatMap</strong> followed by the <strong>groupByKey</strong>, followed by one more <strong>flatMap</strong>.</p><p><img src="./trans11.png" width="600"></p><h2 id="summary-1">Summary</h2><p>Transformation - is a description of how to obtain a new RDD from existing RDDs - is the primary way to “modify” data (given that RDDs are immutable)</p><p>Properties: - Transformations are lazy, i.e. no work is done until data is explicitly requested - There are transformations with narrow and wide dependencies - MapReduce can be expressed with a couple of transformations - Complex transformations (like joins, cogroup) are available</p><h1 id="actions">Actions</h1><p><strong>Driver &amp; executors</strong>: - When you write and invoke your Spark application, driver <strong><em>runs within the driver program.</em></strong> - Driver program drives the execution of your Spark application - Driver delegates tasks to executors to use cluster resources - When something must be done with the data, the driver schedules tasks to be executed by executors. - In <em>local</em> mode, executors are collocated with the driver - In <em>cluster</em> mode, executors are located on cluster machines--&gt; allowing you to use the cluster for a computation</p><p><strong>Actions</strong>: - Triggers data to be materialized and processed on the executors and then passes the outcome to the driver &gt; Actions, together with a transformation code, are executed elsewhere, not in your driver program. Your driver program receives only the outcome.</p><p>Spark does a great job of abstracting away the execution details, and that improves your developer productivity and code readability.</p><ul><li><strong>Example</strong>: actions are used to collect, print and save data</li></ul><h2 id="frequently-used-actions">Frequently used actions</h2><ul><li><strong>collect</strong> action collects the result into the memory of the driver program.</li><li>intended to be used when the output is small enough to fit into the driver's memory.</li><li><strong>take</strong> action takes the given number of items from a data set and passes them back to the driver.</li><li>tries to use only the first partition of a data set to optimize the completion time. <img src="./action1.png" width="600"> When the result is large enough, you may want to save it to HDFS for example. Doing so by collecting items in the driver will quickly run out of memory.</li></ul><p>There are special family of safe actions that do heavy work on the executor side and return a confirmation to the driver.</p><ul><li>SaveAsText file is used for full debugging or for simple applications</li><li>SaveAsHadoopFile leverages Hadoop file formats to serialize data--&gt;common way to save data to HDFS. <img src="./action2.png" width="600"></li></ul><p>If you need to run your own code over a data set, there are <em>foreach</em> and <em>foreachPartition</em> actions that invoke your function on the executor side.</p><p>You can use this function to persist your data in your custom database for example, or to send data over the wire to an external service, or anything else. <img src="./action3.png" width="600"></p><h2 id="summary-2">Summary</h2><ul><li>Actions trigger computation and processing of the dataset</li><li>Actions are executed on executors and they pass results back to the driver</li><li>Actions are used to collect, save, print and fold data</li></ul><h1 id="resiliency">Resiliency</h1><p>How it is possible to continue operation despite machine failures in the cluster?</p><h2 id="fault-tolerance-in-mapreduce">Fault-tolerance in MapReduce</h2><p>Two key aspects: - reliable storage for input and output data - once data is stored in HDFS it is safe - deterministic and side-effect free execution of mappers and reducers - if the computation is deterministic, and has no side effects, the framework can restart it multiple times, and get the same result every time <img src="./resi1.png" width="600"></p><h2 id="fault-tolerance-in-spark">Fault-tolerance in Spark</h2><p>Same two key aspects: - reliable storage for input and output data - deterministic and side-effect free execution of transformations(including closures) - Spark assumes that every transformation is deterministic, and free of side-effects, to be able to restart any failed computation</p><p><strong>Determinism</strong> — every invocation of the function results in the same returned value - e. g. do not use random numbers, do not depend on a hash value order</p><p><strong>Freedom of side-effects</strong> — an invocation of the function does not change anything in the external world - e. g. do not commit to a database, do not rely on global variables - If your function happens to commit to the database, there is no way to roll back changes in case of failure.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Spark is allowed to restart the failed parts of the computation. To decide what to restart, Spark keeps track of the lineage.</span><br></pre></td></tr></table></figure><h2 id="fault-tolerance-transformations">Fault-tolerance &amp; transformations</h2><p><strong>Lineage</strong> — a dependency graph for all partitions of all RDDs involved in a computation up to the data source</p><p>Machine failure renders some partitions in the lineage unavailable. To cope with the failure, you must detect <em>which partitions become unavailable</em>, and decide <em>what to restart</em>. - Detection is done in the Driver Program, because the Driver Program orchestrates the execution, and already tracks the partitions. - <strong>Deciding what to restart</strong>: Given the failed partition, you look at its dependencies, and if they are alive, restart the computation. If the dependencies are failed as well, you recursively try to recover them. - <strong>Restarts</strong> are slightly more fragile in the case of wide dependencies, because to recompute an output partition, all dependencies must be alive, and there are many of them. If dependencies you evicted out of a cache for example, the restart will be expensive.</p><p><img src="./resi2.png" width="600"> <img src="./resi3.png" width="600"></p><h2 id="fault-tolerance-actions">Fault-tolerance &amp; actions</h2><p>Actions are side-effects in Spark (communicate with external services) - Actions have to be <strong>idempotent</strong>幂等（自己重複運算的結果等於它自己的元素） that is safe to be re-executed multiple times given the same input</p><p>Example: <strong>collect()</strong> - all transformations are deterministic, the final data set isn't changed in case of restarts. Therefore, even if the collect action fails, it could be safely re-executed</p><p>Example: <strong>saveAsTextFile()</strong></p><ul><li>since the data set is the same, you can safely override the output file</li></ul><h2 id="summary-3">Summary</h2><p>Resiliency is implemented by - tracking lineage - assuming deterministic &amp; side-effect free execution of transformations(including closures) - all the closures pass to Spark, must be deterministic and side effect free - assuming idempotency for actions</p><p>These properties cannot be checked or enforced at the compile time, and may lead to obscure bugs in your application that are hard to debug and hard to reproduce. ```</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;This is course note of &lt;a href=&quot;https://www.coursera.org/learn/big-data-essentials&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Big Data Essentials: HDFS, MapReduce and Spark RDD&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;apache-spark&quot;&gt;Apache Spark&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Apache Spark&lt;/strong&gt;:a modern distributed fault tolerant computation platform.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;History of Apache Spark&lt;/strong&gt;:&lt;/p&gt;
    
    </summary>
    
      <category term="Big Data" scheme="https://nancyyanyu.github.io/categories/Big-Data/"/>
    
    
      <category term="Spark" scheme="https://nancyyanyu.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>DFS, HDFS, Architecture, Scaling problem</title>
    <link href="https://nancyyanyu.github.io/posts/49a14c15/"/>
    <id>https://nancyyanyu.github.io/posts/49a14c15/</id>
    <published>2019-06-27T23:04:59.943Z</published>
    <updated>2019-06-28T21:31:20.680Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>This is course note of <a href="https://www.coursera.org/learn/big-data-essentials" target="_blank" rel="noopener">Big Data Essentials: HDFS, MapReduce and Spark RDD</a></p></blockquote><h1 id="scaling-dfs">Scaling DFS</h1><h2 id="big-data-storage">Big data storage:</h2><ul><li>Scale up (vertical scaling): get yourself a bigger hard drive</li><li>Lower latency</li><li>Scale out (horizontal scaling)</li><li>Higher latency</li><li>Problem: one node get out of service 3 years averagely</li></ul><p>-&gt; Distributed file system</p><a id="more"></a><p><img src="./week_1.png" width="600"></p><h2 id="google-file-system">Google File System:</h2><p><strong>Keys:</strong> - components failures are a norm (→ replication) - even space utilisation: all files splited into blocks of fixed size, about 100mb - write-once-read-many: it's not allowed to modify in the middle, as it drastically simplifies API and internal implementation of a distributed file system.</p><p><strong>Replication:</strong> <img src="./week_2.png" width="600"> &gt; S.txt and B.txt. They are both split into equal sized blocks, and then distributed over a different machine, with replications. Storage machines are called <em>channel servers</em>, or <em>data nodes</em>.</p><p><strong>Metadata:</strong> include administrative information about creation time, access properties...</p><p><strong>Master node:</strong> stores all metadata in memory; enable to request metadata with minimal latency</p><h2 id="hadoop-distributed-file-system">Hadoop Distributed File System</h2><blockquote><p>an open source implementation of Google File System</p></blockquote><p><strong>Server roles</strong>: 1. Namenode: master node 2. Datanode</p><p><img src="./week_3.png" width="500"></p><p>HDFS client provides command line interface to communicate with that distributed file system -&gt;no need to write any code to access data.</p><h3 id="how-to-read-files-from-hdfs">How to read files from HDFS?</h3><p><img src="./week_4.png" width="600"></p><ol type="1"><li><p>Request name node to get information about file blocks' locations. &gt;These blocks are distributed over different machines, but all of this complexity is hidden behind HDFS API.</p></li><li><p>User only sees a continuous stream of data. &gt;If at some point a datanode you retrieve data from died you get this data from another replica without bothering users about it.</p></li><li><p>You will get data from the closest machine.</p></li></ol><h4 id="closeness">Closeness</h4><blockquote><p>data center topology; it depends on the physical distance and unpredictable system load such as metric overutilization</p></blockquote><p><img src="./week_5.png" width="600"></p><ul><li>d=0: request data from HDFS, and this data is available on the same machine, then you can use data locality to read data directly from hard drive without any extra RPC codes.</li><li>d=2: a datanode is located in the same rack</li><li>d=4: read data from another rack</li><li>d=6: the data is allocated in another data center</li></ul><h3 id="how-to-write-files-into-hdfs">How to write files into HDFS?</h3><h4 id="redundancy-model">Redundancy model:</h4><blockquote><p>When you write a block of data into HDFS, Hadoop distributes replicas over the storage.</p></blockquote><p><strong>first replica</strong>: located on the same node if write data from a DataNode machin; otw, the first DataNode to put replica is chosen by random.</p><p><strong>second replica</strong>: placed in a different rack. If this racks goes down (power supply problems), you will access data from another rack.</p><p><strong>third replica</strong>: located on a different machine in the same rack as the second replica. You don't pay for extra between rack network utilization as the third replica is copied from the second data node.</p><p><strong>further replicas</strong>: applies on the random nodes in the cluster</p><p><img src="./week_6.png" width="400"></p><h4 id="data-flow-of-writing-data">Data flow of writing data</h4><p><img src="./week_7.png" width="500"></p><ol type="1"><li><p>HDFS client request and name node via RPC protocol. &gt; The name node validates if you have rights to create a file and there are no naming conflicts.</p></li><li><p>HDFS client requests a list of datanodes to put a fraction of blocks of the file. &gt; These datanodes form a pipeline as your first client sends packets of data to the closest datanode. The later one transfers copies of packets through a datanode pipeline. As soon as packet is on all of the datanodes, datanodes send acknowledgment packets back.</p></li></ol><p><strong>If something goes wrong</strong> - then if the client closes the datanode pipeline, marks the misbehaving datanode bad and requests a replacement for the bad datanodes from a name node. So a new data node pipeline will be organized, and the process of writing the file to HDFS will continue.</p><h4 id="what-happens-with-failure-blocks">What happens with failure blocks?</h4><p><img src="./week_8.png" width="600"> Datanode serves a state machine for each block. Whenever a datanode recovers from its own failure, or failures of other datanodes in a pipeline, you can be sure that all the necessary replicas will be recovered. And unnecessary ones will be removed.</p><h1 id="block-and-replica-states">Block and Replica States</h1><p><strong>Replica</strong> : a physical data storage on a data node.There are usually several replicas with the same content on different data nodes.</p><p><strong>Block</strong>: a meta-information storage on a name node and provides information about replica's locations and their states.</p><p>Both replica and block have their own states. <img src="./week_9.png" width="400"></p><p><strong>Data node replica's states</strong>: Finalized, Replica Being Written to, Replica Under Recovery, Replica Waiting to be Recovered, Temporary</p><p><strong>Name node replica's states</strong>:</p><p><strong>Difference of Datanode &amp; Namenode</strong>: a block state is stored in memory, it doesn't persist on any disk.</p><h2 id="datanode-state-finalized">Datanode State: Finalized</h2><p><img src="./week_11.png" width="450"></p><p><strong>Finalized state</strong>: the content of this replica is frozen - Meaning: meta-information for this block on name node is aligned with all the corresponding replica's states and data.</p><ul><li><p><strong>Read consistency</strong>: you can safely read data from any data node and you will get exactly the same content.</p></li><li><strong>Generation Stamp(GS)</strong>: Each block of data has a version number called Generation Stamp. All of finalized replicas have the same GS number which can only increase over time. <img src="./week_10.png" width="600"></li><li><p>It happens during error <em>recovery process</em> or during <em>data appending to a block</em>.</p></li></ul><h2 id="datanode-state-replica-being-written">Datanode State: Replica Being Written</h2><p><img src="./week_12.png" width="450"></p><p><strong>RBW</strong>:the state of the last block of an open file or a file which was reopened for appending.</p><ul><li>Different data nodes can return to use a different set of bytes. In short, bytes that are acknowledged by the downstream data nodes in a pipeline are visible for a reader of this replica.</li><li>Data node on disk data and name node meta-information may not match during this state.</li><li><strong>Data Durability</strong>: In case of any failure data node will try to preserve as many bytes as possible.</li></ul><h2 id="datanode-state-replica-waiting-to-be-recovered">Datanode State: Replica Waiting to be Recovered</h2><p><img src="./week_13.png" width="450"></p><p><strong>RWR</strong>: a state of all Being Written replicas after data node failure and recovery after a system reboot or after Pacer.sys or BSOD,</p><ul><li>RWR replicas will not be in any data node pipeline and therefore will not receive any new data packets.</li><li>RWR either become <strong><em>outdated and should be discarded</em></strong>, or they will participate in a special recovery process called a <strong><em>lease recovery</em></strong> if the client also dies.</li></ul><p>HDFS client requests a <em>lease</em> from a name node to have an exclusive access to write or append data to a file. In case of HDFS client lease expiration, replica transition to a RUR state.</p><h2 id="datanode-state-replica-under-recovery">Datanode State: Replica Under Recovery</h2><p><img src="./week_14.png" width="450"></p><p><strong>RUR</strong> (Replica Under Recovery): HDFS client requests a <strong><em>lease</em></strong> from a name node to have an exclusive access to write or append data to a file. In case of HDFS client <strong><em>lease expiration</em></strong>(usually happens during the client's site failure), replica transition to a RUR state.</p><h2 id="datanode-state-temporary">Datanode State: Temporary</h2><p><img src="./week_15.png" width="450"></p><p><strong>Temporary</strong>: As data grows and different nodes are added or removed from a cluster, data can become unevenly distributed over the cluster nodes. A Hadoop administrator can spawn a process of data re-balancing or a data engineer can request increasing of the replication factor of data for the sake of durability. In these cases new generated replicas will be in a state called temporary.</p><ul><li>Similar to RBW except the fact that this data is not visible to user unless finalized.</li><li>In case of failure, the whole chunk of data is removed without any intermediate recovery state.</li></ul><h2 id="namenode-state-under-construction">Namenode State: Under Construction</h2><p><img src="./week_16.png" width="450"></p><p><strong>Under Construction</strong>: opens a file for writing, name node creates the corresponding block with the <em>under_construction</em> state; opens a file for append name node also transition this block to the state <em>under_construction</em>.</p><ul><li>always the last block of a file</li><li>it's length and generation stamp are mutable</li></ul><h2 id="namenode-state-under-recovery">Namenode State: Under Recovery</h2><p><img src="./week_17.png" width="450"></p><p>Name node block keeps track of write pipeline. It means that it contains information about all RBW and RWR replicas. Replicas transitions from RWR to recovery RUR state when the client dies. Even more generally it happens when a client's lease expires. Consequently, the corresponding block transitions from under_construction to under_recovery state. <img src="./week_18.png" width="450"></p><h2 id="namenode-state-committed">Namenode State: Committed</h2><p><img src="./week_19.png" width="450"> The under_construction block transitions to a committed state when a client successfully requests name node to close a file or to create a new consecutive block of data.</p><p>The committed state means that there are already some finalized replicas but not all of them. For this reason in order to serve a read request, the committed block needs to keep track of RBW replicas, until all the replicas are transitioned to the finalized state and HDFS client will be able to close the file.</p><h2 id="namenode-state-final-complete">Namenode State: Final Complete</h2><p><img src="./week_20.png" width="500"></p><p><strong>Final complete state</strong> of a block: a state where all the replicas are in the finalized state and therefore they have identical visible length and generation stamps.</p><ul><li>Only when all the blocks of a file are complete the file can be closed.</li></ul><h2 id="namenode-state-open-file">Namenode State: Open File</h2><p><strong>Open File state</strong>: In case of name node restart, it has to restore the open file state. All the blocks of the un-closed file are loaded as complete except the last block which is loaded as under_construction.</p><p>Then recovery procedures will start to work.</p><p><strong>Recovery</strong>: replica recovery, block recovery, lease recovery, and pipeline recovery.</p><h1 id="recovery-process">Recovery Process</h1><h2 id="block-recovery">Block Recovery</h2><p><strong>Goal</strong>: NameNode has to ensure that all of the corresponding replicas of a block will transition to a common state logically and physically.</p><p><strong>physically</strong>: all the correspondent replicas should have the same on disk content.</p><p>To accomplish it,</p><ol type="1"><li><p><strong>primary datanode(PD)</strong>: NameNode chooses a primary datanode called PD in a design document. PD should contain a replica for the target block. <img src="./week_21.png" width="600"></p></li><li><p>PD request from a NameNode, a new generation stamp, information and location of other replicas for recovery process.</p></li></ol><p><img src="./week_22.png" width="600"></p><ol start="3" type="1"><li>PD connects each relevant DataNodes to participate in the <strong>replica recovery process</strong>.During this phase, all the necessary information or data is propagated through the pipeline.</li></ol><p><strong>Replica recover process</strong> includes: - Aborting active clients right into a replica. - Aborting the previous replica of block recovery process, and participating in final replica size agreement process. <img src="./week_23.png" width="600"></p><ol start="4" type="1"><li>As the last step, PD notifies NameNode about the result, success or failure. In case of failure, NameNode could retry block recovery process. <img src="./week_24.png" width="600"></li></ol><h2 id="lease-recovery">Lease Recovery</h2><blockquote><p>Block recovery process could happen only as a part of the lease recovery process.</p></blockquote><p>Lease manager manages all the leases at the NameNode. HDFS clients request at least every time they would like to write, or append to a file. <img src="./week_25.png" width="600"></p><h3 id="conditions-of-starting-lease-recovery-process">Conditions of starting lease recovery process</h3><p>Lease manager maintains a soft and a hard limit. If a current lease holder doesn't renew his lease during the soft limit timeout, then another client will be able to take over this lease. In this case and in the case of reaching a hard limit, the process of lease recovery will begin. <img src="./week_26.png" width="600"> <img src="./week_28.png" width="600"></p><p><strong>Necessity:</strong> to close open files for the sake of the client.</p><p><strong>Gurantees to be reached</strong>:</p><ul><li><p><strong>concurrency control</strong>: Even if a client is still alive, it won't be able to write data to a file</p></li><li><p><strong>consistency guarantee</strong>: All replicas should draw back to a consistence state to have the same on-disk data and generation stamp.</p></li></ul><p><img src="./week_29.png" width="600"></p><h3 id="lease-recovery-process">Lease recovery process</h3><p><img src="./week_30.png" width="600"></p><ol type="1"><li>Lease recovery starts with a <strong>lease renew</strong>.</li></ol><p>New files lease holder should have the ability to take ownership of any other user's lease. The name of the super user is DFS. Therefore, all the other client request such as get new generation stamp, get new block, close file from other clients to this pass will be rejected.</p><ol start="2" type="1"><li><p>NameNode gets the lease of DataNodes which contains the last block of a file, and sends a primary DataNode and starts a block recovery process. <img src="./week_30.png" width="600"></p></li><li><p>As soon as block recovery process finishes, the NameNode is notified by PD about the outcome. Updates blocking for, and removes the lease for a file.</p></li></ol><p><img src="./week_31.png" width="600"></p><h2 id="pipeline-recovery">Pipeline Recovery</h2><h3 id="pipeline">Pipeline</h3><p>When you write to an HDFS file, HDFS client writes data block by block. Each block is constructed through a <strong>write pipeline</strong>, as the first client breaks down block into pieces called <strong>packets</strong>. These packets are propagated to the DataNodes through the pipeline.</p><blockquote><p>Three stages: Pipeline setup, data streaming, and close</p></blockquote><ul><li>bold lines: data packets</li><li>doted lines: acknowledge messages</li><li>regular lines: control messages.</li></ul><p><img src="./week_32.png" width="600"></p><h4 id="setup">Setup</h4><p>a clients sends a setup message down to the pipeline. Each DataNode opens a replica for writing and sends ack message back upstream with the pipeline. <img src="./week_33.png" width="600"></p><h4 id="data-streaming">Data streaming</h4><p>Data streaming stage is defined by time range from t1 to t2, where t1 is the time when a client to receives the acknowledgement message for the top stage. And t2 is the time when the client receives the acknowledgement message for all the block packets. <img src="./week_34.png" width="600"></p><ul><li><p>data is buffered on the client site to form a packet, then propagated through the DataNode pipeline.</p></li><li><p>Next packet can be sent even before the acknowledgment of the previous packet is received.</p></li></ul><p><strong>flush</strong>: synchronous packets and used as synchronization points for the DataNode right.</p><p><img src="./week_35.png" width="600"></p><h4 id="close">Close</h4><p>Finalize replicas and shut down the pipeline.</p><p><img src="./week_36.png" width="600"></p><ul><li>All of the DataNodes in the pipeline change the replica state to the finalized.</li><li>Report the state to a NameNode and send the acknowledgement message upstream.</li></ul><h3 id="pipeline-recovery-process">Pipeline recovery process</h3><p>Pipeline recovery can be initiated during each of these stages.</p><h4 id="setup-failure">Setup failure</h4><p>A failure happens during writing to a new file, abandon DataNode pipeline and request a new one from scratch <img src="./week_37.png" width="600"></p><p>In this case, some packets can be resent but they will not be extra disk IO overhead for DataNodes that already saved this packet on disk. Once the client detects a failure during close stage, it rebuilds a pipeline with good DataNodes. Bumps generation stamp and requests to finalize replicas.</p><h4 id="data-streaming-failure">Data streaming failure</h4><p>If DataNode is not able to continue process packets appropriately,then it allots the DataNode pipeline about it, by closing all the connections.</p><p>When HDFS client detects a fire, it stops sending new packets to the existing pipeline, request a new generation stamp from a NameNode, and rebuilds a pipeline from good DataNodes. &gt; In this case, some packets can be resent but there will not be extra disk IO overhead for DataNodes that already saved this packet on disk.</p><p>All DataNodes keep track of bytes received, bytes written to a disk and bytes acknowledged. <img src="./week_38.png" width="600"></p><h4 id="close-failure">Close failure</h4><p>Once the client detects a failure during close stage, it rebuilds a pipeline with good DataNodes. Bumps generation stamp and requests to finalize replicas. <img src="./week_39.png" width="600"></p><h1 id="hdfs-client">HDFS Client</h1><h1 id="namenode-architecture">Namenode Architecture</h1><p><strong>NameNode</strong>: a service responsible for keeping hierarchy of folders and files.</p><ul><li>NameNode stores all of this data in memory.</li></ul><p><img src="./week40.png" width="600"></p><h2 id="capacity">Capacity</h2><p>For example: 10 petabytes of data.</p><p><strong>Capacity for data nodes:</strong> - In HDFS, all data is usually stored with replication factor three. &gt; So, you can request to buy approximately 15,000 of two terabyte hard drives.</p><ul><li>On average, 15 hard drives will die every day. &gt;So, you should request at least 1000 extra hard drives for several months of research.</li></ul><p><strong>Capacity for meta information in memory:</strong></p><ul><li><p><strong>Small files problem:</strong> Storing lot of small files which are extremely smaller than the block size cannot be efficiently handled by HDFS. Reading through small files involve lots of seeks and lots of hopping between data node to data node, which is inturn inefficient data processing.</p></li><li><p><strong>128 megabyte</strong> of data was once chosen as a default block size.</p></li></ul><blockquote><p>WHY? It is one choice to have less than one percent overhead for reading the random block of data from a hard drive, and keeping block size small at the same time.</p></blockquote><p><img src="./week_41.png" width="600"></p><p>10 petabytes data will consume at least 35 gigabyte of RAM on the NameNode.</p><h2 id="failure">Failure</h2><p><strong>NameNode server is a single point of failure.</strong> &gt; In case this service goes down, the whole HDFS storage becomes unavailable, even for read-only operations.</p><p>Technical tricks to make NameNode decisions durable and to speed up NameNode recovery process: - write-ahead log (WAL): strategy to persist matter information modifications. This log is called the edit log. It can be replicated to different hard drives. It is also usually replicated to an NFS storage.</p><ul><li><p>NFS storage:you will be able to tolerate full NameNode crash</p></li><li><p><strong>fsimage</strong>: have a snapshot of memory at some point in time from which you can replay transaction stored in the edit log.</p></li></ul><p><img src="./week41.png" width="600"></p><p><strong>secondary NameNode</strong></p><blockquote><p>tackle the problem that edit log grows fast, replay of one week of transactions from edit log will take several hours to boot a NameNode</p></blockquote><p>Secondary NameNode, or better to say, checkpoint NameNodes, compacts the edit log by creating a new fsimage.</p><p>New fsimage is made of all the fsimage by applying all stored transactions in edit log.</p><ul><li>secondary NameNode consumes the same amount of RAM to build a new fsimage.</li><li>secondary NameNode was considered a badly named service.</li><li>secondary NameNode <span class="math inline">\(\neq\)</span> backup NameNode</li></ul><p><img src="./week42.png" width="600"></p><h2 id="evaluate">Evaluate</h2><p>Evaluate how long it takes to read 10 petabytes of data from a hard drive with similar reading speed:</p><p><img src="./week43.png" width="600"></p><p>The amount of drives in your cluster has a linear relation to the speed of data processing</p><h2 id="summary">Summary</h2><ol type="1"><li>explain and reason about HDFS Namenode architecture: (RAM; fsimage + edit log; block size)</li><li>estimate required resources for a Hadoop cluster</li><li>explain what small files problem is and where a bottleneck is</li><li>list differences between different types of Namenodes (Secondary / Checkpoint / Backup</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;This is course note of &lt;a href=&quot;https://www.coursera.org/learn/big-data-essentials&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Big Data Essentials: HDFS, MapReduce and Spark RDD&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;scaling-dfs&quot;&gt;Scaling DFS&lt;/h1&gt;
&lt;h2 id=&quot;big-data-storage&quot;&gt;Big data storage:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Scale up (vertical scaling): get yourself a bigger hard drive&lt;/li&gt;
&lt;li&gt;Lower latency&lt;/li&gt;
&lt;li&gt;Scale out (horizontal scaling)&lt;/li&gt;
&lt;li&gt;Higher latency&lt;/li&gt;
&lt;li&gt;Problem: one node get out of service 3 years averagely&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;-&amp;gt; Distributed file system&lt;/p&gt;
    
    </summary>
    
      <category term="Big Data" scheme="https://nancyyanyu.github.io/categories/Big-Data/"/>
    
    
      <category term="HDFS" scheme="https://nancyyanyu.github.io/tags/HDFS/"/>
    
      <category term="Hadoop" scheme="https://nancyyanyu.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning Basic Questions</title>
    <link href="https://nancyyanyu.github.io/posts/b89c7c35/"/>
    <id>https://nancyyanyu.github.io/posts/b89c7c35/</id>
    <published>2019-06-17T22:49:31.000Z</published>
    <updated>2019-06-18T21:46:20.896Z</updated>
    
    <content type="html"><![CDATA[<h3 id="can-you-state-tom-mitchells-definition-of-learning-and-discuss-t-p-and-e">1. Can you state Tom Mitchell's definition of learning and discuss T, P and E?</h3><p>Mitchell (1997) provides the definition “A computer program is said to learn from <strong>experience E</strong> with respect to some class of <strong>tasks T</strong> and <strong>performance measure P</strong>, if its performance at tasks in <strong>T</strong>, as measured by <strong>P</strong>, improves with experience <strong>E</strong>.</p><a id="more"></a><h3 id="what-can-be-different-types-of-tasks-encountered-in-machine-learning">2. What can be different types of tasks encountered in Machine Learning?</h3><p>Classification, regression, Machine translation, Anomaly detection, Density estimation or probability mass function estimation</p><h3 id="consider-linear-regression.-what-are-t-p-and-e">3. Consider linear regression. What are T, P and E?</h3><p>Task T : to predict y from <span class="math inline">\(x\)</span> by outputting <span class="math inline">\(\hat{y} = \mathbf{w}^T\mathbf{x}\)</span>.</p><p>Performance P: compute the mean squared error of the model on the test set. <span class="math display">\[MSE_{test}=\frac{1}{m}||\hat{y}^{test}-y^{test}||^2_2\]</span> Experience E: training set <span class="math inline">\((X^{train}, y^{train})\)</span>.</p><h3 id="what-are-supervised-unsupervised-semi-supervised-self-supervised-multi-instance-learning-and-reinforcement-learning">4. What are supervised, unsupervised, semi-supervised, self-supervised, multi-instance learning, and reinforcement learning?</h3><p><strong><em>Supervised learning:</em></strong> Training a model from input data and its corresponding labels.</p><p><strong><em>Unsupervised learning:</em></strong> Training a model to find patterns in a dataset, typically an unlabeled dataset.</p><p><strong><em>Semi-supervised learning:</em></strong> Training a model on data where some of the training examples have labels but others don’t. One technique for semi-supervised learning is to infer labels for the unlabeled examples, and then to train on the inferred labels to create a new model. Semi-supervised learning can be useful if labels are expensive to obtain but unlabeled examples are plentiful.</p><p><strong><em>Self-supervised learning:</em></strong> a relatively recent learning technique (in machine learning) where the <strong>training data is automatically labelled</strong>. It is still supervised learning, but the datasets do not need to be manually labelled by human, but they can e.g. be labelled by finding and exploiting the relations (or correlations) between different input signals (input coming e.g. from different sensor modalities).</p><p><strong><em>Multi-instance learning:</em></strong> a type of supervised learning. Instead of receiving a set of instances which are individually labeled, <strong>the learner receives a set of labeled <em>bags</em>, each containing many instances.</strong> In the simple case of multiple-instance <em>binary classification</em>, a bag may be labeled negative if all the instances in it are negative. On the other hand, a bag is labeled positive if there is at least one instance in it which is positive. From a collection of labeled bags, the learner tries to either (i) induce a concept that will label individual instances correctly or (ii) learn how to label bags without inducing the concept.</p><p><strong><em>Reinforcement learning:</em></strong> A machine learning approach to <strong>maximize an ultimate reward</strong> through feedback (rewards and punishments) after a sequence of actions. For example, the ultimate reward of most games is victory. Reinforcement learning systems can become expert at playing complex games by evaluating sequences of previous game moves that ultimately led to wins and sequences that ultimately led to losses.</p><p><strong><em>Reinforcement learning</em></strong> is learning what to do---how to map situations to actions---so as to maximize a numerical reward signal</p><h3 id="prove-that-for-linear-regression-mse-can-be-derived-from-maximal-likelihood-by-proper-assumptions.">5. Prove that for linear regression MSE can be derived from maximal likelihood by proper assumptions.</h3><p><strong>Probabilistic assumption</strong>:</p><ul><li>Assume that the target variables and the inputs are related via the equation:</li></ul><p><span class="math display">\[y^{(i)}=\theta^Tx^{(i)}+\epsilon^{(i)}\]</span></p><p>where <span class="math inline">\(\epsilon^{(i)}\)</span> is an error term that captures either unmodeled effects (such as if there are some features very pertinent to predicting housing price, but that we’d left out of the regression), or random noise.</p><ul><li><p>Assume <span class="math inline">\(\epsilon^{(i)}\)</span> are distributed IID (independently and identically distributed) according to a Gaussian distribution (also called a Normal distribution) mean zero and some variance <span class="math inline">\(\sigma^2\)</span></p><ul><li><p>The density of <span class="math inline">\(\epsilon^{(i)}\)</span> is: <span class="math display">\[p(\epsilon^{(i)})=\frac{1}{\sqrt{2\pi}\sigma}\exp \left(-\frac{(\epsilon^{(i)})^2}{2\sigma^2}\right)\]</span></p></li><li><p>This implies that:</p></li></ul><p><span class="math display">\[p(y^{(i)}|x^{(i)};\theta)=\frac{1}{\sqrt{2\pi}\sigma}\exp \left(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\right)\]</span></p></li></ul><p><strong>Likelihood function</strong>: <span class="math display">\[L(\theta)=L(\theta|\mathbf{X},\mathbf{y})=p(\mathbf{y}|\mathbf{X};\theta)\]</span> <span class="math inline">\(p(\mathbf{y}|\mathbf{X};\theta)\)</span>: This quantity is typically viewed a function of <span class="math inline">\(\mathbf{y}\)</span> (and perhaps X), for a fixed value of θ.</p><p>By the independence assumption on the <span class="math inline">\(\epsilon^{(i)}\)</span>’s (and hence also the <span class="math inline">\(y^{(i)}\)</span>’s given the <span class="math inline">\(x^{(i)}\)</span> ’s), this can also be written: <span class="math display">\[\begin{align}L(\theta)&amp;= \prod_{i=1}^n p(y^{(i)}|x^{(i)};\theta) \\&amp;=\prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma}\exp \left(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\right)\end{align}\]</span> The principal of <strong><em>maximum likelihood</em></strong> says that we should choose <span class="math inline">\(θ\)</span> so as to make the data as high probability as possible <span class="math inline">\(\rightarrow\)</span> maximize <span class="math inline">\(L(θ)\)</span>.</p><p>Instead of maximizing <span class="math inline">\(L(θ)\)</span>, we can also maximize any strictly increasing function of $L(θ) $ <span class="math inline">\(\rightarrow\)</span> <strong>log likelihood</strong> <span class="math inline">\(ℓ(θ)\)</span>: <span class="math display">\[\begin{align}ℓ(θ)=\log L(\theta)&amp;=\log \prod_{i=1}^n p(y^{(i)}|x^{(i)};\theta) \\&amp;=\sum_{i=1}^n \log \frac{1}{\sqrt{2\pi}\sigma}\exp \left(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\right) \\&amp;= n\log\frac{1}{\sqrt{2\pi}\sigma}-\frac{1}{2\sigma^2} \sum_{i=1}^n (y^{(i)}-\theta^Tx^{(i)})^2\end{align}\]</span> Hense, maximizing <span class="math inline">\(ℓ(θ)\)</span> gives the same answer as minimizing <span class="math display">\[\frac{1}{2}\sum_{i=1}^n(h_\theta(x^{(i)})-y^{(i)})^2 =J(\theta)\]</span> To summarize: Under the previous probabilistic assumptions on the data, least-squares regression corresponds to finding the maximum likelihood estimate of θ. Note also that, in our previous discussion, our final choice of θ did not depend on what was <span class="math inline">\(\sigma^2\)</span> , and indeed we’d have arrived at the same result even if <span class="math inline">\(\sigma^2\)</span> were unknown.</p><h3 id="derive-the-normal-equation-for-linear-regression.">6. Derive the normal equation for linear regression.</h3><p>Linear function: <span class="math display">\[h_θ(x) = θ_0 + θ_1x_1 + θ_2x_2=θ^Tx\]</span> Least-squares cost function: <span class="math display">\[J(\theta)=\frac{1}{2}\sum_{i=1}^n(h_\theta(x^{(i)})-y^{(i)})^2\]</span></p><p><span class="math display">\[\begin{align}\mathbf{X}&amp;=\begin{bmatrix}- (x^{(1)})^T -  \\- (x^{(2)})^T - \\ ...\\- (x^{(n)})^T -\end{bmatrix} \\\mathbf{y}&amp;=\begin{bmatrix} y^{(1)} \\y^{(2)} \\... \\y^{(n)}\end{bmatrix}   \\\mathbf{X}\theta-\mathbf{y}&amp;=\begin{bmatrix} (x^{(1)})^T\theta-y^{(1)} \\(x^{(2)})^T\theta-y^{(2)} \\... \\(x^{(n)})^T\theta-y^{(n)}\end{bmatrix} \\&amp;=\begin{bmatrix} (h_\theta(x^{(1)})-y^{(1)} \\h_\theta(x^{(2)})-y^{(2)} \\... \\h_\theta(x^{(1)})-y^{(n)}\end{bmatrix}\end{align}\]</span> For a vector <span class="math inline">\(z\)</span>, we have that: <span class="math inline">\(z^Tz=\sum_i z^2_i\)</span> <span class="math display">\[\frac{1}{2}(\mathbf{X}\theta-\mathbf{y})^T(\mathbf{X}\theta-\mathbf{y})=\frac{1}{2}\sum_{i=1}^n(h_\theta(x^{(i)})-y^{(i)})^2 =J(\theta)\]</span> We know that: <span class="math display">\[\begin{align}\frac{\partial f(A)}{\partial A^T}&amp;=(\frac{\partial f(A)}{\partial A})^T \\\frac{\partial \mathbf{y}^T\mathbf{A}\mathbf{x}}{\partial \mathbf{x}}&amp;=\mathbf{y}^T\mathbf{A} \\\frac{\partial \mathbf{y}^T\mathbf{A}\mathbf{x}}{\partial \mathbf{y}}&amp;=\frac{\partial \mathbf{x}^T\mathbf{A}^T\mathbf{y}}{\partial \mathbf{y}}=\mathbf{x}^T\mathbf{A}^T \\\frac{\partial \mathbf{x}^T\mathbf{A}\mathbf{x}}{\partial \mathbf{x}}&amp;=\mathbf{x}^T\mathbf{A}^T +\mathbf{x}^T\mathbf{A}=\mathbf{x}^T（\mathbf{A}^T +\mathbf{A}）\\\end{align}\]</span> To minimize <span class="math inline">\(J\)</span>, let’s find its derivatives with respect to <span class="math inline">\(θ\)</span>: <span class="math display">\[\begin{align}\frac{\partial J(\theta)}{\partial \theta}&amp;= \frac{\partial \frac{1}{2}(\mathbf{X}\theta-\mathbf{y})^T(\mathbf{X}\theta-\mathbf{y})}{\partial \theta}\\&amp;= \frac{1}{2}\frac{\partial (\theta^T\mathbf{X}^T\mathbf{X}\theta-\theta^T\mathbf{X}^T\mathbf{y}-\mathbf{y}^T\mathbf{X}\theta+\mathbf{y}^T\mathbf{y})}{\partial \theta} \\&amp;=\frac{1}{2}\frac{\partial (\theta^T\mathbf{X}^T\mathbf{X}\theta-\theta^T\mathbf{X}^T\mathbf{y}-\mathbf{y}^T\mathbf{X}\theta+\mathbf{y}^T\mathbf{y})}{\partial \theta} \\&amp;=\frac{1}{2} (\theta^T\mathbf{X}^T\mathbf{X}+\theta^T\mathbf{X}^T\mathbf{X}-\mathbf{y}^T\mathbf{X}-\mathbf{y}^T\mathbf{X}  )\\&amp;=\frac{1}{2}(\mathbf{X}^T\mathbf{X}\theta-2\mathbf{y}^T\mathbf{X}) \\&amp;=\mathbf{X}^T\mathbf{X}\theta-\mathbf{X}^T\mathbf{y}=0\end{align}\]</span> <strong><em>Normal Equation</em></strong>: <span class="math display">\[\theta=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\]</span></p><h3 id="why-is-a-validation-set-necessary">7. Why is a validation set necessary?</h3><p>Let's assume that you are training a model whose performance depends on a set of hyperparameters. In the case of a neural network, these parameters may be for instance the learning rate or the number of training iterations.</p><p>Given a choice of hyperparameter values, you use the <strong>training</strong> set to train the model. But, how do you set the values for the hyperparameters? That's what the <strong>validation</strong> set is for. You can use it to evaluate the performance of your model for different combinations of hyperparameter values (e.g. by means of a grid search process) and keep the best trained model.</p><p>But, how does your selected model compares to other different models? Is your neural network performing better than, let's say, a random forest trained with the same combination of training/test data? You cannot compare based on the validation set, because that validation set was part of the fitting of your model. You used it to select the hyperparameter values!</p><p>The <strong>test</strong> set allows you to compare different models in an unbiased way, by basing your comparisons in data that were not use in any part of your training/hyperparameter selection process.</p><h3 id="what-is-the-no-free-lunch-theorem-in-connection-to-machine-learning">8. What is the no free lunch theorem in connection to Machine Learning?</h3><p>The <strong><em>no free lunch theorem</em></strong> for machine learning (Wolpert, 1996) states that, <strong>averaged over</strong> <strong>all possible data generating distributions, every classification algorithm has the</strong> <strong>same error rate when classifying previously unobserved points</strong>. In other words, in some sense, <strong>no machine learning algorithm is universally any better than any</strong> <strong>other.</strong> The most sophisticated algorithm we can conceive of has the same average performance (over all possible tasks) as merely predicting that every point belongs to the same class.</p><p>Fortunately, these results hold only when we average over all possible data generating distributions. **If we make assumptions about the kinds of probability<em> distributions we encounter in real-world applications,</em> then we can design learning algorithms that perform well on these distributions.</p><p>This means that the goal of machine learning research is not to seek a universal learning algorithm or the absolute best learning algorithm. Instead, <em>our goal is to</em> <em>understand what kinds of distributions are relevant to the “real world” that an AI</em> <em>agent experiences,</em> and what kinds of machine learning algorithms perform well on data drawn from the kinds of data generating distributions we care about.</p><h3 id="discuss-training-error-test-error-generalization-error-overfitting-and-underfitting.">9. Discuss training error, test error, generalization error, overfitting, and underfitting.</h3><p><strong><em>overfitting:</em></strong> the gap between the training error and test error is too large</p><p><strong><em>underfitting:</em></strong> the model is not able to obtain a sufficiently low error value on the training set</p><p><strong><em>training error:</em></strong> when training a machine learning model, we have access to a training set, we can compute some error measure on the training set</p><p><strong><em>generalization error/test error:</em></strong> the expected value of the error on a new input. Here the expectation is taken across different possible inputs, drawn from the distribution of inputs we expect the system to encounter in practice</p><p><img src="./1.png" width="600"></p><h3 id="compare-representational-capacity-vs.-effective-capacity-of-a-model.">10. Compare representational capacity vs. effective capacity of a model.</h3><ul><li><strong>Representational capacity</strong> - the functions which the model <em>can</em> learn; The model specifies which <strong>family of functions</strong> the learning algorithm can choose from when varying the parameters in order to reduce a training objective.</li><li><strong>Effective capacity</strong> - in practice, a learning algorithm is not likely to find the <em>best</em> function out of the possible functions it can learn, though it can learn one that performs exceptionally well - those functions that the learning algorithm is capable of finding defines the model's <em>effective</em> capacity.</li></ul><p>These additional limitations, such as the imperfection of the optimization algorithm, mean that the learning algorithm’s <strong><em>effective capacity</em></strong> may be less than the <strong><em>representational capacity</em></strong> of the model family</p><h3 id="what-is-an-ideal-model-what-is-bayes-error-what-isare-the-sources-of-bayes-error-occur">11. What is an ideal model? What is Bayes error? What is/are the source(s) of Bayes error occur?</h3><p><strong><em>The ideal model</em></strong>: is an oracle that simply knows the true probability distribution that generates the data.</p><ul><li>Even such a model will still incur some error on many problems, because there may still be some noise in the distribution. In the case of supervised learning, the mapping from <span class="math inline">\(x\)</span> to <span class="math inline">\(y\)</span> may be inherently stochastic, or <span class="math inline">\(y\)</span> may be a deterministic function that involves other variables besides those included in <span class="math inline">\(x\)</span>.</li></ul><p><strong><em>Bayes error</em></strong>: the lowest possible prediction error that can be achieved and is the same as irreducible error. ; The error incurred by an oracle making predictions from the true distribution p(x, y).</p><p><strong><em>Source(s) of Bayes error occur</em></strong>: noise in the distribution if the process is random</p><h3 id="what-are-nonparametric-models-what-is-nonparametric-learning">12. What are nonparametric models? What is nonparametric learning?</h3><p>Parametric models: learn a function described by a parameter vector whose size is finite and fixed before any data is observed (linear regression)</p><p>Non-parametric models: assume that the data distribution cannot be defined in terms of a finite set of parameters. But they can often be defined by assuming an infinite dimensional <span class="math inline">\(\theta\)</span> . Usually we think of <span class="math inline">\(\theta\)</span> as a function (nearest neighbor regression)</p><h3 id="what-is-regularization-intuitively-what-does-regularization-do-during-the-optimization-procedure">13. What is regularization? Intuitively, what does regularization do during the optimization procedure?</h3><p><strong><em>Regularization</em></strong> is any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error.</p><p>We regularize a model that learns a function <span class="math inline">\(f(x; θ)\)</span> by adding a penalty called a <strong>regularizer</strong> to the cost function. <strong>Expressing preferences for one function over another</strong> implicitly and explicitly is a more general way of controlling a model’s capacity than including or excluding members from the hypothesis space.</p><h3 id="what-is-weight-decay-what-is-it-added">14. What is weight decay? What is it added?</h3><p><strong><em>Weight decay</em></strong> is an additional term that causes the weights to exponentially decay to zero.</p><p>To perform linear regression with <strong>weight decay</strong>, we minimize a sum comprising both the mean squared error on the training and a criterion <span class="math inline">\(J (w)\)</span> that expresses a preference for the weights to have smaller squared L2 norm. Specifically, <span class="math display">\[J(w) = MSE_{train} + λ\mathbf{w}^T\mathbf{w}\]</span> Minimizing <span class="math inline">\(J(w)\)</span> results in a choice of weights that make a tradeoff between fitting the training data and being small. This gives us solutions that have a smaller slope, or put weight on fewer of the features.</p><h3 id="what-is-a-hyperparameter-how-do-you-choose-which-settings-are-going-to-be-hyperparameters-and-which-are-going-to-be-learnt">15. What is a hyperparameter? How do you choose which settings are going to be hyperparameters and which are going to be learnt?</h3><p><strong>Hyperparameter</strong>: Most machine learning algorithms have several settings that we can use to control the behavior of the learning algorithm.</p><ul><li>The values of hyperparameters are not adapted by the learning algorithm itself</li></ul><p>Sometimes a setting is chosen to be a hyperparameter that the learning algorithm does not learn because it is <strong>difficult to optimize</strong> or it is not appropriate to learn that hyperparameter on the training set. This applies to all hyperparameters that control model capacity. If learned on the training set, such hyperparameters would always choose the maximum possible model capacity, resulting in <strong>overfitting</strong></p><h3 id="why-is-maximal-likelihood-the-preferred-estimator-in-ml">16. Why is maximal likelihood the preferred estimator in ML?</h3><p>The main appeal of the maximum likelihood estimator is that it can be shown to be the best estimator asymptotically, as the number of examples m → ∞, in terms of its rate of convergence as m increases</p><p>Under appropriate conditions, the maximum likelihood estimator has the property of :</p><ul><li><strong>consistency</strong>: as the number of training examples approaches infinity, the maximum likelihood estimate of a parameter converges to the true value of the parameter. <span class="math inline">\(\hat{\theta} \rightarrow^{n \rightarrow \infin} \theta\)</span>.</li><li><strong>efficiency</strong>: the Cramér-Rao lower bound (Rao, 1945; Cramér, 1946) shows that no consistent estimator has a lower mean squared error <span class="math inline">\(Var(\hat{\theta}_n)\)</span> than the maximum likelihood estimator</li></ul><p>When the number of examples is small enough to yield overfitting behavior, regularization strategies such as weight decay may be used to obtain a biased version of maximum likelihood that has less variance when training data is limited.</p><h3 id="under-what-conditions-do-the-maximal-likelihood-estimator-guarantee-consistency">17. Under what conditions do the maximal likelihood estimator guarantee consistency?</h3><ol type="1"><li>The true distribution <span class="math inline">\(p_{data}\)</span> must lie within the model family <span class="math inline">\(p_{model}(·; θ)\)</span>. Otherwise, no estimator can recover <span class="math inline">\(p_{data}\)</span>.</li><li>The true distribution <span class="math inline">\(p_{data}\)</span> must correspond to exactly one value of <span class="math inline">\(θ\)</span>. Otherwise, maximum likelihood can recover the correct <span class="math inline">\(p_{data}\)</span> , but will not be able to determine which value of <span class="math inline">\(θ\)</span> was used by the data generating processing.</li></ol><h3 id="what-do-you-mean-by-affine-transformation-discuss-affine-vs.-linear-transformation.">18. What do you mean by affine transformation? Discuss affine vs. linear transformation.</h3><p>A function 𝑓 is linear if <span class="math inline">\(𝑓(𝑎𝑥+𝑏𝑦)=𝑎𝑓(𝑥)+𝑏𝑓(𝑦)\)</span> for all relevant values of 𝑎, 𝑏, 𝑥 and 𝑦.</p><p>A function 𝑔 is affine if <span class="math inline">\(𝑔(𝑥)=𝑓(𝑥)+𝑐\)</span> for some linear function 𝑓 and constant 𝑐. Note that we allow 𝑐=0, which implies that every linear function is an affine function.</p><ol type="1"><li>All linear transformations are affine transformations.</li><li>Not all affine transformations are linear transformations.</li><li>It can be shown that any affine transformation 𝐴:𝑈→𝑉 can be written as 𝐴(𝑥)=𝐿(𝑥)+𝑣0, where 𝑣0 is some vector from 𝑉 and 𝐿:𝑈→𝑉 is a linear transformation.</li></ol><p>Discuss VC dimension.</p><p>The VC dimension measures the capacity of a binary classifier. The VC dimension is defined as being the largest possible value of m for which there exists a training set of m different x points that the classifier can label arbitrarily.</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;can-you-state-tom-mitchells-definition-of-learning-and-discuss-t-p-and-e&quot;&gt;1. Can you state Tom Mitchell&#39;s definition of learning and discuss T, P and E?&lt;/h3&gt;
&lt;p&gt;Mitchell (1997) provides the definition “A computer program is said to learn from &lt;strong&gt;experience E&lt;/strong&gt; with respect to some class of &lt;strong&gt;tasks T&lt;/strong&gt; and &lt;strong&gt;performance measure P&lt;/strong&gt;, if its performance at tasks in &lt;strong&gt;T&lt;/strong&gt;, as measured by &lt;strong&gt;P&lt;/strong&gt;, improves with experience &lt;strong&gt;E&lt;/strong&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Interview" scheme="https://nancyyanyu.github.io/tags/Interview/"/>
    
  </entry>
  
  <entry>
    <title>ESL Note: Model Averaging and Stacking</title>
    <link href="https://nancyyanyu.github.io/posts/4f6e00ef/"/>
    <id>https://nancyyanyu.github.io/posts/4f6e00ef/</id>
    <published>2019-06-17T21:21:28.000Z</published>
    <updated>2019-06-18T17:33:58.897Z</updated>
    
    <content type="html"><![CDATA[<h2 id="bayesian-model-averaging">Bayesian Model Averaging</h2><p>We have a set of candidate models <span class="math inline">\(M_m\)</span>; m = 1,…,M for our training set <span class="math inline">\(Z\)</span>.</p><p><strong>Suppose</strong> <span class="math inline">\(\zeta\)</span> is some quantity of interest, for example, a prediction f(x) at some fixed feature value <span class="math inline">\(x\)</span>. The <strong><em>posterior distribution</em></strong> of <span class="math inline">\(\zeta\)</span> is <span class="math display">\[\Pr(\zeta|\mathbf{Z})=\sum_{i=1}^M\Pr(\zeta|M_m,\mathbf{Z})\Pr(M_m| \mathbf{Z})\]</span> with <strong><em>posterior mean</em></strong>: <span class="math display">\[E(\zeta|\mathbf{Z})=\sum_{i=1}^ME(\zeta|M_m,\mathbf{Z})\Pr(M_m| \mathbf{Z})\]</span> This Bayesian prediction is a weighted average of the individual predictions, with weights proportional to the posterior probability of each model.</p><h3 id="frequentist-viewpoint">Frequentist Viewpoint</h3><p>Given predictions <span class="math inline">\(\hat{f}_1(x); \hat{f}_2(x),…, \hat{f}_M(x)\)</span>, under squared-error loss, we can seek the weights $w = (w_1, w_2,…, w_M) $ such that <span class="math display">\[\hat{w}=\arg \min_w E_P[Y-\sum_{i=1}^Mw_m\hat{f}_m(x)]^2\]</span> Here the input value <span class="math inline">\(x\)</span> is fixed and the <span class="math inline">\(N\)</span> observations in the dataset <span class="math inline">\(Z\)</span> (and the target <span class="math inline">\(Y\)</span> ) are distributed according to <span class="math inline">\(P\)</span>. The solution is the population linear regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(\hat{F}(x)^T=[\hat{f}_1(x); \hat{f}_2(x),…, \hat{f}_M(x)]\)</span> : <span class="math display">\[\hat{w}=E_P[\hat{F}(x)\hat{F}(x)^T]^{-1}E_P[\hat{F}(x)Y]\]</span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;bayesian-model-averaging&quot;&gt;Bayesian Model Averaging&lt;/h2&gt;
&lt;p&gt;We have a set of candidate models &lt;span class=&quot;math inline&quot;&gt;\(M_m\)&lt;/span
      
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Model Inference" scheme="https://nancyyanyu.github.io/tags/Model-Inference/"/>
    
  </entry>
  
  <entry>
    <title>Deep Learning Questions Part I: UAT, Motivation</title>
    <link href="https://nancyyanyu.github.io/posts/c7bd9d66/"/>
    <id>https://nancyyanyu.github.io/posts/c7bd9d66/</id>
    <published>2019-06-17T20:22:02.000Z</published>
    <updated>2019-06-18T21:46:15.099Z</updated>
    
    <content type="html"><![CDATA[<h2 id="universal-approximation-of-neural-networks">Universal Approximation of neural networks</h2><h3 id="state-the-universal-approximation-theorem-what-is-the-technique-used-to-prove-that">1. State the universal approximation theorem? What is the technique used to prove that?</h3><p><strong>Universal approximation theorem</strong> (Hornik et al., 1989; Cybenko, 1989) states that a feedforward network with a linear output layer and at least one hidden layer with any “squashing” activation function (such as the logistic sigmoid activation function) can approximate any Borel measurable function from one finite-dimensional space to another with any desired non-zero amount of error, provided that the network is given enough hidden units.</p><p>The universal approximation theorem means that <strong>regardless of what function we are trying to learn, we know that a large MLP will be able to represent this function.</strong></p><p>However, we are not guaranteed that the training algorithm will be able to learn that function. Even if the MLP is able to represent the function, learning can fail for two different reasons.</p><ol type="1"><li>The <strong>optimization algorithm</strong> used for training may not be able to find the value of the parameters that corresponds to the desired function.</li><li>The training algorithm might <strong>choose the wrong function due to overfitting</strong></li></ol><p>The universal approximation theorem says that there exists a network large enough to achieve any degree of accuracy we desire, but the theorem does not say how large this network will be.</p><h3 id="what-is-a-borel-measurable-function">2. What is a Borel measurable function?</h3><p>Any continuous function on a closed and bounded subset of <span class="math inline">\(R^n\)</span> is Borel measurable and therefore may be approximated by a neural network.</p><a id="more"></a><h2 id="deep-learning-motivation">Deep Learning motivation</h2><ol type="1"><li>What is the mathematical motivation of Deep Learning as opposed to standard Machine Learning techniques?</li><li>In standard Machine Learning vs. Deep Learning, how is the order of number of samples related to the order of regions that can be recognized in the function space?</li><li>What are the reasons for choosing a deep model as opposed to shallow model? (1. Number of regions O(2^k) vs O(k) where k is the number of training examples 2. # linear regions carved out in the function space depends exponentially on the depth. )</li><li>How Deep Learning tackles the curse of dimensionality?</li></ol><h2 id="general-questions">General questions</h2><ol type="1"><li>How will you implement dropout during forward and backward pass?</li><li>What do you do if Neural network training loss/testing loss stays constant? (ask if there could be an error in your code, going deeper, going simpler…)</li><li>Why do RNNs have a tendency to suffer from exploding/vanishing gradient? How to prevent this? (Talk about LSTM cell which helps the gradient from vanishing, but make sure you know why it does so. Talk about gradient clipping, and discuss whether to clip the gradient element wise, or clip the norm of the gradient.)</li><li>Do you know GAN, VAE, and memory augmented neural network? Can you talk about it?</li><li>Does using full batch means that the convergence is always better given unlimited power? (Beautiful explanation by Alex Seewald: https://www.quora.com/Is-full-batch-gradient-descent-with-unlimited-computer-power-always-better-than-mini-batch-gradient-descent)</li><li>What is the problem with sigmoid during backpropagation? (Very small, between 0.25 and zero.)</li><li>Given a black box machine learning algorithm that you can’t modify, how could you improve its error? (you can transform the input for example.)</li><li>How to find the best hyper parameters? (Random search, grid search, Bayesian search (and what it is?))</li><li>What is transfer learning?</li><li>Compare and contrast L1-loss vs. L2-loss and L1-regularization vs. L2-regularization.</li></ol><p><strong>Ref</strong>:</p><p><a href="https://github.com/Sroy20/machine-learning-interview-questions" target="_blank" rel="noopener">machine-learning-interview-questions</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;universal-approximation-of-neural-networks&quot;&gt;Universal Approximation of neural networks&lt;/h2&gt;
&lt;h3 id=&quot;state-the-universal-approximation-theorem-what-is-the-technique-used-to-prove-that&quot;&gt;1. State the universal approximation theorem? What is the technique used to prove that?&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Universal approximation theorem&lt;/strong&gt; (Hornik et al., 1989; Cybenko, 1989) states that a feedforward network with a linear output layer and at least one hidden layer with any “squashing” activation function (such as the logistic sigmoid activation function) can approximate any Borel measurable function from one finite-dimensional space to another with any desired non-zero amount of error, provided that the network is given enough hidden units.&lt;/p&gt;
&lt;p&gt;The universal approximation theorem means that &lt;strong&gt;regardless of what function we are trying to learn, we know that a large MLP will be able to represent this function.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;However, we are not guaranteed that the training algorithm will be able to learn that function. Even if the MLP is able to represent the function, learning can fail for two different reasons.&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;The &lt;strong&gt;optimization algorithm&lt;/strong&gt; used for training may not be able to find the value of the parameters that corresponds to the desired function.&lt;/li&gt;
&lt;li&gt;The training algorithm might &lt;strong&gt;choose the wrong function due to overfitting&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The universal approximation theorem says that there exists a network large enough to achieve any degree of accuracy we desire, but the theorem does not say how large this network will be.&lt;/p&gt;
&lt;h3 id=&quot;what-is-a-borel-measurable-function&quot;&gt;2. What is a Borel measurable function?&lt;/h3&gt;
&lt;p&gt;Any continuous function on a closed and bounded subset of &lt;span class=&quot;math inline&quot;&gt;\(R^n\)&lt;/span&gt; is Borel measurable and therefore may be approximated by a neural network.&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Interview" scheme="https://nancyyanyu.github.io/tags/Interview/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning Questions - Part IV: Clustering &amp; Bayesian</title>
    <link href="https://nancyyanyu.github.io/posts/25b6d1fa/"/>
    <id>https://nancyyanyu.github.io/posts/25b6d1fa/</id>
    <published>2019-06-17T20:22:00.000Z</published>
    <updated>2019-06-18T21:45:53.718Z</updated>
    
    <content type="html"><![CDATA[<h2 id="clustering">Clustering</h2><h3 id="describe-the-k-means-algorithm.">1. Describe the k-means algorithm.</h3><p><strong>K-means clustering</strong> is a simple and elegant approach for partitioning a data set into K distinct, <strong><em>non-overlapping</em></strong> clusters.</p><p>The idea behind <strong>K-means clustering</strong> is that a <em>good</em> clustering is one for which the <strong><em>within-cluster</em></strong> <strong><em>variation</em></strong> is as small as possible.</p><p>The <strong>within-cluster variation</strong> for cluster <span class="math inline">\(C_k\)</span> is a measure <span class="math inline">\(W(C_k)\)</span> of the amount by which the observations within a cluster differ from each other.</p><a id="more"></a><p><strong>Define the within-cluster variation</strong>: <strong><em>Euclidean distance</em></strong>: <span class="math display">\[W(C_k)=\frac{1}{|C_k|}\sum_{i,i^{&#39;}\in C_k}\sum_{j=1}^p(x_{ij}-x_{i^{&#39;}j})^2\]</span></p><ul><li>where <span class="math inline">\(|C_k|\)</span> denotes the number of observations in the kth cluster.</li><li>The within-cluster variation for the kth cluster is <em>the sum of all of the pairwise squared Euclidean distances between the observations in the kth cluster</em>, divided by the total number of observations in the kth cluster.</li></ul><p><strong>Objective funtion</strong>: <span class="math display">\[\min_{C_1,...,C_K}\left\{ \sum_{i=1}^K\frac{1}{|C_k|}\sum_{i,i^{&#39;}\in C_k}\sum_{j=1}^p(x_{ij}-x_{i^{&#39;}j})^2\right\}\]</span></p><ul><li>partition the observations into K clusters such that the total within-cluster variation, summed over all K clusters, is <em>as small as possible</em>.</li></ul><p><strong>Algorithm</strong></p><p><img src="./1.png" width="600"></p><p>In the algorithm above, <span class="math inline">\(k\)</span> (a parameter of the algorithm) is the number of clusters we want to find; and the cluster <strong>centroids</strong> <span class="math inline">\(\mu_j\)</span> represent our current guesses for the positions of the centers of the clusters. To initialize the cluster centroids, we could choose <span class="math inline">\(k\)</span> training examples randomly, and set the cluster centroids to be equal to the values of these k examples. (Other initialization methods are also possible.)</p><p>The inner-loop of the algorithm repeatedly carries out two steps:</p><ul><li>“Assigning” each training example <span class="math inline">\(x^{(i)}\)</span> to the closest cluster centroid <span class="math inline">\(\mu_j\)</span>, and</li><li>Moving each cluster centroid <span class="math inline">\(\mu_j\)</span> to the mean of the points assigned to it.</li></ul><p><strong>Local optimum</strong> : This means that as the algorithmis run, the clustering obtained will continually improve until the result no longer changes; the objective will never increase.</p><ul><li>It is important to run the algorithm multiple times from different random initial configurations, because the results obtained will depend on the initial (random) cluster assignmentof each observation in Step 1 of Algorithm 10.1</li></ul><h3 id="what-is-distortion-function-is-it-convex-or-non-convex">2. What is distortion function? Is it convex or non-convex?</h3><p><strong>Distortion function</strong>: <span class="math display">\[J(c,\mu)=\sum_{i=1}^n ||x^{(i)}-\mu_{c^{(i)}}||^2\]</span> <span class="math inline">\(J\)</span> measures the sum of squared distances between each training example <span class="math inline">\(x^{(i)}\)</span> and the cluster centroid <span class="math inline">\(\mu_{c^{(i)}}\)</span> to which it has been assigned.</p><ul><li>k-means is exactly <em>coordinate descent</em> 坐标下降 on <span class="math inline">\(J\)</span>. Specifically, the inner-loop of k-means repeatedly minimizes <span class="math inline">\(J\)</span> with respect to <span class="math inline">\(c\)</span> while holding <span class="math inline">\(\mu\)</span> fixed, and then minimizes <span class="math inline">\(J\)</span> with respect to <span class="math inline">\(\mu\)</span> while holding <span class="math inline">\(c\)</span> fixed. Thus,<span class="math inline">\(J\)</span> must monotonically decrease, and the value of <span class="math inline">\(J\)</span> must converge.</li></ul><p>The distortion function <span class="math inline">\(J\)</span> is a <strong><em>non-convex</em></strong> function, and so coordinate descent on <span class="math inline">\(J\)</span> is not guaranteed to converge to the global minimum.</p><h3 id="describe-the-em-algorithm-intuitively.">3. Describe the EM algorithm intuitively.</h3><ol start="4" type="1"><li>What is the Gaussian Mixture Model?</li><li>What are the two steps of the EM algorithm</li><li>Compare GMM vs GDA.</li></ol><h2 id="bayesian-machine-learning">Bayesian Machine Learning</h2><h3 id="what-are-the-differences-between-bayesian-and-freqentist-approach-for-machine-learning">1. What are the differences between “Bayesian” and “Freqentist” approach for Machine Learning?</h3><p>The <strong>Bayesian</strong> approach differs from the standard (&quot;<strong>frequentist</strong>&quot;) method for inference in its use of a <em>prior distribution</em> to express the uncertainty present before seeing the data, and to allow the uncertainty remaining after seeing the data to be expressed in the form of a <em>posterior distribution</em>.</p><p>Given a specific set of data, the <strong>frequentist</strong> believes that there is a true, underlying distribution from which said data was generated. The inability to get the exact parameters is a function of finite sample size. The <strong>Bayesian</strong>, on the other hand, think that we start with some assumption about the parameters (even if unknowingly) and use the data to refine our opinion about those parameters.</p><ul><li><p>The <strong>Bayesian</strong> probability measures a &quot;degree of belief&quot;. It pretty much matches our every-day intuitive understanding of probability,</p></li><li><p>The <strong>frequentists</strong> interpretation needs some explanation. Frequentists can assign probabilities only to events/obervations that come from repeatable experiments. With &quot;<em>probability of an event</em>&quot; they mean the relative frequency of the event occuring in an infinitively long series of repetitions. For instance, when a frequentists says that the probability for &quot;heads&quot; in a coin toss is 0.5 (50%) he means that in infinititively many such coin tosses, 50% of the coins will show &quot;head&quot;.</p></li></ul><p><strong>Frequentist:</strong> best suited to falsify a hypothesis <strong>Bayesian:</strong> best suited to (re)allocate the credibility of a statement</p><h4 id="downsides-of-frequentists">Downsides of Frequentists</h4><ul><li><p>Frequentists approach relies on data more than Bayesian as we totally ignore our knowledge or logical thinking which have been introduced in a form of prior probability.</p></li><li><p>P-value does not provide the probability of your hypothesis to be collect. It only avoids the most extreme value that seems to be rare. It sometimes make the situation difficult as you may find it challenging to explain the actual meaning of value. Whereas, the posterior probability describes in percentage how likely your hypothesis is correct based on our prior knowledge.</p></li></ul><h4 id="downsides-of-bayesians">Downsides of Bayesians</h4><ul><li>Bayesian Statistic requires more mathematical knowledge since the formula requires us to deduce two probability distributions.</li><li>What if your prior has become meaningless as the logic we have is no longer valid? (Some articles suggest that the prior at early stage can be any number as it can be updated as more information comes in)</li></ul><h3 id="when-will-you-use-bayesian-methods-instead-of-frequentist-methods">2. When will you use Bayesian methods instead of Frequentist methods?</h3><p>Small dataset, large feature set</p><h3 id="compare-maximum-likelihood-and-maximum-a-posteriori-estimation.">3. Compare maximum likelihood and maximum a posteriori estimation.</h3><p><strong><em>MLE</em></strong>:</p><p>Likelihood function: <span class="math inline">\(P(X|\theta)\)</span></p><p>MLE for θ, the parameter we want to infer: <span class="math display">\[\begin{align}\theta_{MLE}&amp;=\arg \max_\theta P(X|\theta) \\&amp;=\arg \max_\theta \prod_i P(x_i|\theta)\end{align}\]</span> As taking a product of some numbers less than 1 would approaching 0 as the number of those numbers goes to infinity, it would be not practical to compute, because of computation underflow. Hence, we will instead work in the log space, as logarithm is monotonically increasing, so maximizing a function is equal to maximizing the log of that function. <span class="math display">\[\begin{align}\theta_{MLE}&amp;=\arg \max_\theta \log P(X|\theta) \\&amp;=\arg \max_\theta \log \prod_i P(x_i|\theta) \\&amp;=\arg \max_\theta \sum_i \log  P(x_i|\theta)\end{align}\]</span> To use this framework, we just need to derive the log likelihood of our model, then maximizing it with regard of θθ using our favorite optimization algorithm like Gradient Descent.</p><p><strong><em>MAP</em></strong>:</p><p>MAP usually comes up in Bayesian setting. Because, as the name suggests, it works on a posterior distribution, not only the likelihood.</p><p>Bayes’ rule: <span class="math display">\[P(\theta|X)=\frac{P(X|\theta)P(\theta)}{P(X)} \\\propto P(X|\theta)P(\theta)\]</span></p><p><span class="math display">\[\begin{align}\theta_{MAP}&amp;=\arg \max_\theta P(X|\theta) P(\theta) \\&amp;=\arg \max_\theta \log P(X|\theta) P(\theta) \\&amp;=\arg \max_\theta \log \prod_i P(x_i|\theta) P(\theta)\\&amp;=\arg \max_\theta \sum_i \log  P(x_i|\theta)P(\theta) \end{align}\]</span></p><p>Comparing both MLE and MAP equation, the only thing differs is the <strong>inclusion of prior P(θ)</strong> in MAP. What it means is that, <strong>the likelihood is now weighted with some weight coming from the prior</strong>.</p><p>Let’s consider what if we use the simplest prior in our MAP estimation, i.e. <strong><em>uniform prior.</em></strong> This means, we assign equal weights everywhere, on all possible values of the θ. For example ,our prior P(θ) is <span class="math inline">\(\frac{1}{6}\)</span> <span class="math display">\[\begin{align}\theta_{MAP} &amp;=\arg \max_\theta \sum_i \log  P(x_i|\theta)P(\theta) \\&amp;= \arg \max_\theta \sum_i \log  P(x_i|\theta) const \\&amp;= \arg \max_\theta \sum_i \log  P(x_i|\theta) \\&amp;= \theta_{MLE}\end{align}\]</span></p><p><strong>Ref</strong>:</p><p><a href="https://github.com/Sroy20/machine-learning-interview-questions" target="_blank" rel="noopener">machine-learning-interview-questions</a></p><p><a href="%5Bhttp://cs229.stanford.edu/notes-spring2019/cs229-notes7a.pdf%5D(http://cs229.stanford.edu/notes-spring2019/cs229-notes7a.pdf)">CS229 Lecture notes: Unsupervised Learning, k-means clustering</a></p><p><a href="%5Bhttp://cs229.stanford.edu/notes-spring2019/cs229-notes7b.pdf%5D(http://cs229.stanford.edu/notes-spring2019/cs229-notes7b.pdf)">CS229 Lecture notes: Mixtures of Gaussians and the EM algorithm</a></p><p><a href="https://medium.com/@yongddeng/a-meaningless-debate-frequentists-vs-bayesians-7317317b458f" target="_blank" rel="noopener">A Meaningless Debate: Frequentists vs Bayesians</a></p><p><a href="https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading20.pdf" target="_blank" rel="noopener">Comparison of frequentist and Bayesian inference</a></p><p><a href="https://wiseodd.github.io/techblog/2017/01/01/mle-vs-map/" target="_blank" rel="noopener">MLE vs MAP: the connection between Maximum Likelihood and Maximum A Posteriori Estimation</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;clustering&quot;&gt;Clustering&lt;/h2&gt;
&lt;h3 id=&quot;describe-the-k-means-algorithm.&quot;&gt;1. Describe the k-means algorithm.&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;K-means clustering&lt;/strong&gt; is a simple and elegant approach for partitioning a data set into K distinct, &lt;strong&gt;&lt;em&gt;non-overlapping&lt;/em&gt;&lt;/strong&gt; clusters.&lt;/p&gt;
&lt;p&gt;The idea behind &lt;strong&gt;K-means clustering&lt;/strong&gt; is that a &lt;em&gt;good&lt;/em&gt; clustering is one for which the &lt;strong&gt;&lt;em&gt;within-cluster&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;variation&lt;/em&gt;&lt;/strong&gt; is as small as possible.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;within-cluster variation&lt;/strong&gt; for cluster &lt;span class=&quot;math inline&quot;&gt;\(C_k\)&lt;/span&gt; is a measure &lt;span class=&quot;math inline&quot;&gt;\(W(C_k)\)&lt;/span&gt; of the amount by which the observations within a cluster differ from each other.&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Interview" scheme="https://nancyyanyu.github.io/tags/Interview/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning Questions Part III: SVM</title>
    <link href="https://nancyyanyu.github.io/posts/c8f688ba/"/>
    <id>https://nancyyanyu.github.io/posts/c8f688ba/</id>
    <published>2019-06-17T18:07:59.000Z</published>
    <updated>2019-06-18T21:46:08.638Z</updated>
    
    <content type="html"><![CDATA[<h2 id="support-vector-machine">Support Vector Machine</h2><h3 id="svm-v.s.-logistic-regression">1. SVM v.s. Logistic Regression</h3><p><strong>SVM Optimization problem</strong>: <span class="math display">\[\max_{\beta_0,...\beta_p,\epsilon_1,..,\epsilon_n} M \\s.t.  \sum_{j=1}^p \beta_j^2=1,  \quad (9.13) \\ y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2},...+\beta_px_{ip})&gt;M(1-\epsilon_i) \quad \forall i=1,..,n.  \quad (9.14) \\ \epsilon_i\geq0,\sum_{i=1}^p\epsilon_i \leq C, \quad (9.15)\]</span> Rewrite the criterion (9.12)–(9.15) for fitting the support vector classifier <span class="math inline">\(f(X) = β_0 + β_1X_1 + . . . + β_pX_p\)</span> as <span class="math display">\[\min_{\beta_0,...,\beta_p}\left\{ \sum_{i=1}^n\max[0,1-y_if(x_i)]+\lambda\sum_{j=1}^p\beta_j^2 \right\}\]</span></p><ul><li>λ is small: few violations to the margin ; high-variance, low-bias; <span class="math inline">\(\Leftrightarrow\)</span> small <span class="math inline">\(C\)</span>;</li></ul><a id="more"></a><p><strong>“Loss + Penalty” form</strong>: <span class="math display">\[\min_{\beta_0,...,\beta_p}\left\{ L(\mathbf{X},\mathbf{y},\beta)+\lambda P(\beta) \right\}\]</span></p><ul><li><span class="math inline">\(L(\mathbf{X},\mathbf{y},\beta)\)</span> : loss function</li><li><span class="math inline">\(P(\beta)\)</span>: penalty function</li></ul><p><strong>Ridge regression and the lasso</strong>: <span class="math display">\[L(\mathbf{X},\mathbf{y},\beta)=\sum_{i=1}^n \left( y_i-\beta_0-\sum_{j=1}^p x_{ij}\beta_j \right)^2 \\P(\beta) = \sum_{j=1}^p \beta_j^2 \quad ridge \, regression \\P(\beta) = \sum_{j=1}^p |\beta_j| \quad lasso\]</span> <strong>SVM</strong>: <strong><em>hindge loss</em></strong> <span class="math display">\[L(\mathbf{X},\mathbf{y},\beta)=\sum_{i=1}^n \max[0,1-y_i(\beta_0+\beta_1x_{i1}+,,,+\beta_px_{ip})]\]</span> <strong>Optimization problems of linear SVM and (regularized) LR</strong>: <span class="math display">\[\min_\beta \lambda||\beta||^2+\sum_{i=1}^n \max[0,1-y_i(\beta_0+\beta_1x_{i1}+,,,+\beta_px_{ip})] \\\min_\beta \lambda||\beta||^2+\sum_{i=1}^n \log(1+\exp(1-y_i(\beta_0+\beta_1x_{i1}+,,,+\beta_px_{ip})))\]</span> That is, they only differ in the loss function — <strong>SVM minimizes hinge loss while logistic regression minimizes logistic loss.</strong></p><ul><li>Logistic loss diverges faster than hinge loss. So, in general, it will be more sensitive to outliers.</li><li>Logistic loss does not go to zero even if the point is classified sufficiently confidently. This might lead to minor degradation in accuracy.</li></ul><p><strong>Main Difference</strong>:</p><ul><li>SVM try to maximize the margin between the closest support vectors while LR the posterior class probability. Thus, SVM find a solution which is as fare as possible for the two categories while LR has not this property.</li></ul><p><img src="./1.png" width="500"></p><ul><li>LR is more sensitive to outliers than SVM because the cost function of LR diverges faster than those of SVM. So putting an outlier on above picture would give below picture:</li></ul><p><img src="./2.png" width="500"></p><ul><li>Logistic Regression produces probabilistic values while SVM produces 1 or 0. So in a few words LR makes not absolute prediction and it does not assume data is enough to give a final decision. This maybe be good property when what we want is an estimation or we do not have high confidence into data.<ul><li>In order to get discrete values <strong>1 or 0</strong> for the LR we can say that when a function value is greater than a threshold we classify as 1 and when a function value is smaller than the threshold we classify as 0.</li></ul></li></ul><p><strong>When to use which one?</strong></p><p><img src="./4.png" width="500"></p><h3 id="what-is-a-large-margin-classifier">2. What is a large margin classifier?</h3><p><strong><em>Margin</em></strong>: the smallest (perpendicular) distance from each training observation to a given separating hyperplane <span class="math inline">\(\Rightarrow\)</span> the minimal distance from the observations to the hyperplane.</p><p><strong><em>Maximal margin hyperplane</em></strong>: the separating hyperplane that is farthest from the training observations.</p><ul><li>The maximal margin hyperplane is the separating hyperplane for which the <em>margin</em> is <strong>largest</strong></li><li>Overfitting when <span class="math inline">\(p\)</span> is large.</li></ul><p><strong><em>Maximal margin classifier</em></strong>: classify a test observation based on which side of the maximal margin hyperplane it lies.</p><p>The <strong>maximal margin hyperplane</strong> is the solution to the optimization problem <span class="math display">\[\max_{\beta_0,...\beta_p} M \\s.t.  \sum_{j=1}^p \beta_j^2=1,  \quad (9.10) \\ y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2},...+\beta_px_{ip})&gt;M \quad \forall i=1,..,n.  \quad (9.11)\]</span></p><ul><li>The constraint in (9.11) in fact requires that each observation be on the correct side of the hyperplane, with some cushion, provided that <strong>margin</strong> <span class="math inline">\(M\)</span> is positive.)</li><li>The constraint in (9.10) makes sure the perpendicular distance from the i-th observation to the hyperplane is given by</li></ul><p><span class="math display">\[y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2},...+\beta_px_{ip})\]</span></p><h3 id="why-svm-is-an-example-of-a-large-margin-classifier">3. Why SVM is an example of a large margin classifier?</h3><p><strong><em>Support Vector Classifier (Soft Margin Classifier)</em></strong>: Rather than seeking the largest possible margin that every observation is not only on the correct side of the hyperplane but also on the correct side of the margin, we instead allow some observationsto be on the incorrect side of the margin, or even the incorrect side of the hyperplane.</p><p>​ <strong>Optimization problem</strong>: <span class="math display">\[\max_{\beta_0,...\beta_p,\epsilon_1,..,\epsilon_n} M \\s.t.  \sum_{j=1}^p \beta_j^2=1,  \quad (9.13) \\ y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2},...+\beta_px_{ip})&gt;M(1-\epsilon_i) \quad \forall i=1,..,n.  \quad (9.14) \\ \epsilon_i\geq0,\sum_{i=1}^p\epsilon_i \leq C, \quad (9.15)\]</span></p><ul><li><strong><em>Slack variables</em></strong>: <span class="math inline">\(\epsilon_1,..,\epsilon_n\)</span> - allow individual observations to be on the wrong side of the margin or the hyperplane<ul><li><span class="math inline">\(\epsilon_i=0\)</span>: the i-th observation is on the correct side of the <em>margin</em></li><li><span class="math inline">\(\epsilon_i &gt;0\)</span>: the i-th observation is on the wrong side of the <em>margin</em> <span class="math inline">\(\Rightarrow\)</span> i-th observation <strong><em>violated</em></strong> the margin.</li><li><span class="math inline">\(\epsilon_i &gt;1\)</span>: the i-th observation is on the wrong side of the <em>hyperplane</em></li></ul></li><li>Classify the test observation based on the sign of <span class="math inline">\(f(x^∗) = \beta_0+\beta_1x_{1}^*+\beta_2x_{2}^*,...+\beta_px_{p}^*\)</span>.</li></ul><p>The <strong><em>support vector machine (SVM)</em></strong> is an extension of the support vector classifier that results from enlarging the feature space using <strong>kernels</strong>.</p><h3 id="svm-being-a-large-margin-classifier-is-it-influenced-by-outliers">4. SVM being a large margin classifier, is it influenced by outliers?</h3><p>Yes, if C is large, otherwise not</p><h3 id="what-is-the-role-of-c-in-svm">5. What is the role of C in SVM?</h3><p><strong><em>Tuning parameter C</em></strong>: <span class="math inline">\(C\)</span> bounds the sum of the <span class="math inline">\(\epsilon_i\)</span>'s, and so it determines the number and severity of the violationsto the margin(and to the hyperplane) that we will tolerate.</p><ul><li><strong><em>budget</em></strong> for the amount that the margin can be violated by the <span class="math inline">\(n\)</span> observations.</li><li>Generally chosen via <em>cross-validation</em>.</li><li><span class="math inline">\(C\)</span> controls the <strong>bias-variance trade-off</strong> of the support vector classifier.<ul><li>C is small: highly fit to the data, fewer support vectors <span class="math inline">\(\Rightarrow\)</span> low bias , high variance;</li><li>C is large: margin wider, many support vectors <span class="math inline">\(\Rightarrow\)</span> high bias , low variance;</li></ul></li></ul><h3 id="what-is-a-kernel-in-svm-why-do-we-use-kernels-in-svm">6. What is a kernel in SVM? Why do we use kernels in SVM?</h3><p><strong>Kernel</strong>: Kernel is a function that quantifies the similarity of two observations.</p><ul><li><strong><em>Linear kernel</em></strong>: <span class="math inline">\(K(x_i,x_{i^{&#39;}})=\sum_{j=1}^px_{ij}x_{i^{&#39;}j}\)</span><ul><li>Linear kernel essentially quantifies the similarity of a pair of observations using <strong>Pearson</strong> (standard) correlation.</li></ul></li><li><strong><em>Polynomial kernel</em></strong> of degree d: <span class="math inline">\(K(x_i,x_{i^{&#39;}})=(1+\sum_{j=1}^px_{ij}x_{i^{&#39;}j})^d\)</span><ul><li>fitting a support vector classifier in a higher-dimensional space involving polynomials of degree <span class="math inline">\(d\)</span>.</li></ul></li><li><strong><em>Radial kernel</em></strong>: <span class="math inline">\(K(x_i,x_{i^{&#39;}})=\exp(-\gamma \sum_{j=1}^p(x_{ij}-x_{i^{&#39;}j})^2)\)</span><ul><li>Radial kernel has very <em>local</em> behavior: only nearby training observations have an effect on the class label of a test observation<ul><li>If a given test observation <span class="math inline">\(x^∗ = (x^∗_1 . . .x^∗_p)^T\)</span> is far from a training observation <span class="math inline">\(x_i\)</span> in terms of <strong><em>Euclidean distance</em></strong>; <span class="math inline">\(\Rightarrow\)</span> $ _{j=1}<sup>p(x_{ij}-x_{i</sup>{'}j})^2 $ will be large <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(K(x_i,x_{i^{&#39;}})=\exp(-\gamma \sum_{j=1}^p(x_{ij}-x_{i^{&#39;}j})^2)\)</span> will be very tiny. <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(x_i\)</span> will play virtually no role in <span class="math inline">\(f(x^∗)\)</span>.</li></ul></li></ul></li></ul><p><strong><em>Support Vector Machine</em></strong>: When the support vector classifieris combined with a non-linear kernel, the resulting classifier is known as a support vector machine. <span class="math display">\[f(x)=\beta_0+\sum_{i \in S}^n \alpha_i K(x,x_i)\]</span> In machine learning, a <strong>“kernel”</strong> is usually used to refer to the kernel trick, a method of <strong><em>using a linear classifier to solve a non-linear problem</em></strong>. The kernel function is what is applied on each data instance to map the original non-linear observations into a higher-dimensional space in which they become <em>separable</em>.</p><p><strong>Advantage of Kernel over enlarging the feature space using functions of the original features: </strong></p><ul><li><strong><em>Computational</em></strong>: one need only compute <span class="math inline">\(K(x_i,x_{i^{&#39;}})\)</span> for all <span class="math inline">\(\left(\begin{array}{c}n\\ 2\end{array}\right)\)</span> distinct pairs <span class="math inline">\(i, i^{&#39;}\)</span>. This can bedone without explicitly working in the <em>enlarged feature space.</em><ul><li><strong>Curse of dimensionality</strong>: for some kernels, such as the radial kernel, the feature space is implicit and infinite-dimensional.</li></ul></li></ul><h3 id="can-we-apply-the-kernel-trick-to-logistic-regression-why-is-it-not-used-in-practice-then">7. Can we apply the kernel trick to logistic regression? Why is it not used in practice then?</h3><p>While converting the primal SVM formulation into its dual form (which gives us the kernel version of the SVM), we notice that one of the equations we get is, <span class="math display">\[\beta=\sum_{i=1}^n \alpha_iy_ix_i\]</span> This, in the kernelized version where the kernel <span class="math inline">\(k\)</span> has an implicit representation for points given by x <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(\phi(x)\)</span>, <span class="math display">\[\beta=\sum_{i=1}^n \alpha_iy_i\phi(x_i)\]</span> Surprisingly, one can also get this form from the representer theorem [2]. This suggests something general about the classifiers we learn.</p><p>Now, let us get back to logistic regression, which is modeled as, <span class="math display">\[p(y=1|x)=\frac{1}{1+\exp(-\beta^Tx)}\]</span> First of all, let us map the x to the space of implicit representation of the kernel. So, our model in the implicit representation space would look like, <span class="math display">\[p(y=1|x)=\frac{1}{1+\exp(-\beta^T\phi(x))}\]</span> Next, let us use the form of <span class="math inline">\(\beta\)</span>, we observed from SVM and the representer theorem. This will give us, <span class="math display">\[p(y=1|x)=\frac{1}{1+\exp(-\sum_{i=1}^n\alpha_iy_i\phi(x_i)^T\phi(x))} \\=\frac{1}{1+\exp(-\sum_{i=1}^n\alpha_iy_iK(x_i,x))}\]</span> This gives us the kernelized logistic regression model.</p><p><strong>Why is it not used in practice then?</strong>: logistic regression with kernels is merely an <strong>SVM</strong> without maximum margins</p><h3 id="how-does-the-svm-guassian-kernel-parameter-lambda-affect-the-biasvariance-trade-off">8. How does the SVM Guassian kernel parameter <span class="math inline">\(\lambda\)</span> affect the bias/variance trade off?</h3><p><strong>Larger RBF kernel bandwidths (i.e. smaller 𝛾γ) produce smoother decision boundaries because they produce smoother feature space mappings</strong>. Smoother mappings produce simpler decision boundaries:</p><p><img src="./5.png" width="700"> <span class="math display">\[K(x_i,x_j)=\exp(-\lambda ||x_i-x_j||^2)\]</span></p><ul><li><strong>Small <span class="math inline">\(\lambda\)</span>:</strong> a Gaussian with a <em>large variance</em> so the influence of <span class="math inline">\(x_j\)</span> is more, i.e. if <span class="math inline">\(x_j\)</span> is a support vector, the class of this support vector will have influence on deciding the class of the vector <span class="math inline">\(x_i\)</span> even if the distance between them is large <span class="math inline">\(\rightarrow\)</span> <strong>Larger RBF kernel bandwidths</strong> <span class="math inline">\(\rightarrow\)</span> smoother decision boundaries $  $ <strong>high bias , low variance</strong></li><li><strong>Large <span class="math inline">\(\lambda\)</span>:</strong> variance is small <span class="math inline">\(\rightarrow\)</span> support vector does not have wide-spread influence. Technically speaking <span class="math inline">\(\rightarrow\)</span> <strong>low bias , high variance</strong>.</li></ul><p><strong>Ref</strong>:</p><p><a href="https://github.com/Sroy20/machine-learning-interview-questions" target="_blank" rel="noopener">machine-learning-interview-questions</a></p><p><a href="https://towardsdatascience.com/support-vector-machine-vs-logistic-regression-94cc2975433f" target="_blank" rel="noopener">Support Vector Machine vs Logistic Regression</a></p><p><a href="https://towardsdatascience.com/kernel-function-6f1d2be6091" target="_blank" rel="noopener">Kernel Functions</a></p><p><a href="https://www.quora.com/What-are-kernels-in-machine-learning-and-SVM-and-why-do-we-need-them" target="_blank" rel="noopener">What are kernels in machine learning and SVM and why do we need them?</a></p><p><a href="https://www.quora.com/How-can-one-use-kernels-utilizing-the-kernel-trick-in-logistic-regression" target="_blank" rel="noopener">How can one use kernels (utilizing the kernel trick) in logistic regression?</a></p><p><a href="https://www.quora.com/What-are-C-and-gamma-with-regards-to-a-support-vector-machine" target="_blank" rel="noopener">What are C and gamma with regards to a support vector machine?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;support-vector-machine&quot;&gt;Support Vector Machine&lt;/h2&gt;
&lt;h3 id=&quot;svm-v.s.-logistic-regression&quot;&gt;1. SVM v.s. Logistic Regression&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;SVM Optimization problem&lt;/strong&gt;: &lt;span class=&quot;math display&quot;&gt;\[
\max_{\beta_0,...\beta_p,\epsilon_1,..,\epsilon_n} M \\
s.t.  \sum_{j=1}^p \beta_j^2=1,  \quad (9.13) \\
 y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2},...+\beta_px_{ip})&amp;gt;M(1-\epsilon_i) \quad \forall i=1,..,n.  \quad (9.14) \\
 \epsilon_i\geq0,\sum_{i=1}^p\epsilon_i \leq C, \quad (9.15)
\]&lt;/span&gt; Rewrite the criterion (9.12)–(9.15) for fitting the support vector classifier &lt;span class=&quot;math inline&quot;&gt;\(f(X) = β_0 + β_1X_1 + . . . + β_pX_p\)&lt;/span&gt; as &lt;span class=&quot;math display&quot;&gt;\[
\min_{\beta_0,...,\beta_p}\left\{ \sum_{i=1}^n\max[0,1-y_if(x_i)]+\lambda\sum_{j=1}^p\beta_j^2 \right\}
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;λ is small: few violations to the margin ; high-variance, low-bias; &lt;span class=&quot;math inline&quot;&gt;\(\Leftrightarrow\)&lt;/span&gt; small &lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Interview" scheme="https://nancyyanyu.github.io/tags/Interview/"/>
    
  </entry>
  
</feed>
