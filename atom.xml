<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Nancy&#39;s Notes</title>
  
  <subtitle>Code changes world!</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://nancyyanyu.github.io/"/>
  <updated>2019-06-12T02:46:07.009Z</updated>
  <id>https://nancyyanyu.github.io/</id>
  
  <author>
    <name>Nancy Yan</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>ISLR Note - SVM: Support Vector Machines</title>
    <link href="https://nancyyanyu.github.io/undefined/a1139410/"/>
    <id>https://nancyyanyu.github.io/undefined/a1139410/</id>
    <published>2019-06-11T23:03:07.000Z</published>
    <updated>2019-06-12T02:46:07.009Z</updated>
    
    <content type="html"><![CDATA[<h1 id="svms-with-kernel">SVMs with Kernel</h1><p>The <strong><em>support vector machine (SVM)</em></strong> is an extension of the support vector classifier that results from enlarging the feature space using <strong>kernels</strong>.</p><p>The <strong><em>solution to the support vector classifier problem</em></strong> involves only the <strong><em>inner products</em></strong> of the observations: <span class="math display">\[\langle x_i,x_{i^{&#39;}} \rangle =\sum_{j=1}^px_{ij}x_{i^{&#39;}j}\]</span> (Details won't be discussed in this note)</p><p>The <strong>linear support vector classifier</strong> can be represented as <span class="math display">\[f(x)=\beta_0+\sum_{i=1}^n \alpha_i \langle x,x_i \rangle\]</span></p><ul><li><span class="math inline">\(α_i\)</span> is nonzero only for the support vectors in the solution—that is, if a training observation is not a support vector, then its <span class="math inline">\(α_i\)</span>equals zero.</li></ul><a id="more"></a><p>So if <span class="math inline">\(S\)</span> is the collection of indices of these support points: <span class="math display">\[f(x)=\beta_0+\sum_{i \in S}^n \alpha_i \langle x,x_i \rangle\]</span> <strong>Generalization</strong>: <em>Kernel</em> <span class="math display">\[K(x_i,x_{i^{&#39;}})\]</span> <strong>Kernel</strong>: Kernel is a function that quantifies the similarity of two observations.</p><ul><li><strong><em>Linear kernel</em></strong>: <span class="math inline">\(K(x_i,x_{i^{&#39;}})=\sum_{j=1}^px_{ij}x_{i^{&#39;}j}\)</span><ul><li>Linear kernel essentially quantifies the similarity of a pair of observations using <strong>Pearson</strong> (standard) correlation.</li></ul></li><li><strong><em>Polynomial kernel</em></strong> of degree d: <span class="math inline">\(K(x_i,x_{i^{&#39;}})=(1+\sum_{j=1}^px_{ij}x_{i^{&#39;}j})^d\)</span><ul><li>fitting a support vector classifier in a higher-dimensional space involving polynomials of degree <span class="math inline">\(d\)</span>.</li></ul></li><li><strong><em>Radial kernel</em></strong>: <span class="math inline">\(K(x_i,x_{i^{&#39;}})=\exp(-\gamma \sum_{j=1}^p(x_{ij}-x_{i^{&#39;}j})^2)\)</span><ul><li>Radial kernel has very <em>local</em> behavior: only nearby training observations have an effect on the class label of a test observation<ul><li>If a given test observation <span class="math inline">\(x^∗ = (x^∗_1 . . .x^∗_p)^T\)</span> is far from a training observation <span class="math inline">\(x_i\)</span> in terms of <strong><em>Euclidean distance</em></strong>; <span class="math inline">\(\Rightarrow\)</span> $ _{j=1}<sup>p(x_{ij}-x_{i</sup>{'}j})^2 $ will be large <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(K(x_i,x_{i^{&#39;}})=\exp(-\gamma \sum_{j=1}^p(x_{ij}-x_{i^{&#39;}j})^2)\)</span> will be very tiny. <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(x_i\)</span> will play virtually no role in <span class="math inline">\(f(x^∗)\)</span>.</li></ul></li></ul></li></ul><p><strong><em>Support Vector Machine</em></strong>: When the support vector classifieris combined with a non-linear kernel, the resulting classifier isknown as a support vector machine. <span class="math display">\[f(x)=\beta_0+\sum_{i \in S}^n \alpha_i K(x,x_i)\]</span> <img src="./SVM1.png"></p><p><strong>Advantage of Kernel over enlarging the feature space using functions of the original features: </strong></p><ul><li><strong><em>Computational</em></strong>: one need only compute <span class="math inline">\(K(x_i,x_{i^{&#39;}})\)</span> for all <span class="math inline">\(\left(\begin{array}{c}n\\ 2\end{array}\right)\)</span> distinct pairs <span class="math inline">\(i, i^{&#39;}\)</span>. This can bedone without explicitly working in the <em>enlarged feature space.</em><ul><li><strong>Curse of dimensionality</strong>: for some kernels, such as the radial kernel, the feature space is implicit and infinite-dimensional.</li></ul></li></ul><h1 id="svms-with-more-than-two-classes">SVMs with More than Two Classes</h1><h2 id="one-versus-one-classification">One-Versus-One Classification</h2><p>A <strong><em>one-versus-one</em></strong> or <strong><em>all-pairs</em></strong> approach constructs <span class="math inline">\(\left(\begin{array}{c}K\\ 2\end{array}\right)\)</span> SVMs, each of which compares a pair of classes</p><ol type="1"><li>One such SVM might compare the <span class="math inline">\(k\)</span>-th class, coded as +1, to the <span class="math inline">\(k^{&#39;}\)</span>-th class, codedas −1.</li><li>We classify a test observation using each of the <span class="math inline">\(\left(\begin{array}{c}K\\ 2\end{array}\right)\)</span> classifiers.</li><li>We tally the number of times that the test observation is assigned to each of the K classes.</li><li>The final classification is performed by assigning the test observation to the class to which it was most frequently assigned in these <span class="math inline">\(\left(\begin{array}{c}K\\ 2\end{array}\right)\)</span> pairwise classifications.</li></ol><h2 id="one-versus-all-classification">One-Versus-All Classification</h2><p>The <strong><em>one-versus-all</em></strong> approach:</p><ol type="1"><li>We fit <span class="math inline">\(K\)</span> SVMs, each time comparing one of all the K classes to the remaining K − 1 classes.</li><li>Let <span class="math inline">\(β_{0k}, β_{1k}, . . . , β_{pk}\)</span> denote the parameters that result from fitting an SVM comparing the kth class(coded as +1) to the others (coded as −1).</li><li>Let $ x^∗$ denote a test observation. We assign the observation to the class for which <span class="math inline">\(β_{0k}x_1^*+β_{1k}x_2^*+, . . . ,+ β_{pk}x_p^*\)</span> is largest, as this amounts to a high level of confidence that the test observation belongs to the kth class rather than to any of the other classes.</li></ol><h1 id="relationship-to-logistic-regression">Relationship to Logistic Regression</h1><p>Rewrite the criterion (9.12)–(9.15) for fitting the support vector classifier <span class="math inline">\(f(X) = β_0 + β_1X_1 + . . . + β_pX_p\)</span> as</p><p><span class="math display">\[\min_{\beta_0,...,\beta_p}\left\{ \sum_{i=1}^n\max[0,1-y_If(x_i)]+\lambda\sum_{j=1}^p\beta_j^2 \right\}\]</span></p><ul><li>λ is small: few violations to the margin ; high-variance, low-bias; <span class="math inline">\(\Leftrightarrow\)</span> small <span class="math inline">\(C\)</span>;</li></ul><p><strong>“Loss + Penalty” form</strong>: <span class="math display">\[\min_{\beta_0,...,\beta_p}\left\{ L(\mathbf{X},\mathbf{y},\beta)+\lambda P(\beta) \right\}\]</span></p><ul><li><span class="math inline">\(L(\mathbf{X},\mathbf{y},\beta)\)</span> : loss function</li><li><span class="math inline">\(P(\beta)\)</span>: penalty function</li></ul><p><strong>Ridge regression and the lasso</strong>: <span class="math display">\[L(\mathbf{X},\mathbf{y},\beta)=\sum_{i=1}^n \left( y_i-\beta_0-\sum_{j=1}^p x_{ij}\beta_j \right)^2 \\P(\beta) = \sum_{j=1}^p \beta_j^2 \quad ridge \, regression \\P(\beta) = \sum_{j=1}^p |\beta_j| \quad lasso\]</span> <strong>SVM</strong>:<strong><em>hidge loss</em></strong> <span class="math display">\[L(\mathbf{X},\mathbf{y},\beta)=\sum_{i=1}^n \max[0,1-y_i(\beta_0+\beta_1x_{i1}+,,,+\beta_px_{ip})]\]</span> <img src="SVM2.png"></p><ul><li>Due to thesimilarities between their loss functions, logistic regression and the supportvector classifier often give very similar results.</li><li>When the classes are wellseparated, SVMs tend to behave better than logistic regression</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;svms-with-kernel&quot;&gt;SVMs with Kernel&lt;/h1&gt;
&lt;p&gt;The &lt;strong&gt;&lt;em&gt;support vector machine (SVM)&lt;/em&gt;&lt;/strong&gt; is an extension of the support vector classifier that results from enlarging the feature space using &lt;strong&gt;kernels&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;&lt;em&gt;solution to the support vector classifier problem&lt;/em&gt;&lt;/strong&gt; involves only the &lt;strong&gt;&lt;em&gt;inner products&lt;/em&gt;&lt;/strong&gt; of the observations: &lt;span class=&quot;math display&quot;&gt;\[
\langle x_i,x_{i^{&amp;#39;}} \rangle =\sum_{j=1}^px_{ij}x_{i^{&amp;#39;}j}
\]&lt;/span&gt; (Details won&#39;t be discussed in this note)&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;linear support vector classifier&lt;/strong&gt; can be represented as &lt;span class=&quot;math display&quot;&gt;\[
f(x)=\beta_0+\sum_{i=1}^n \alpha_i \langle x,x_i \rangle
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(α_i\)&lt;/span&gt; is nonzero only for the support vectors in the solution—that is, if a training observation is not a support vector, then its &lt;span class=&quot;math inline&quot;&gt;\(α_i\)&lt;/span&gt;equals zero.&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="SVM" scheme="https://nancyyanyu.github.io/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>ISLR Note - SVM: Support Vector Classifiers</title>
    <link href="https://nancyyanyu.github.io/undefined/e6eddd64/"/>
    <id>https://nancyyanyu.github.io/undefined/e6eddd64/</id>
    <published>2019-06-11T22:29:11.000Z</published>
    <updated>2019-06-12T02:45:59.076Z</updated>
    
    <content type="html"><![CDATA[<h1 id="overview-of-the-support-vector-classifier">Overview of the Support Vector Classifier</h1><p><strong>Maximal margin hyperplane</strong> is extremely sensitive to a change in a single observation suggests that it may have <strong><em>overfit</em></strong> the training data.</p><p>In this case, we might be willing to consider a classifier based on a hyperplane that does not perfectly separate the two classes, in the interest of</p><ul><li>Greater <em>robustness</em> to individual observations, and</li><li>Better classification of most of the training observations.</li></ul><p><strong><em>Support Vector Classifier (Soft Margin Classifier)</em></strong>: Rather than seeking the largest possible margin that every observation is not only on the correct side of the hyperplane but also on the correct side of the margin, we instead allow some observationsto be on the incorrect side of the margin, or even the incorrect side ofthe hyperplane.</p><a id="more"></a><p><img src="SVC1.png"></p><h1 id="details-of-the-support-vector-classifier">Details of the Support Vector Classifier</h1><p><strong>Optimization problem</strong>: <span class="math display">\[\max_{\beta_0,...\beta_p,\epsilon_1,..,\epsilon_n} M \\s.t.  \sum_{j=1}^p \beta_j^2=1,  \quad (9.13) \\ y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2},...+\beta_px_{ip})&gt;M(1-\epsilon_i) \quad \forall i=1,..,n.  \quad (9.14) \\ \epsilon_i\geq0,\sum_{i=1}^p\epsilon_i \leq C, \quad (9.15)\]</span></p><ul><li><strong><em>Slack variables</em></strong>: <span class="math inline">\(\epsilon_1,..,\epsilon_n\)</span> - allow individual observations to be on the wrong side of the margin or the hyperplane<ul><li><span class="math inline">\(\epsilon_i=0\)</span>: the i-th observation is on the correct side of the <em>margin</em></li><li><span class="math inline">\(\epsilon_i &gt;0\)</span>: the i-th observation is on the wrong side of the <em>margin</em> <span class="math inline">\(\Rightarrow\)</span> i-th observation <strong><em>violated</em></strong> the margin.</li><li><span class="math inline">\(\epsilon_i &gt;1\)</span>: the i-th observation is on the wrong side of the <em>hyperplane</em></li></ul></li><li>Classify the test observation based on the sign of <span class="math inline">\(f(x^∗) = \beta_0+\beta_1x_{1}^*+\beta_2x_{2}^*,...+\beta_px_{p}^*\)</span>.</li><li><strong><em>Tuning parameter C</em></strong>: <span class="math inline">\(C\)</span> bounds the sum of the <span class="math inline">\(\epsilon_i\)</span>'s, and so it determines the number and severity of the violationsto the margin(and to the hyperplane) that we will tolerate. $<ul><li><strong><em>budget</em></strong> for the amount that the margin can be violated by the <span class="math inline">\(n\)</span> observations.</li><li>Generally chosen via <em>cross-validation</em>.</li><li><span class="math inline">\(C\)</span> controls the <strong>bias-variance trade-off</strong> of the support vector classifier.<ul><li>C is small: a classifier highly fit to the data, fewersupport vectors <span class="math inline">\(\Rightarrow\)</span> low bias , high variance;</li><li>C is large: margin wider, many support vectors <span class="math inline">\(\Rightarrow\)</span> high bias , low variance;</li></ul></li></ul></li></ul><p><img src="SVC2.png"></p><p><strong>Properties</strong>:</p><ul><li>Only observations that either <em>lie on the margin or that violate the margin</em> (<strong>support vectors</strong>) will affect the hyperplane, and hence the classifier obtained.</li><li>The fact that the support vector classifier’s decision rule is based only on a potentially small subset of the training observations (the <strong><em>support vectors</em></strong>) means that it is quite <strong>robust</strong> to the behavior of observations that are far away from the hyperplane.<ul><li>Different from LDA which depends on the mean of <em>all</em> of the observations within each class, as well as the <em>within-class covariance matrix</em> computed using all of the observations</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;overview-of-the-support-vector-classifier&quot;&gt;Overview of the Support Vector Classifier&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Maximal margin hyperplane&lt;/strong&gt; is extremely sensitive to a change in a single observation suggests that it may have &lt;strong&gt;&lt;em&gt;overfit&lt;/em&gt;&lt;/strong&gt; the training data.&lt;/p&gt;
&lt;p&gt;In this case, we might be willing to consider a classifier based on a hyperplane that does not perfectly separate the two classes, in the interest of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Greater &lt;em&gt;robustness&lt;/em&gt; to individual observations, and&lt;/li&gt;
&lt;li&gt;Better classification of most of the training observations.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Support Vector Classifier (Soft Margin Classifier)&lt;/em&gt;&lt;/strong&gt;: Rather than seeking the largest possible margin that every observation is not only on the correct side of the hyperplane but also on the correct side of the margin, we instead allow some observationsto be on the incorrect side of the margin, or even the incorrect side ofthe hyperplane.&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="SVM" scheme="https://nancyyanyu.github.io/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>ISLR Note - SVM: Maximal Margin Classifier</title>
    <link href="https://nancyyanyu.github.io/undefined/bc53b72b/"/>
    <id>https://nancyyanyu.github.io/undefined/bc53b72b/</id>
    <published>2019-06-11T21:48:50.000Z</published>
    <updated>2019-06-12T03:01:10.402Z</updated>
    
    <content type="html"><![CDATA[<h1 id="what-is-a-hyperplane">What Is a Hyperplane?</h1><p><strong><em>Hyperplane </em></strong>: In a p-dimensional space, a hyperplane is a flat affine subspace of dimension <span class="math inline">\(p − 1\)</span>.</p><ul><li>e.g. in two dimensions, a hyperplane is a flat one-dimensional subspace—in other words, a line.</li></ul><p><strong>Mathematical definition of a hyperplane</strong>: <span class="math display">\[\beta_0+\beta_1X_1+\beta_2X_2,...+\beta_pX_p=0, \quad (9.1)\]</span></p><ul><li>Any <span class="math inline">\(X = (X1,X2,…X_p)^T\)</span> for which (9.1) holds is a point on the hyperplane.</li></ul><a id="more"></a><h1 id="classification-using-a-separating-hyperplane">Classification Using a Separating Hyperplane</h1><p><strong>Setting</strong>:</p><ul><li><span class="math inline">\(n \times p\)</span> data matrix <span class="math inline">\(X\)</span> that consists of <span class="math inline">\(n\)</span> training observations in p-dimensional space</li><li>These observations fall into two classes—that is, $y_1, . . . , y_n {−1, 1} $.</li><li>Test observation: a p-vector of observed features <span class="math inline">\(x^∗ =\{x^∗_1 . . . x^∗_p\}^T\)</span>.</li></ul><p><strong>Separating hyperplane</strong> has the property that: <span class="math display">\[y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2},...+\beta_px_{ip})&gt;0, \quad (9.8)\]</span></p><ul><li>for all i=1,…,n</li></ul><p>We classify the test observation <span class="math inline">\(x^*\)</span> based on the sign of <span class="math inline">\(f(x^∗) = \beta_0+\beta_1x_{1}^*+\beta_2x_{2}^*,...+\beta_px_{p}^*\)</span>.</p><ul><li>If <span class="math inline">\(f(x^∗)\)</span> is positive, then we assign the test observation to class 1, and if f(x∗) is negative, then we assign it to class −1.</li><li><strong>Magnitude</strong> of <span class="math inline">\(f(x^∗)\)</span>. If <span class="math inline">\(f(x^∗)\)</span>is far from zero, then this means that <span class="math inline">\(x^∗\)</span> lies far from the hyperplane,and so we can be confident about our class assignment for <span class="math inline">\(x^∗\)</span>.</li></ul><p><img src="./S1.png" width="600" aign="middle"></p><h1 id="the-maximal-margin-classifier">The Maximal Margin Classifier</h1><p><strong><em>Margin</em></strong>: the smallest (perpendicular) distance from each training observation to a given separating hyperplane <span class="math inline">\(\Rightarrow\)</span> the minimal distance from the observations to the hyperplane.</p><p><strong><em>Maximal margin hyperplane</em></strong>: the separating hyperplane that is farthest from the training observations.</p><ul><li>The maximal margin hyperplane is the separating hyperplane for which the <em>margin</em> is <strong>largest</strong></li><li>Overfitting when <span class="math inline">\(p\)</span> is large.</li></ul><p><strong><em>Maximal margin classifier</em></strong>: classify a test observation based on which side of the maximal margin hyperplane it lies.</p><p><strong><em>Support vectors</em></strong>: training observations that are equidistant from the maximal margin hyperplane that indicate <em>the width of the margin</em>.</p><ul><li>They “support” the maximal margin hyperplane in the sense vector that if these points were moved slightly then the maximal margin hyperplanewould move as well.</li><li><em>The maximal margin hyperplane depends directly on the support vectors, but not on the other observations</em></li></ul><p><img src="./S2.png" width="600"></p><h1 id="construction-of-the-maximal-margin-classifier">Construction of the Maximal Margin Classifier</h1><p>The maximal margin hyperplane is the solution to the optimization problem <span class="math display">\[\max_{\beta_0,...\beta_p} M \\s.t.  \sum_{j=1}^p \beta_j^2=1,  \quad (9.10) \\ y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2},...+\beta_px_{ip})&gt;M \quad \forall i=1,..,n.  \quad (9.11)\]</span></p><ul><li>The constraint in (9.11) in fact requires that each observation be on the correct side of the hyperplane, with some cushion, provided that <strong>margin</strong> <span class="math inline">\(M\)</span> is positive.)</li><li>The constraint in (9.10) makes sure the perpendicular distance from the i-th observation to the hyperplane is given by</li></ul><p><span class="math display">\[y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2},...+\beta_px_{ip})\]</span></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;what-is-a-hyperplane&quot;&gt;What Is a Hyperplane?&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Hyperplane &lt;/em&gt;&lt;/strong&gt;: In a p-dimensional space, a hyperplane is a flat affine subspace of dimension &lt;span class=&quot;math inline&quot;&gt;\(p − 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;e.g. in two dimensions, a hyperplane is a flat one-dimensional subspace—in other words, a line.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Mathematical definition of a hyperplane&lt;/strong&gt;: &lt;span class=&quot;math display&quot;&gt;\[
\beta_0+\beta_1X_1+\beta_2X_2,...+\beta_pX_p=0, \quad (9.1)
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Any &lt;span class=&quot;math inline&quot;&gt;\(X = (X1,X2,…X_p)^T\)&lt;/span&gt; for which (9.1) holds is a point on the hyperplane.&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="SVM" scheme="https://nancyyanyu.github.io/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>离开纽约前的一些想法</title>
    <link href="https://nancyyanyu.github.io/undefined/babf682e/"/>
    <id>https://nancyyanyu.github.io/undefined/babf682e/</id>
    <published>2019-06-11T18:17:57.000Z</published>
    <updated>2019-06-11T17:19:31.555Z</updated>
    
    <content type="html"><![CDATA[]]></content>
    
    <summary type="html">
    
      
      
        

      
    
    </summary>
    
      <category term="Journal" scheme="https://nancyyanyu.github.io/categories/Journal/"/>
    
    
  </entry>
  
  <entry>
    <title>ISLR Note - Clustering</title>
    <link href="https://nancyyanyu.github.io/undefined/9c99c8b6/"/>
    <id>https://nancyyanyu.github.io/undefined/9c99c8b6/</id>
    <published>2019-06-11T17:52:06.000Z</published>
    <updated>2019-06-12T03:02:07.939Z</updated>
    
    <content type="html"><![CDATA[<h1 id="clustering-methods">Clustering Methods</h1><p><strong>Clustering</strong> refers to a very broad set of techniques for finding subgroups, or clusters, in a data set.</p><ul><li><p>When we cluster the observations of a data set, we seek to partition them into distinct groups so that the observations within each group are quite similar to each other</p></li><li><p>This is an unsupervised problem because we are trying to discover structure</p></li></ul><a id="more"></a><h2 id="clustering-v.s.-pca">Clustering v.s. PCA</h2><p>Both clustering and PCA seek to simplify the data via a small number of summaries, but their mechanisms are different:</p><ul><li><p>PCA looks to find a low-dimensional representation of the observations that explain a good fraction of the variance;</p></li><li><p>Clustering looks to find homogeneous subgroups among the observations.</p></li></ul><p><strong>Application:market segmentation</strong></p><h1 id="k-means-clustering">K-Means Clustering</h1><p><strong>K-means clustering</strong> is a simple and elegant approach for partitioning adata set into K distinct, <strong><em>non-overlapping</em></strong> clusters.</p><p>The idea behind <strong>K-means clustering</strong> is that a <em>good</em> clustering is one for which the <strong><em>within-cluster</em></strong> <strong><em>variation</em></strong> is as small as possible.</p><p>The <strong>within-cluster variation</strong> for cluster <span class="math inline">\(C_k\)</span> is a measure <span class="math inline">\(W(C_k)\)</span> of the amount by which the observationswithin a cluster differ from each other.</p><p>Hence we want to solve the problem, <span class="math display">\[\min_{C_1,...,C_K}\left\{ \sum_{i=1}^KW(C_k) \right\}\]</span></p><ul><li>partition the observations into K clusters such that the total within-cluster variation, summed over all K clusters, is <em>as small as possible</em>.</li></ul><p><strong>Define the within-cluster variation</strong>: <strong><em>Euclidean distance</em></strong>: <span class="math display">\[W(C_k)=\frac{1}{|C_k|}\sum_{i,i^{&#39;}\in C_k}\sum_{j=1}^p(x_{ij}-x_{i^{&#39;}j})^2\]</span></p><ul><li>where <span class="math inline">\(|C_k|\)</span> denotes the number of observations in the kth cluster.</li><li>The within-cluster variation for the kth cluster is <em>the sum of all ofthe pairwise squared Euclidean distances between the observations in the kth cluste</em>r, divided by the total number of observations in the kth cluster.</li></ul><h2 id="k-means-clustering-optimization-problem">K-means Clustering Optimization Problem</h2><p><strong>Objective funtion</strong>: <span class="math display">\[\min_{C_1,...,C_K}\left\{ \sum_{i=1}^K\frac{1}{|C_k|}\sum_{i,i^{&#39;}\in C_k}\sum_{j=1}^p(x_{ij}-x_{i^{&#39;}j})^2\right\}\]</span> <img src="./C1.png"></p><p>Algorithm 10.1 is guaranteed to decrease the value of the objective at each step: <span class="math display">\[\frac{1}{|C_k|}\sum_{i,i^{&#39;}\in C_k}\sum_{j=1}^p(x_{ij}-x_{i^{&#39;}j})^2=2\sum_{i\in C_k}\sum_{j=1}^p(x_{ij}-\bar{x}_{kj})^2, \quad (10.12)\]</span></p><ul><li>where <span class="math inline">\(\bar{x}_{kj}=\frac{1}{C_k}\sum_{i \in C_k}x_{ij}\)</span> is the mean for feature <span class="math inline">\(j\)</span> in cluster <span class="math inline">\(C_k\)</span>.</li></ul><p><strong>Step 2(a)</strong> : the <em>cluster means</em> for each feature are the constants that minimize the sum-of-squared deviations</p><p><strong>Step 2(b)</strong> : reallocating the observations can only improve (10.12).</p><p><strong>Local optimum</strong> : This means that as the algorithmis run, the clustering obtained will continually improve until the result no longer changes; the objective will never increase.</p><ul><li>It is important to run the algorithm multiple times from different random initial configurations, because the results obtained will depend on the initial (random) cluster assignmentof each observation in Step 1 of Algorithm 10.1</li></ul><p><img src="./C2.png"></p><h1 id="hierarchical-clustering">Hierarchical Clustering</h1><p><strong>Hierarchical clustering</strong> is an alternative approach which does not require that we commit to a particular choice of <span class="math inline">\(K\)</span>.</p><ul><li>Added advantage over K-means clustering in that it results in an attractive tree-based representation of the observations, called a <strong><em>dendrogram</em></strong>.</li></ul><p><strong>Bottom-up</strong> or <strong>agglomerative</strong> clustering: the most common type of hierarchical clustering.</p><h2 id="interpreting-a-dendrogram">Interpreting a Dendrogram</h2><p><img src="./C4.png"></p><p><img src="./C3.png"></p><p>In the left-hand panel of Figure 10.9, each <em>leaf</em> of the dendrogram representsone of the 45 observations in Figure 10.8.</p><ul><li><p>As we move up the tree, some leaves begin to <strong>fuse</strong> into branches: observations that are similar to each other.</p></li><li>The earlier(lower in the tree) fusions occur, the more similar the groups of observationsare to each other.</li><li>For any two observations, we can look for the point in the tree where branches containing those two observations are first fused.<ul><li>The <em>height</em> of this fusion, as measured on the vertical axis, indicates how different the two observations are.</li></ul></li></ul><p>Thus, observations that fuse at the very bottom of the tree are quite similar to each other, whereas observationsthat fuse close to the top of the tree will tend to be quite different.</p><p><strong>Identifying clusters on the basisof a dendrogram</strong>:</p><p>Make a <em>horizontal cut</em> across the dendrogram, as shown in the center and right-hand panels of Figure 10.9. The distinct sets of observations beneath the cut can be interpreted as clusters.</p><blockquote><p><strong><em>The height of the cut to the dendrogram serves the same role as the K in K-means clustering: it controls the number ofclusters obtained.</em></strong></p></blockquote><p><strong><em>Hierarchical</em></strong> refers to the fact that clusters obtained by cutting the dendrogram at a given height are necessarily <em>nested</em> within the clusters obtained by cutting the <em>dendrogram</em> at any greater height.</p><p><strong>Disadvantage</strong>: hierarchical clustering can sometimes yield worse results than <em>K-means clustering</em> when the assumption of hierarchical structure unrealistic.</p><h2 id="the-hierarchical-clustering-algorithm">The Hierarchical Clustering Algorithm</h2><p><img src="C5.png"></p><p><strong>Explanation:</strong></p><p>The two clusters that are most similar to each other are then fused so that there now are n−1 clusters. Next the two clusters that are most similar to each other arefused again, so that there now are n − 2 clusters. The algorithm proceeds in this fashion until all of the observations belong to one single cluster, and the dendrogram is complete.</p><p><img src="C6.png"></p><blockquote><p>How did we determine that the cluster {5, 7} should be fused with the cluster {8}? - linkage</p></blockquote><p><strong>Linkage</strong> : defines the <em>dissimilarity</em> between two groups of observations.</p><ul><li>Complete, average, single, and centroid</li><li>The dissimilarities computed in Step 2(b)of the hierarchical clustering algorithm will depend on the type of linkage used, as well as on the choice of dissimilarity measure.</li></ul><p><img src="C7.png"></p><h2 id="choice-of-dissimilarity-measure">Choice of Dissimilarity Measure</h2><p><strong>Dissimilarity Measure</strong>:</p><ul><li><strong><em>Euclidean distance</em></strong></li><li><strong><em>Correlation-based distance</em></strong>: considers two observationsto be similar if their features are highly correlated</li></ul><p>In general, careful attention should be paid to the <em>type</em> of data being clustered and the scientific question at hand which determines what type of dissimilarity measureis used for hierarchical clustering.</p><p><strong>Note:</strong> variables should be <strong><em>scaled</em></strong> to have standarddeviation one before the dissimilarity between the observations iscomputed.</p><h1 id="practical-issues-in-clustering">Practical Issues in Clustering</h1><h2 id="small-decisions-with-big-consequences">Small Decisions with Big Consequences</h2><p>In order to perform clustering, some decisions must be made.</p><ul><li>Should the observations or features first be standardized to have mean zero and scaled to have standard deviation one.</li><li>In the case of hierarchical clustering,<ul><li>What dissimilarity measure should be used?</li><li>What type of linkage should be used?</li><li>Where should we cut the dendrogramin order to obtain clusters?</li></ul></li><li>In the case of K-means clustering, how many clusters should we lookfor in the data?\</li></ul><p>We try several different choices, and look for the one withthe most useful or interpretable solution.</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;clustering-methods&quot;&gt;Clustering Methods&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Clustering&lt;/strong&gt; refers to a very broad set of techniques for finding subgroups, or clusters, in a data set.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;When we cluster the observations of a data set, we seek to partition them into distinct groups so that the observations within each group are quite similar to each other&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This is an unsupervised problem because we are trying to discover structure&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Clustering" scheme="https://nancyyanyu.github.io/tags/Clustering/"/>
    
      <category term="Unsupervised" scheme="https://nancyyanyu.github.io/tags/Unsupervised/"/>
    
  </entry>
  
  <entry>
    <title>ISLR Note - PCA</title>
    <link href="https://nancyyanyu.github.io/undefined/9b1a6524/"/>
    <id>https://nancyyanyu.github.io/undefined/9b1a6524/</id>
    <published>2019-06-09T05:40:07.674Z</published>
    <updated>2019-06-12T03:01:44.372Z</updated>
    
    <content type="html"><![CDATA[<h1 id="principal-components-analysis">Principal Components Analysis</h1><p><strong>Principal component analysis (PCA)</strong> refers to the process by which principal components are computed, and the subsequent use of these components in understanding the data. - PCA also serves as a tool for data visualization (visualization of the observations or visualization of the variables).</p><h2 id="what-are-principal-components">What Are Principal Components?</h2><p><strong>PCA</strong> :finds a low-dimensional representation of a data set that contains as much as possible of the <strong>variation</strong></p><p>Each of the dimensions found by PCA is a linear combination of the <span class="math inline">\(p\)</span> features.</p><p><strong><em>The first principal component</em></strong> of a set of features <span class="math inline">\(X_1,X_2, . . . , X_p\)</span> is the normalized linear combination of the features</p><p><span class="math display">\[\begin{align}Z_1=\phi_{11}X_1+\phi_{21}X_2+,,,+\phi_{p1}X_p\end{align}\]</span> that has the <strong>largest variance</strong>.</p><a id="more"></a><p><strong>Normalized</strong>: <span class="math inline">\(\sum_{j=1}^p \phi_{j1}^2=1\)</span></p><p><strong>Loadings</strong>: <span class="math inline">\(\phi_{11}, . . . , \phi_{p1}\)</span> the loadings of the first principal component; - Together, the loadings make up the principal component loading vector, <span class="math inline">\(\phi_1=(\phi_{11},\phi_{21},...,\phi_{p1})^T\)</span></p><h3 id="compute-the-first-principal-component">Compute the first principal component</h3><ul><li><p>Assume that each of the variables in <span class="math inline">\(X\)</span> has been centered to have mean zero. We then look for the linear combination of the sample feature values of the form <span class="math display">\[\begin{align}z_{i1}=\phi_{11}x_{i1}+\phi_{21}x_{i2}+,,,+\phi_{p1}x_{ip} \quad \quad i=1,2,...,n\end{align}\]</span> that has largest sample variance, subject to the constraint that <span class="math inline">\(\sum_{j=1}^p \phi_{j1}^2=1\)</span></p></li><li>The first principal component loading vector solves the optimization problem <span class="math display">\[\begin{align}\max_{\phi_{11},...,\phi_{p1}}{\left\{ \frac{1}{n} \sum_{i=1}^n \left( \sum_{j=1}^p \phi_{j1}x_{ij}   \right) \right\}} \, subject \, to \, \sum_{j=1}^p \phi_{j1}^2=1\end{align}\]</span></li><li>Since <span class="math inline">\(\sum_{i=1}^nx_{ij}/n=1\)</span>, the average of the <span class="math inline">\(z_{11}, . . . , z_{n1}\)</span> will be zero as well. Hence the objective that we are maximizing is just the <strong>sample variance</strong> of the <span class="math inline">\(n\)</span> values of zi1</li><li><p><strong>Scores</strong>: We refer to <span class="math inline">\(z_{11}, . . . , z_{n1}\)</span> as the scores of the first principal component.</p></li></ul><p><strong>Geometric interpretation</strong>: for the first principal component: The loading vector <span class="math inline">\(\phi_1\)</span> with elements <span class="math inline">\(\phi_{11},\phi_{21},...,\phi_{p1}\)</span> defines a direction in feature space along which the data <strong>vary the most</strong>. If we project the n data points <span class="math inline">\(x_1, . . . , x_n\)</span> onto this direction, the projected values are the principal component scores <span class="math inline">\(z_{11}, . . . , z_{n1}\)</span> themselves.</p><h3 id="compute-the-second-principal-component">Compute the second principal component</h3><p><strong>The second principal component <span class="math inline">\(Z_2\)</span></strong>: the linear combination of <span class="math inline">\(X_1,X_2, . . . , X_p\)</span> that has maximal variance out of all linear combinations that are <strong>uncorrelated with <span class="math inline">\(Z_1\)</span></strong>.</p><p>The second principal component scores <span class="math inline">\(z_{12}, . . . , z_{n2}\)</span> take the form</p><p><span class="math display">\[\begin{align}z_{i2}=\phi_{12}x_{i1}+\phi_{22}x_{i2}+,,,+\phi_{p2}x_{ip} \quad \quad i=1,2,...,n\end{align}\]</span> where <span class="math inline">\(\phi_2\)</span> is the second principal component <strong>loading</strong> vector, with elements <span class="math inline">\(\phi_{12},\phi_{22},...,\phi_{p2}\)</span>.</p><p>It turns out that constraining <span class="math inline">\(Z_2\)</span> to be uncorrelated with <span class="math inline">\(Z_1\)</span> is equivalent to constraining the direction <span class="math inline">\(\phi_2\)</span> to be <strong>orthogonal</strong> (perpendicular) to the direction <span class="math inline">\(\phi_1\)</span>.</p><p>To find <span class="math inline">\(\phi_2\)</span>, we solve a problem similar to (10.3) with <span class="math inline">\(\phi_2\)</span> replacing <span class="math inline">\(\phi_1\)</span>, and with the additional constraint that <span class="math inline">\(\phi_2\)</span> is orthogonal to <span class="math inline">\(\phi_1\)</span></p><p><img src="./1.png" width="600"></p><p><strong>Interpretation:</strong> - 1st loading vector places approximately equal weight on Assault, Murder, and Rape, with much less weight UrbanPop. Hence this component roughly corresponds to a measure of overall rates of serious crimes. - Overall, we see that the crime-related variables (Murder, Assault, and Rape) are located close to each other, and that the UrbanPop variable is far from the other three. - This indicates that the crime-related variables are correlated with each other—states with high murder rates tend to have high assault and rape rates—and that the UrbanPop variable is less correlated with the other three.</p><h2 id="another-interpretation-of-principal-components">Another Interpretation of Principal Components</h2><p><strong>An alternative interpretation for principal components</strong>: principal components provide low-dimensional linear surfaces that are closest to the observations</p><ul><li><strong>The first principal component loading vector has a very special property</strong>: it is the line in p-dimensional space that is closest to the n observations (using average squared Euclidean distance as a measure of closeness).</li><li><p>The appeal of this interpretation : we seek a single dimension of the data that lies as close as possible to all of the data points, since such a line will likely provide a good summary of the data.</p></li><li><p><strong>The first two principal components</strong> of a data set <strong>span the plane</strong> that is closest to the n observations, in terms of average squared Euclidean distance</p></li><li>Together <strong>the first M principal component</strong> score vectors and the first M principal component loading vectors provide the best M-dimensional approximation (in terms of Euclidean distance) to the ith observation <span class="math inline">\(x_{ij}\)</span> . <span class="math display">\[\begin{align}x_{ij} \approx \sum_{m=1}^Mz_{im}\phi_{jm}\end{align}\]</span> (assuming the original data matrix X is column-centered).</li><li><p>When <span class="math inline">\(M = min(n − 1, p)\)</span>, then the representation is exact: <span class="math inline">\(x_{ij} = \sum_{m=1}^Mz_{im}\phi_{jm}\)</span></p></li></ul><h2 id="more-on-pca">More on PCA</h2><h3 id="scaling-the-variables">Scaling the Variables</h3><p>Before PCA is performed, the variables should be <strong>centered to have mean zero</strong>. Furthermore, the results obtained when we perform PCA will also depend on whether the variables have been <strong>individually scaled</strong> (each multiplied by a different constant)</p><p><img src="./2.png" width="600"></p><h3 id="uniqueness-of-the-principal-components">Uniqueness of the Principal Components</h3><p><strong>Each principal component loading vector <span class="math inline">\(\phi_1=(\phi_{11},\phi_{21},...,\phi_{p1})^T\)</span> and the score vectors <span class="math inline">\(z_{11}, . . . , z_{n1}\)</span> is unique, up to a sign flip. </strong> - Two different software packages will yield the same principal component loading vectors and score vectors, although the signs of those loading vectors may differ. - <strong>The signs may differ</strong> because each principal component loading vector specifies a direction in p-dimensional space: flipping the sign has no effect as the direction does not change.</p><h3 id="the-proportion-of-variance-explained">The Proportion of Variance Explained</h3><p><strong>How much of the variance in the data is not contained in the first few principal components?</strong></p><p><strong>Proportion of variance explained (PVE)</strong> by each principal component: - The total variance present in a data set (assuming that the variables have been centered to have mean zero) is defined as</p><p><span class="math display">\[\begin{align}\sum_{j=1}^pVar(X_j)=\sum_{j=1}^p\frac{1}{n}\sum_{i=1}^nx_{ij}^2\end{align}\]</span></p><ul><li>The variance explained by the mth principal component is</li></ul><p><span class="math display">\[\begin{align}\frac{1}{n}\sum_{i=1}^nz_{im}^2=\frac{1}{n}\sum_{i=1}^n \left( \sum_{j=1}^p \phi_{jm}x_{ij} \right)^2\end{align}\]</span></p><ul><li>Therefore, the <strong>PVE of the mth principal component</strong> is given by</li></ul><p><span class="math display">\[\begin{align}\frac{\sum_{i=1}^n \left( \sum_{j=1}^p \phi_{jm}x_{ij} \right)^2}{\sum_{j=1}^p\sum_{i=1}^nx_{ij}^2}\end{align}\]</span></p><p>The PVE of each principal component is a positive quantity. In order to compute the <strong>cumulative PVE</strong> of the first <span class="math inline">\(M\)</span> principal components, we can simply sum (10.8) over each of the first <span class="math inline">\(M\)</span> PVEs. In total, there are <span class="math inline">\(min(n − 1, p)\)</span> principal components, and their PVEs sum to one.</p><p><img src="./3.png" width="600"></p><h3 id="deciding-how-many-principal-components-to-use">Deciding How Many Principal Components to Use</h3><p>We would like to use the smallest number of principal components required to get a good understanding of the data.</p><p><strong>How many principal components are needed?</strong> - We typically decide on the number of principal components required to visualize the data by examining a <strong>scree plot</strong> (Right FIGURE 10.4) - We choose the smallest number of principal components that are required in order to explain a sizable amount of the variation in the data. - We tend to look at the first few principal components in order to find interesting patterns in the data. If no interesting patterns are found in the first few principal components, then further principal components are unlikely to be of interest.</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;principal-components-analysis&quot;&gt;Principal Components Analysis&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Principal component analysis (PCA)&lt;/strong&gt; refers to the process by which principal components are computed, and the subsequent use of these components in understanding the data. - PCA also serves as a tool for data visualization (visualization of the observations or visualization of the variables).&lt;/p&gt;
&lt;h2 id=&quot;what-are-principal-components&quot;&gt;What Are Principal Components?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;PCA&lt;/strong&gt; :finds a low-dimensional representation of a data set that contains as much as possible of the &lt;strong&gt;variation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Each of the dimensions found by PCA is a linear combination of the &lt;span class=&quot;math inline&quot;&gt;\(p\)&lt;/span&gt; features.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;The first principal component&lt;/em&gt;&lt;/strong&gt; of a set of features &lt;span class=&quot;math inline&quot;&gt;\(X_1,X_2, . . . , X_p\)&lt;/span&gt; is the normalized linear combination of the features&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[\begin{align}
Z_1=\phi_{11}X_1+\phi_{21}X_2+,,,+\phi_{p1}X_p
\end{align}\]&lt;/span&gt; that has the &lt;strong&gt;largest variance&lt;/strong&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Unsupervised" scheme="https://nancyyanyu.github.io/tags/Unsupervised/"/>
    
      <category term="PCA" scheme="https://nancyyanyu.github.io/tags/PCA/"/>
    
      <category term="Dimension Reduction" scheme="https://nancyyanyu.github.io/tags/Dimension-Reduction/"/>
    
  </entry>
  
  <entry>
    <title>ISLR Note - Bagging,Random_Forest,Boosting</title>
    <link href="https://nancyyanyu.github.io/undefined/e9c631e9/"/>
    <id>https://nancyyanyu.github.io/undefined/e9c631e9/</id>
    <published>2019-06-09T05:36:15.314Z</published>
    <updated>2019-06-12T03:02:10.206Z</updated>
    
    <content type="html"><![CDATA[<h1 id="bagging">Bagging</h1><p><strong>Bootstrap aggregation</strong>, or <strong>bagging</strong>, is a general-purpose procedure for reducing the variance of a statistical learning method, frequently used in the context of decision trees.</p><p><strong>Averaging a set of observations reduces variance</strong>: Recall that given a set of n independent observations Z1, . . . , Zn, each with variance <span class="math inline">\(σ^2\)</span>, the variance of the mean <span class="math inline">\(\bar{Z}\)</span> of the observations is given by <span class="math inline">\(σ^2/n\)</span>. - A natural way to reduce the variance and hence increase the prediction accuracy of a statistical learning method is to <strong>take many training sets from the population</strong>, build a separate prediction model using each training set, and average the resulting predictions.</p><a id="more"></a><p><strong>Bootstrap</strong> taking repeated samples from the (single) training data set</p><p><strong>Bagging</strong> - Generate B different bootstrapped training data sets. - Train our method on the bth bootstrapped training set in order to get <span class="math inline">\(\hat{f}^{*b}(x)\)</span> - Finally average all the predictions, to obtain</p><p><span class="math display">\[\begin{align}\hat{f}_{bag}(x)=\frac{1}{B}\sum_{b=1}^B\hat{f}^{*b}(x)\end{align}\]</span></p><p><strong>Apply bagging to regression trees</strong> - Construct B regression trees using B bootstrapped training sets - Average the resulting predictions. These trees are grown deep, and are not pruned. Hence each individual tree has high variance, but low bias. Averaging these B trees reduces the variance.</p><p><strong>Bagging on Classification Tree</strong> - For a given test observation, we can record the class predicted by each of the B trees, and take a <strong>majority vote</strong>: the overall prediction is the most commonly occurring class among the B predictions.</p><p><strong>B</strong> - In practice we use a value of B sufficiently large that the error has settled down, like B=100.</p><h2 id="out-of-bag-error-estimation">Out-of-Bag Error Estimation</h2><p>Recall that the key to bagging is that trees are repeatedly fit to bootstrapped subsets of the observations. One can show that on average, each bagged tree makes use of around 2/3 of the observations. The remaining one-third of the observations not used to fit a given bagged tree are referred to as the <strong>out-of-bag (OOB)</strong> observations.</p><blockquote><p>We can predict the response for the ith observation using each of the trees inwhich that observation was OOB.</p></blockquote><ul><li>This will yield around B/3 predictions for the ith observation.</li><li>To obtain a single prediction for the ith observation, we can <strong>average</strong> these predicted responses (regression) or can take a <strong>majority vote</strong> (classification).</li><li>This leads to a single OOB prediction for the ith observation.</li></ul><p>The OOB approach for estimating the test error is particularly convenient when performing bagging on large data sets for which <strong>cross-validation</strong> would be computationally onerous.</p><h2 id="variable-importance-measures">Variable Importance Measures</h2><p><strong>Bagging improves prediction accuracy at the expense of interpretability</strong> - When we bag a large number of trees, it is no longer possible to represent the resulting statistical learning procedure using a single tree, and it is no longer clear which variables are most important to the procedure</p><p><strong>Variable Importance</strong> - One can obtain an overall summary of the importance of each predictor using the RSS (for bagging regression trees) or the Gini index (for bagging classification trees). - <strong>Bagging regression trees</strong>: Record the total amount that the RSS is decreased due to splits over a given predictor, averaged over all B trees. A large value indicates an important predictor. <span class="math display">\[\begin{align}RSS=\sum_{j=1}^J\sum_{i \in R_j} (y_i-\hat{y}_{R_j})^2\end{align}\]</span> - <strong>Bagging classification trees</strong>: Add up the total amount that the <strong>Gini index</strong> is decreased by splits over a given predictor, averaged over all B trees.</p><p><img src="./11.png" width="600"></p><h1 id="random-forest">Random Forest</h1><p><strong>Random forests</strong> provide an improvement over bagged trees by way of a small tweak that <strong>decorrelates</strong> the trees.</p><p>As in bagging, we build a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, <em>a random sample of m predictors is chosen as split candidates</em> from the full set of p predictors.</p><p><strong>The split is allowed to use only one of those m predictors.</strong> A fresh sample of m predictors is taken at each split, and typically we choose <span class="math inline">\(m ≈\sqrt{p}\)</span></p><p><strong>Rationale</strong>: - Suppose that there is one very strong predictor in the data set, along with a number of other moderately strong predictors. Then in the collection of bagged trees, most or all of the trees will use this strong predictor in the top split. Consequently, <em>all of the bagged trees will look quite similar to each other.</em> - Hence the predictions from the bagged trees will be highly correlated. Unfortunately, averaging many highly correlated quantities does not lead to as large of a reduction in variance as averaging many uncorrelated quantities.</p><p><strong>Decorrelating</strong> the trees: Random forests forces each split to consider only a subset of the predictors, making the average of the resulting trees less variable and hence more reliable.</p><h1 id="boosting">Boosting</h1><p><strong>Boosting</strong>: another approach for improving the predictions resulting from a decision tree. - Trees are grown <strong>sequentially</strong>: each tree is grown using information from previously grown trees. - Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set.</p><p><img src="./12.png" width="600"></p><p><strong>Idea behind this procedure</strong> - Unlike fitting a single large decision tree to the data, which amounts to fitting the data hard and potentially overfitting, the boosting approach instead <strong>learns slowly</strong>. - Given the current model, we fit a decision tree to the residuals from the model. That is, we fit a tree using the current residuals, rather than the outcome Y , as the response. - We then add this new decision tree into the fitted function in order to update the residuals. Each of these trees can be rather small, with just a few terminal nodes, determined by the parameter <strong>d</strong> in the algorithm. - By fitting small trees to the residuals, we slowly improve <span class="math inline">\(\hat{f}\)</span> in areas where it does not perform well. - The shrinkage parameter <strong>λ</strong> slows the process down even further, allowing more and different shaped trees to attack the residuals.</p><blockquote><p>Note that in boosting, unlike in bagging, the construction of each tree depends strongly on the trees that have already been grown.</p></blockquote><p><strong>Boosting has three tuning parameters:</strong> 1. The number of trees <span class="math inline">\(B\)</span>. 2. The shrinkage parameter <span class="math inline">\(λ\)</span>, a small positive number. This controls the rate at which boosting learns. 3. The number <span class="math inline">\(d\)</span> of splits in each tree, which controls the complexity of the boosted ensemble. Often d = 1 works well, in which case each tree is a <strong>stump</strong>, consisting of a single split. In this case, the boosted ensemble is fitting an <strong>additive model</strong>, since each term involves only a single variable. More generally <span class="math inline">\(d\)</span> is the <strong>interaction depth</strong>, and controls the interaction order of the boosted model, since <span class="math inline">\(d\)</span> splits can involve at most d variables.</p><p><strong>Boosting V.S. Random forests:</strong></p><ul><li>In boosting, because the growth of a particular tree takes into account the other trees that have already been grown, smaller trees are typically sufficient.</li><li>Using smaller trees can aid in interpretability as well; for instance, using <strong>stumps</strong> leads to an additive model.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;bagging&quot;&gt;Bagging&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Bootstrap aggregation&lt;/strong&gt;, or &lt;strong&gt;bagging&lt;/strong&gt;, is a general-purpose procedure for reducing the variance of a statistical learning method, frequently used in the context of decision trees.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Averaging a set of observations reduces variance&lt;/strong&gt;: Recall that given a set of n independent observations Z1, . . . , Zn, each with variance &lt;span class=&quot;math inline&quot;&gt;\(σ^2\)&lt;/span&gt;, the variance of the mean &lt;span class=&quot;math inline&quot;&gt;\(\bar{Z}\)&lt;/span&gt; of the observations is given by &lt;span class=&quot;math inline&quot;&gt;\(σ^2/n\)&lt;/span&gt;. - A natural way to reduce the variance and hence increase the prediction accuracy of a statistical learning method is to &lt;strong&gt;take many training sets from the population&lt;/strong&gt;, build a separate prediction model using each training set, and average the resulting predictions.&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Bagging" scheme="https://nancyyanyu.github.io/tags/Bagging/"/>
    
      <category term="Random Forest" scheme="https://nancyyanyu.github.io/tags/Random-Forest/"/>
    
      <category term="Boosting" scheme="https://nancyyanyu.github.io/tags/Boosting/"/>
    
      <category term="Trees" scheme="https://nancyyanyu.github.io/tags/Trees/"/>
    
  </entry>
  
  <entry>
    <title>ISLR Note - The Basics of Decision Trees</title>
    <link href="https://nancyyanyu.github.io/undefined/6b588a86/"/>
    <id>https://nancyyanyu.github.io/undefined/6b588a86/</id>
    <published>2019-06-09T05:36:08.114Z</published>
    <updated>2019-06-12T03:01:19.640Z</updated>
    
    <content type="html"><![CDATA[<h1 id="regression-trees">Regression Trees</h1><h2 id="predicting-baseball-players-salaries-using-regression-trees">Predicting Baseball Players’ Salaries Using Regression Trees</h2><p><strong>Terminal nodes</strong>: The regions R1, R2, and R3 are known as terminal nodes or leaves of the tree.</p><p><strong>Internal nodes</strong>: The points along the tree where the predictor space is split are referred to as internal nodes.</p><p><strong>Branches</strong>: The segments of the trees that connect the nodes as branches</p><a id="more"></a><p><img src="./2.png" width="500"> <img src="./1.png" width="500"></p><h2 id="prediction-via-stratification-of-the-feature-space">Prediction via Stratification of the Feature Space</h2><p><strong>Process of building a regression tree</strong></p><p><strong>Step 1</strong>: We divide the predictor space—that is, the set of possible values for X1,X2, . . .,Xp—into J distinct and non-overlapping regions, R1,R2, . . . , RJ .</p><p><strong>Step 2</strong>: For every observation that falls into the region Rj, we make the same prediction, which is simply the <em>mean of the response values</em> for the training observations in Rj .</p><h3 id="step-1">Step 1</h3><p><strong>How do we construct the regions R1, . . .,RJ?</strong> - We choose to divide the predictor space into high-dimensional rectangles, or <strong>boxes</strong>, for ease of interpretation of the resulting predictive model.</p><ul><li>The goal is to find boxes R1, . . . , RJ that <strong>minimize the RSS</strong>, given by <span class="math display">\[\begin{align}\sum_{j=1}^J\sum_{i \in R_j} (y_i-\hat{y}_{R_j})^2\end{align}\]</span> where <span class="math inline">\(\hat{y}_{R_j}\)</span> is the mean response for the training observations within the jth box.</li></ul><h4 id="recursive-binary-splitting">Recursive Binary Splitting</h4><p><strong>Recursive Binary Splitting</strong>: a <em>top-down, greedy</em> approach - <strong>Top-down</strong>: begins at the top of the tree (at which point all observations belong to a single region) and then successively splits the predictor space; each split is indicated via two new branches further down on the tree. - <strong>Greedy</strong>: at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step.</p><p><strong>Methods</strong>: 1. Select the predictor <span class="math inline">\(X_j\)</span> and the cutpoint <span class="math inline">\(s\)</span> such that splitting the predictor space into the regions <span class="math inline">\({X|X_j &lt; s}\)</span> and <span class="math inline">\({X|X_j ≥ s}\)</span> leads to the greatest possible reduction in RSS - In greater detail, for any <span class="math inline">\(j\)</span> and <span class="math inline">\(s\)</span>, we define the pair of half-planes <span class="math display">\[\begin{align}R_1(j, s) = {X|X_j &lt; s} ,\quad R_2(j, s) = {X|X_j ≥ s}\end{align}\]</span> and we seek the value of <span class="math inline">\(j\)</span> and <span class="math inline">\(s\)</span> that <strong>minimize</strong> the equation <span class="math display">\[\begin{align}\sum_{:x_i \in R_1(j,s)}(y_i-\hat{y}_{R_1})^2+\sum_{:x_i \in R_2(j,s)}(y_i-\hat{y}_{R_2})^2\end{align}\]</span> where <span class="math inline">\(\hat{y}_{R_1}\)</span>is the mean response for the training observations in <span class="math inline">\(R_1(j, s)\)</span>,</p><ol start="2" type="1"><li>Repeat the process, looking for the best predictor and best cutpoint in order to split the data further so as to minimize the RSS within each of the resulting regions.</li></ol><ul><li><p>However, this time, instead of splitting the entire predictor space, we split one of the two previously identified regions.</p></li><li><p>We now have three regions. Again, we look to split one of these three regions further, so as to minimize the RSS.</p></li></ul><ol start="3" type="1"><li>The process continues until a stopping criterion is reached; for instance, we may continue until no region contains more than five observations.</li></ol><p><img src="./3.png" width="600"></p><h3 id="step-2">Step 2</h3><p>Predict the response for a given test observation using the mean of the training observations in the region to which that test observation belongs.</p><h2 id="tree-pruning">Tree Pruning</h2><p>A better strategy is to grow a very large tree <span class="math inline">\(T_0\)</span>, and then <strong>prune</strong> it back in order to obtain a <strong>subtree</strong></p><h3 id="cost-complexity-pruning">Cost complexity pruning</h3><p>a.k.a.: <strong>weakest link pruning</strong></p><p>Consider a sequence of trees indexed by a nonnegative tuning parameter α</p><p><img src="./4.png" width="600"></p><p>For each value of α there corresponds a subtree <span class="math inline">\(T ⊂ T_0\)</span> such that</p><p><span class="math display">\[\begin{align}\sum_{m=1}^T\sum_{i:x_i \in R_m}(y_i − \hat{y}_{R_m})^2 + \alpha|T|  \quad \quad (8.4)\end{align}\]</span></p><p>is as small as possible.</p><ul><li><span class="math inline">\(|T|\)</span>: the number of terminal nodes of the tree T ,</li><li><span class="math inline">\(R_m\)</span>: the rectangle (i.e. the subset of predictor space) corresponding to the m-th <strong>terminal node</strong>,</li><li><span class="math inline">\(\hat{y}_{R_m}\)</span>: the predicted response associated with <span class="math inline">\(R_m\)</span>—that is, the mean of the training observations in <span class="math inline">\(R_m\)</span>.</li></ul><p>The tuning parameter <span class="math inline">\(α\)</span> controls a <em>trade-off</em> between the subtree’s <strong>complexity</strong> and its <strong>fit to the training data</strong>. When α = 0, then the subtree T will simply equal T0, because then (8.4) just measures the training error. However, as α increases, there is a price to pay for having a tree with many terminal nodes, and so the quantity (8.4) will tend to be minimized for a smaller subtree.</p><p>Equation 8.4 is reminiscent of the lasso, in which a similar formulation was used in order to control the complexity of a linear model.</p><p><img src="./5.png" width="600"> <img src="./6.png" width="600"></p><h1 id="classification-trees">Classification Trees</h1><p>For a classification tree, - We predict that each observation belongs to the <strong>most commonly occurring class</strong> of training observations in the region to which it belongs. - RSS cannot be used as a criterion for making the binary splits <span class="math inline">\(\Rightarrow\)</span> <strong>classification error rate</strong>.</p><h2 id="classification-error-rate">Classification Error Rate</h2><ul><li>Since we plan to assign an observation in a given region to the most commonly occurring class of training observations in that region, the classification error rate is simply the <strong>fraction of the training observations in that region that do not belong to the most common class</strong>:</li></ul><p><span class="math display">\[\begin{align}E=1-\max_k(\hat{p}_{mk})\end{align}\]</span></p><ul><li><span class="math inline">\(\hat{p}_{mk}\)</span> : the proportion of training observations in the mth region that are from the kth class.</li><li>classification error is not sufficiently sensitive for tree-growing, and in practice two other measures are preferable: <strong>Gini index, cross-entropy.</strong></li></ul><h2 id="gini-index">Gini index</h2><p><span class="math display">\[\begin{align}G=\sum_{k=1}^K\hat{p}_{mk}(1-\hat{p}_{mk})\end{align}\]</span></p><ul><li>A measure of total variance across the K classes. It is not hard to see that the Gini index takes on a small value if all of the <span class="math inline">\(\hat{p}_{mk}\)</span>’s are close to zero or one.</li><li>For this reason the Gini index is referred to as a measure of node <strong>purity</strong>—a small value indicates that a node contains predominantly observations from a single class.</li></ul><h2 id="cross-entropy">Cross-Entropy</h2><p><span class="math display">\[\begin{align}D=-\sum_{k=1}^K\hat{p}_{mk}\log{\hat{p}_{mk}}\end{align}\]</span></p><ul><li>Since 0 ≤ <span class="math inline">\(\hat{p}_{mk}\)</span> ≤ 1, it follows that <span class="math inline">\(0 ≤ −\hat{p}_{mk}\log{\hat{p}_{mk}}\)</span>.</li><li>Cross-entropy will take on a value near zero if the <span class="math inline">\(\hat{p}_{mk}\)</span>’s are all near zero or near one. Therefore, like the Gini index, the cross-entropy will take on a small value if the mth node is <strong>pure</strong>.</li></ul><hr><p><strong>Cross-Entropy v.s. Gini index v.s. Classification Error Rate</strong> - When building a classification tree, either the Gini index or the crossentropy are typically used to evaluate the quality of a particular split, since these two approaches are more sensitive to node purity than is the classification error rate. Any of these three approaches might be used when pruning the tree, but the classification error rate is preferable if prediction accuracy of the final pruned tree is the goal.</p><p><img src="./9.png" width="800"></p><ul><li><strong>A surprising characteristic</strong>: some of the splits yield two terminal nodes that have the same predicted value.</li><li><strong>Why is the split performed at all?</strong> The split is performed because it leads to <strong>increased node purity.</strong></li><li><strong>Why is node purity important?</strong> Suppose that we have a test observation that belongs to the region given by that right-hand leaf. Then we can be pretty certain that its response value is Yes. In contrast, if a test observation belongs to the region given by the left-hand leaf, then its response value is probably Yes, but we are much less certain. Even though the split RestECG&lt;1 does not reduce the classification error, it improves the <strong>Gini index and the cross-entropy</strong>, which are more sensitive to node purity.</li></ul><h1 id="trees-versus-linear-models">Trees Versus Linear Models</h1><p>Linear regression assumes a model of the form <span class="math display">\[\begin{align}f(X)=\beta_0+\sum_{i=1}^p\beta_iX_i\end{align}\]</span></p><p>Regression trees assume a model of the form <span class="math display">\[\begin{align}f(X)=\sum_{m=1}^Mc_m \cdot I_{X \in R_m}\end{align}\]</span> where R1, . . .,RM represent a partition of feature space</p><p><strong>Linear regression works better</strong>: If the relationship between the features and the response is well approximated by a linear model; regression tree does not exploit this linear structure.</p><p><strong>Regression tree works better</strong>: If instead there is a highly non-linear and complex relationship between the features and the response.</p><h1 id="advantages-and-disadvantages-of-trees">Advantages and Disadvantages of Trees</h1><p><strong>Advantages of decision trees for regression and classification:</strong></p><p>▲ <strong>Interpretation</strong>: Trees are very <strong>easy to explain</strong> to people. In fact, they are even easier to explain than linear regression!</p><p>▲ Some people believe that decision trees more closely <strong>mirror human decision-making</strong> than do the regression and classification approaches.</p><p>▲ <strong>Visualization</strong>: Trees can be <strong>displayed graphically</strong>, and are easily interpreted even by a non-expert.</p><p>▲ Trees can easily handle qualitative predictors without the need to create dummy variables.</p><p><strong>Disadvantages of decision trees for regression and classification:</strong></p><p>▼ Unfortunately, trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches seen in this book.</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;regression-trees&quot;&gt;Regression Trees&lt;/h1&gt;
&lt;h2 id=&quot;predicting-baseball-players-salaries-using-regression-trees&quot;&gt;Predicting Baseball Players’ Salaries Using Regression Trees&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Terminal nodes&lt;/strong&gt;: The regions R1, R2, and R3 are known as terminal nodes or leaves of the tree.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Internal nodes&lt;/strong&gt;: The points along the tree where the predictor space is split are referred to as internal nodes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Branches&lt;/strong&gt;: The segments of the trees that connect the nodes as branches&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Trees" scheme="https://nancyyanyu.github.io/tags/Trees/"/>
    
  </entry>
  
  <entry>
    <title>ISLR Note - Considerations In High Dimensions.</title>
    <link href="https://nancyyanyu.github.io/undefined/7d03d840/"/>
    <id>https://nancyyanyu.github.io/undefined/7d03d840/</id>
    <published>2019-06-09T05:33:02.541Z</published>
    <updated>2019-06-12T03:02:00.452Z</updated>
    
    <content type="html"><![CDATA[<h1 id="high-dimensional-data">High-Dimensional Data</h1><p><strong>High-dimensional</strong>: Data sets containing more features than observations are often referred to as high-dimensional. - Classical approaches such as least squares linear highregression are not appropriate in this setting</p><h1 id="what-goes-wrong-in-high-dimensions">What Goes Wrong in High Dimensions?</h1><a id="more"></a><ol type="1"><li>When the number of features p is as large as, or &gt;n, least squares cannot be performed.</li></ol><p><strong>Reason</strong>: regardless of whether or not there truly is a relationship between the features and the response, least squares will yield a set of coefficient estimates that result in a perfect fit to the data, such that the residuals are zero. - This perfect fit will almost certainly lead to overfitting of the data - The problem is simple: when p &gt; n or p ≈ n, a simple least squares regression line is too <strong><em>flexible</em></strong> and hence overfits the data.</p><ol start="2" type="1"><li>Examines only the R2 or the training set MSE might erroneously conclude that the model with the greatest number of variables is best. <img src="./12.png" width="650"></li></ol><ul><li><p><strong>Cp, AIC, and BIC</strong> approaches are not appropriate in the high-dimensional setting, because estimating ˆσ2 is problematic.(For instance, the formula for ˆσ2 from Chapter 3 yields an estimate ˆσ2 = 0 in this setting.)</p></li><li><p><strong>Adjusted R2</strong> in the high-dimensional setting is problematic, since one can easily obtain a model with an adjusted R2 value of 1.</p></li></ul><h1 id="regression-in-high-dimensions">Regression in High Dimensions</h1><p><strong>Alternative approaches better-suited to the high-dimensional setting:</strong></p><ul><li>forward stepwise selection</li><li>ridge regression</li><li>the lasso</li><li>principal components regression</li></ul><p><strong>Reason:</strong> these approaches avoid overfitting by using a less flexible fitting approach than least squares.</p><p><strong>Three important points:</strong> (1) regularization or shrinkage plays a key role in high-dimensional problems,</p><ol start="2" type="1"><li><p>appropriate tuning parameter selection is crucial for good predictive performance, and</p></li><li><p>the test error tends to increase as the dimensionality of the problem (i.e. the number of features or predictors) increases, unless the additional features are truly associated with the response.<span class="math inline">\(\Rightarrow\)</span> <strong>curse of dimensionality</strong></p></li></ol><h2 id="curse-of-dimensionality">Curse of dimensionality</h2><p>Adding additional signal features that are truly associated with the response will improve the fitted model; However, adding noise features that are not truly associated with the response will lead to a deterioration in the fitted model.</p><p><strong>Reason</strong>: This is because noise features increase the dimensionality of the problem, exacerbating the risk of overfitting (since noise features may be assigned nonzero coefficients due to chance associations with the response on the training set) without any potential upside in terms of improved test set error.</p><h1 id="interpreting-results-in-high-dimensions">Interpreting Results in High Dimensions</h1><ol type="1"><li>Be cautious in reporting the results obtained when we perform the lasso, ridge regression, or other regression procedures in the high-dimensional setting.</li></ol><ul><li>In the high-dimensional setting, the <strong>multicollinearity</strong> problem is extreme: any variable in the model can be written as a linear combination of all of the other variables in the model. This means that we can never know exactly which variables (if any) truly are predictive of the outcome, and we can never identify the best coefficients for use in the regression.</li></ul><ol start="2" type="1"><li>Be cautious in reporting errors and measures of model fit in the high-dimensional setting</li></ol><ul><li>e.g.: when p &gt; n, it is easy to obtain a useless model that has zero residuals.</li><li><strong>One should never use sum of squared errors, p-values, R2 statistics, or other traditional measures of model fit on the training data as evidence of a good model fit in the high-dimensional setting</strong></li><li>It is important to instead report results on an independent test set, or cross-validation errors. For instance, the MSE or R2 on an independent test set is a valid measure of model fit, but the MSE on the training set certainly is not.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;high-dimensional-data&quot;&gt;High-Dimensional Data&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;High-dimensional&lt;/strong&gt;: Data sets containing more features than observations are often referred to as high-dimensional. - Classical approaches such as least squares linear highregression are not appropriate in this setting&lt;/p&gt;
&lt;h1 id=&quot;what-goes-wrong-in-high-dimensions&quot;&gt;What Goes Wrong in High Dimensions?&lt;/h1&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Regression" scheme="https://nancyyanyu.github.io/tags/Regression/"/>
    
      <category term="Curse of dimensionality" scheme="https://nancyyanyu.github.io/tags/Curse-of-dimensionality/"/>
    
  </entry>
  
  <entry>
    <title>ESL Note - Subset Selection.</title>
    <link href="https://nancyyanyu.github.io/undefined/5c16c99c/"/>
    <id>https://nancyyanyu.github.io/undefined/5c16c99c/</id>
    <published>2019-06-09T05:21:13.119Z</published>
    <updated>2019-06-12T03:02:14.879Z</updated>
    
    <content type="html"><![CDATA[<h1 id="subset-selection">3.3 Subset Selection</h1><h4 id="drawbacks-of-least-squares-estimates">Drawbacks of least squares estimates:</h4><ul><li><em>prediction accuracy</em>: the least squares estimates often have low bias but large variance. Prediction accuracy can sometimes be improved by shrinking or setting some coeﬃcients to zero.By doing so we sacriﬁce a little bit of bias to reduce the variance of the predicted values, and hence may improve the overall prediction accuracy.</li><li><em>interpretation</em>: With a large number of predictors, we often would like to determine a smaller subset that exhibit the strongest eﬀects. In order to get the “big picture,” we are willing to sacriﬁce some of the small details.</li></ul><h2 id="best-subset-selection">3.3.1 Best-Subset Selection</h2><p>Best subset regression ﬁnds for each k ∈{0, 1, 2,...,p} the subset of size k that gives smallest residual sum of squares.</p><p>We choose the smallest model that minimizes an estimate of the expected prediction error.</p><a id="more"></a><figure><img src="./best_subset.png" alt="title"><figcaption>title</figcaption></figure><blockquote><p>FIGURE 3.5.All possible subset models for the prostate cancer example. At each subset size is shown the residual sum-of-squares for each model of that size.</p></blockquote><h2 id="forward--and-backward-stepwise-selection">3.3.2 Forward- and Backward-Stepwise Selection</h2><blockquote><p>Rather than search through all possible subsets (which becomes infeasible for p much larger than 40), we can seek a good path through them.</p></blockquote><h3 id="forward-stepwise-selection">Forward-stepwise selection</h3><p><strong>Forward-stepwise selection</strong> starts with the intercept, and then sequentially adds into the model the predictor that most improves the ﬁt.</p><p>Forward-stepwise selection is a <em>greedy algorithm</em>, producing a nested sequence of models. In this sense it might seem sub-optimal compared to best-subset selection.</p><h4 id="advantages">Advantages:</h4><ul><li><strong>Computational</strong>: for large p we cannot compute the best subset sequence, but we can always compute the forward stepwise sequence</li><li><strong>Statistical</strong>: a price is paid in variance for selecting the best subset of each size; forward stepwise is a more constrained search, and will have lower variance, but perhaps more bias</li></ul><h3 id="backward-stepwise-selection">Backward-stepwise selection</h3><p><strong>Backward-stepwise selection</strong> starts with the full model, and sequentially deletes the predictor that has the least impact on the ﬁt. The candidate for dropping is the variable with the smallest Z-score</p><h3 id="comparison">Comparison</h3><p>Backward-stepwise selection can only be used when N&gt;p, while forward stepwise can always be used.</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;subset-selection&quot;&gt;3.3 Subset Selection&lt;/h1&gt;
&lt;h4 id=&quot;drawbacks-of-least-squares-estimates&quot;&gt;Drawbacks of least squares estimates:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;prediction accuracy&lt;/em&gt;: the least squares estimates often have low bias but large variance. Prediction accuracy can sometimes be improved by shrinking or setting some coeﬃcients to zero.By doing so we sacriﬁce a little bit of bias to reduce the variance of the predicted values, and hence may improve the overall prediction accuracy.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;interpretation&lt;/em&gt;: With a large number of predictors, we often would like to determine a smaller subset that exhibit the strongest eﬀects. In order to get the “big picture,” we are willing to sacriﬁce some of the small details.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;best-subset-selection&quot;&gt;3.3.1 Best-Subset Selection&lt;/h2&gt;
&lt;p&gt;Best subset regression ﬁnds for each k ∈{0, 1, 2,...,p} the subset of size k that gives smallest residual sum of squares.&lt;/p&gt;
&lt;p&gt;We choose the smallest model that minimizes an estimate of the expected prediction error.&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Model Selection" scheme="https://nancyyanyu.github.io/tags/Model-Selection/"/>
    
  </entry>
  
  <entry>
    <title>ESL Note - Shrinkage Methods</title>
    <link href="https://nancyyanyu.github.io/undefined/afb70442/"/>
    <id>https://nancyyanyu.github.io/undefined/afb70442/</id>
    <published>2019-06-09T05:21:02.282Z</published>
    <updated>2019-06-12T03:02:17.171Z</updated>
    
    <content type="html"><![CDATA[<h1 id="shrinkage-methods">3.4 Shrinkage Methods</h1><blockquote><p><em>Subset selection methods</em> are discrete process—variables are either retained or discarded—it often exhibits high variance,and so doesn’t reduce the prediction error of the full model. <em>Shrinkage methods</em> are more continuous, and don’t suﬀer as much from high variability.</p></blockquote><h2 id="ridge-regression">3.4.1 Ridge Regression</h2><p><strong>Ridge regression</strong> shrinks the regression coeﬃcients by imposing a penalty on their size.The ridge coeﬃcients minimize a penalized residual sum of squares:</p><p><span class="math display">\[\begin{align}\hat{\beta}^{ridge}=argmin_\beta {\sum_{i=1}^N(y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j)^2+\lambda\sum_{j=1}^p\beta_j^2}\end{align}\]</span></p><a id="more"></a><ul><li>λ ≥ 0 is a complexity parameter that controls the amount of shrinkage</li></ul><p>Writing the criterion in matrix form:</p><p><span class="math display">\[\begin{align}RSS(\lambda)=(\mathbf{y}-\mathbf{X}\beta)^T(\mathbf{y}-\mathbf{X}\beta)+\lambda\beta^T\beta\end{align}\]</span></p><p>The ridge regression solutions: <span class="math display">\[\begin{align}\hat{\beta}^{ridge}=(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}\end{align}\]</span></p><ul><li><span class="math inline">\(\mathbf{I}\)</span> is the p×p identity matrix</li></ul><p>Note: - the ridge regression solution is again a linear function of <span class="math inline">\(\mathbf{y}\)</span>; - The solution adds a positive constant to the diagonal of <span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> before inversion, which makes the problem nonsingular.</p><h3 id="singular-value-decomposition-svd">Singular value decomposition (SVD)</h3><p>The <strong>singular value decomposition (SVD)</strong> of the centered input matrix X gives us some additional insight into the nature of ridge regression. The SVD of the N × p matrix X has the form:</p><p><span class="math display">\[\begin{align}X=UDV^T\end{align}\]</span></p><ul><li>U: N×p orthogonal matrices, with the columns of U spanning the column space of X</li><li>V: p×p orthogonal matrices, the columns of V spanning the row space of X</li><li>D: p×p diagonal matrix, with diagonal entries d1 ≥ d2 ≥···≥ dp ≥ 0 called the singular values of X. If one or more values dj =0,X is singular</li></ul><p>least squares ﬁtted vector:</p><p><span class="math display">\[\begin{align}\mathbf{X}\hat{\beta}^{ls}&amp;=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y} \\&amp;=UDV^T (VD^TU^TUDV^T)^{-1}VD^TU^Ty \\&amp;=UDV^T (VD^TDV^T)^{-1}VD^TU^Ty \\&amp;=UDV^T (V^T)^{-1}D^{-1}(D^T)^{-1}V^{-1}VD^TU^Ty \\&amp;=\mathbf{U}\mathbf{U}^T\mathbf{y}\end{align}\]</span></p><p>Note: <span class="math inline">\(\mathbf{U}^T\mathbf{y}\)</span> are the coordinates of y with respect to the orthonormal basis U.</p><p>The ridge solutions: <span class="math display">\[\begin{align}\mathbf{X}\hat{\beta}^{ridge}&amp;=\mathbf{X}(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y} \\&amp;=UD(D^2+\lambda\mathbf{I})^{-1}D^TU^Ty \\&amp;=\sum_{j=1}^p\mathbf{u}_j\frac{d^2_j}{d^2_j+\lambda}\mathbf{u}^T_j\mathbf{y}\end{align}\]</span></p><ul><li><span class="math inline">\(\mathbf{u}_j\)</span> are the columns of U</li></ul><p>Note: ridge regression computes the coordinates of y with respect to the orthonormal basis U. It then shrinks these coordinates by the factors <span class="math inline">\(\frac{d^2_j}{d^2_j+\lambda}\)</span></p><h4 id="what-does-a-small-value-of-d2_j-mean">What does a small value of <span class="math inline">\(d^2_j\)</span> mean?</h4><p>The SVD of the centered matrix X is another way of expressing the <strong>principal components</strong> of the variables in X. The sample covariance matrix is given by <span class="math inline">\(S=X^TX/N\)</span>, we have</p><p><strong>Eigen decomposition of <span class="math inline">\(X^TX\)</span>:</strong></p><p><span class="math display">\[\begin{align}\mathbf{X}^T\mathbf{X}=VD^TU^TUDV^T=VD^2V^T\end{align}\]</span></p><p>The eigenvectors <span class="math inline">\(v_j\)</span> (columns of V) are also called the <strong>principal components</strong> (or Karhunen–Loeve) directions of X. The ﬁrst principal component direction <span class="math inline">\(v_1\)</span> has the property that <span class="math inline">\(z_1=Xv_1\)</span> has the largest sample variance amongst all normalized linear combinations of the columns of X, which is:</p><p><span class="math display">\[\begin{align}Var(z_1)=Var(Xv_1)=\frac{d^2_1}{N}\end{align}\]</span></p><p>and in fact <span class="math inline">\(z_1=Xv_1=u_1d_1\)</span>. The derived variable <span class="math inline">\(z_1\)</span> is called the ﬁrst principal component of X, and hence <span class="math inline">\(u_1\)</span> is the normalized ﬁrst principal component.Subsequent principal components <span class="math inline">\(z_j\)</span> have maximum variance <span class="math inline">\(\frac{d^2_j}{N}\)</span>, subject to being orthogonal to the earlier ones.</p><p>Hence the small singular values <span class="math inline">\(d_j\)</span> correspond to directions in the column space of X having small variance, and ridge regression shrinks these directions the most.</p><h3 id="eﬀective-degrees-of-freedom">Eﬀective degrees of freedom</h3><p><span class="math display">\[\begin{align}df(\lambda)&amp;=tr[\mathbf{X}(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})^{-1}\mathbf{X}^T] \\&amp;=tr[\mathbf{H}\lambda] \\&amp;=\sum^p_{j=1}\frac{d^2_j}{d^2_j+\lambda}\end{align}\]</span></p><p>This monotone decreasing function of λ is the eﬀective degrees of freedom of the ridge regression ﬁt. Usually in a linear-regression ﬁt with p variables,the degrees-of-freedom of the ﬁt is p, the number of free parameters.</p><p>Note that &gt; df(λ)= p as λ = 0 (no regularization)</p><blockquote><p>df(λ) → 0 as λ →∞.</p></blockquote><h2 id="the-lasso">3.4.2 The Lasso</h2><p>The lasso is a shrinkage method like ridge, with subtle but important differences.The lasso estimate is deﬁned by:</p><p><span class="math display">\[\begin{align}\hat{\beta}^{lasso}&amp;=argmin_\beta\sum_{i=1}^N(y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j)^2 \\&amp; s.t. \sum_{j=1}^p|\beta_j|\leq t\end{align}\]</span></p><p>Lasso problem in <em>Lagrangian form</em>: <span class="math display">\[\begin{align}\hat{\beta}^{lasso}&amp;=argmin_\beta\{ \sum_{i=1}^N(y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j)^2+\lambda\sum_{j=1}^p|\beta_j| \}\end{align}\]</span></p><h4 id="difference-with-ridge">Difference with ridge:</h4><p>The L2 ridge penalty <span class="math inline">\(\sum_{j=1}^p\beta_j^2\)</span> is replaced by the L1 lasso penalty <span class="math inline">\(\sum_{j=1}^p|\beta_j|\)</span>. This latter constraint makes the solutions nonlinear in the <span class="math inline">\(y_i\)</span>, and there is no closed form expression as in ridge regression.</p><blockquote><p>t should be adaptively chosen to minimize an estimate of expected prediction error.</p></blockquote><ul><li>if <span class="math inline">\(t&gt;t_0=\sum_{j=1}^p|\hat{\beta_j^{ls}}|\)</span>, then the lasso estimates are the <span class="math inline">\(\hat{\beta_j^{ls}}\)</span></li><li>if <span class="math inline">\(t&gt;t_0/2\)</span>, the least squares coeﬃcients are shrunk by about 50% on average</li></ul><p>The standardized parameter: <span class="math inline">\(s=t/\sum_1^p|\hat{\beta_j}|\)</span></p><ul><li>s=1.0, the lasso coeﬃcients are the least squares estimates</li><li>s-&gt;0, as the lasso coeﬃcients -&gt;0</li></ul><h2 id="discussion-subset-selection-ridge-regression-and-the-lasso">3.4.3 Discussion: Subset Selection, Ridge Regression and the Lasso</h2><ul><li>Ridge regression: does a proportional shrinkage</li><li>Lasso: translates each coeﬃcient by a constant factor λ, truncating at zero --“soft thresholding,”</li><li>Best-subset selection: drops all variables with coeﬃcients smaller than the Mth largest --“hard-thresholding.” &lt;img src=&quot;./images/lass_ridge.png&quot;,width=550&gt;</li></ul><h3 id="bayes-view">Bayes View</h3><p>Consider the criterion</p><p><span class="math display">\[\begin{align}\tilde{\beta}&amp;=argmin_\beta\{ \sum_{i=1}^N(y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j)^2+\lambda\sum_{j=1}^p|\beta_j|^q \}\end{align}\]</span></p><p>for q ≥ 0. The contours of constant value of <span class="math inline">\(\sum_{j=1}^p|\beta_j|^q\)</span> are shown in Figure 3.12, for the case of two inputs. &lt;img src=&quot;./images/q.png&quot;,width=550&gt;</p><p><font color="red">The lasso, ridge regression and best subset selection are Bayes estimates with diﬀerent priors:</font>Thinking of <span class="math inline">\(\sum_{j=1}^p|\beta_j|^q\)</span> as the log-prior density for βj , these are also the equi-contours of the prior distribution of the parameters.</p><ul><li>q = 0 :variable subset selection, as the penalty simply counts the number of nonzero parameters;</li><li><p>q = 1 :the lasso, also Laplace distribution for each input, with density <span class="math inline">\(\frac{1}{2\tau}exp(-|\beta|/\tau)\)</span>, where <span class="math inline">\(\tau=1/\lambda\)</span></p></li><li><p>q = 2 :the ridge</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;shrinkage-methods&quot;&gt;3.4 Shrinkage Methods&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Subset selection methods&lt;/em&gt; are discrete process—variables are either retained or discarded—it often exhibits high variance,and so doesn’t reduce the prediction error of the full model. &lt;em&gt;Shrinkage methods&lt;/em&gt; are more continuous, and don’t suﬀer as much from high variability.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;ridge-regression&quot;&gt;3.4.1 Ridge Regression&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Ridge regression&lt;/strong&gt; shrinks the regression coeﬃcients by imposing a penalty on their size.The ridge coeﬃcients minimize a penalized residual sum of squares:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[\begin{align}
\hat{\beta}^{ridge}=argmin_\beta {\sum_{i=1}^N(y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j)^2+\lambda\sum_{j=1}^p\beta_j^2}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Ridge" scheme="https://nancyyanyu.github.io/tags/Ridge/"/>
    
      <category term="Lasso" scheme="https://nancyyanyu.github.io/tags/Lasso/"/>
    
  </entry>
  
  <entry>
    <title>ISLR Note - Dimension Reduction-PCA</title>
    <link href="https://nancyyanyu.github.io/undefined/cac93a23/"/>
    <id>https://nancyyanyu.github.io/undefined/cac93a23/</id>
    <published>2019-06-09T05:20:51.879Z</published>
    <updated>2019-06-12T03:01:56.241Z</updated>
    
    <content type="html"><![CDATA[<h1 id="intro-to-dimension-reduction-methods">Intro to Dimension Reduction Methods</h1><p>Subset selection and shrinkage methods all use the original predictors, X1,X2, . . . , Xp.</p><p>Dimension Reduction Methods <strong><em>transform</em></strong> the predictors and then fit a least squares model using the transformed variables.</p><h2 id="approach">Approach</h2><p>Let Z1,Z2, . . . ,ZM represent M &lt; p linear combinations of our original p predictors. That is,</p><p><span class="math display">\[\begin{align}Z_m=\sum_{j=1}^p\phi_{jm}X_j\end{align}\]</span></p><a id="more"></a><p>for some constants φ1m, φ2m . . . , φpm, m = 1, . . .,M. We can then fit the linear regression model</p><p><span class="math display">\[\begin{align}y_i=\theta_0+\sum_{m=1}^M\theta_m z_{im}+\epsilon_i  \quad  i=1,2,3,4,...,n\end{align}\]</span></p><p><strong>Dimension reduction</strong>: reduces the problem of estimating the p+1 coefficients β0, β1, . . . , βp to the simpler problem of estimating the M + 1 coefficients θ0, θ1, . . . , θM, where M &lt; p. In other words, the dimension of the problem has been reduced from p + 1 to M + 1.</p><p><span class="math display">\[\begin{align}\sum_{m=1}^M\theta_m z_{im}&amp;=\sum_{m=1}^M\theta_m \sum_{j=1}^p\phi_{jm}x_{ij}=\sum_{m=1}^M\sum_{j=1}^p\theta_m \phi_{jm}x_{ij}=\sum_{j=1}^p \beta_jx_{ij}  \\\beta_j&amp;=\sum_{m=1}^M\theta_m \phi_{jm}\end{align}\]</span></p><p><strong>All dimension reduction methods work in two steps:</strong></p><ol type="1"><li>the transformed predictors Z1, Z2, . . . , ZM are obtained.</li><li>the model is fit using these M predictors. However, the choice of Z1, Z2, . . . , ZM, or equivalently, the selection of the φjm’s, can be achieved in different ways.</li></ol><h1 id="principal-components-regression">Principal Components Regression</h1><h2 id="an-overview-of-principal-components-analysis">An Overview of Principal Components Analysis</h2><p><strong>PCA</strong>: is a technique for reducing the dimension of a n × p data matrix X.</p><h3 id="st-principal-component">1st Principal Component</h3><h4 id="interpretation-1-greatest-variability">Interpretation 1: greatest variability</h4><p><strong>The first principal component</strong> direction of the data: is that along which the observations <strong>vary the most</strong>.</p><p><img src="./7.png" width="600"></p><p>The first principal component direction is the direction along which there is the greatest variability in the data. That is, if we projected the 100 observations onto this line (as shown in the left-hand panel of Figure 6.15), then the resulting projected observations would have the largest possible variance</p><p><img src="./8.png" width="600"></p><p>The first principal component is given by the formula</p><p><span class="math display">\[\begin{align}Z_1 = 0.839 × (pop − \bar{pop}) + 0.544 × (ad − \bar{ad})\end{align}\]</span></p><p>Here φ11 = 0.839 and φ21 = 0.544 are the <strong>principal component loadings</strong>, which define the direction referred to above.</p><blockquote><p>The idea is that out of every possible linear combination of pop and ad such that <span class="math inline">\(\phi_{11}^2+\phi_{21}^2=1\)</span>, this particular linear combination yields the highest variance: i.e. this is the linear combination for which <span class="math inline">\(Var(φ_{11} × (pop − \bar{pop}) + φ_{21} × (ad − \bar{ad}))\)</span> is maximized.</p></blockquote><p><strong>Principal Component Scores</strong></p><p>The values of <span class="math inline">\(z_{11}, . . . , z_{n1}\)</span> are known as the <strong>principal component scores</strong>, and can be seen in the right-hand panel of Figure 6.15. For example,</p><p><span class="math display">\[\begin{align}z_{i1} = 0.839 × (pop_i − \bar{pop}) + 0.544 × (ad_i − \bar{ad})\end{align}\]</span></p><h4 id="interpretation-2-closest-to-data">Interpretation 2: closest to data</h4><p>There is also another interpretation for PCA: the first principal component vector defines the line that is as close as possible to the data.</p><p>In Figure 6.14, the first principal component line minimizes the sum of the squared perpendicular distances between each point and the line.</p><p>In the right-hand panel of Figure 6.15, the left-hand panel has been rotated so that the first principal component direction coincides with the x-axis. It is possible to show that the <strong><em>first principal component score</em></strong> for the ith observation is the distance in the x-direction of the ith cross from zero.</p><h4 id="interpretation-3-single-number-summarization">Interpretation 3: single number summarization</h4><p>We can think of the values of the principal component Z1 as single number summaries of the joint pop and ad budgets for each location.</p><p>In this example, if <span class="math inline">\(z_{i1} = 0.839 × (pop_i − pop) + 0.544 × (ad_i − ad) &lt; 0\)</span>, then this indicates a city with below-average population size and belowaverage ad spending.</p><p><img src="./9.png" width="650"></p><p>Figure 6.16 displays <span class="math inline">\(z_{i1}\)</span> versus both pop and ad. The plots show a strong relationship between the first principal component and the two features. In other words, the first principal component appears to <em>capture most of the information</em> contained in the pop and ad predictors.</p><h3 id="nd-principal-component">2nd Principal Component</h3><p>The second principal component Z2 is a linear combination of the variables that is uncorrelated with Z1, and has largest variance subject to this constraint.</p><p>It turns out that the zero correlation condition of Z1 with Z2 is equivalent to the condition that the direction must be perpendicular, or orthogonal, to the first principal component direction.</p><p>The second principal component is given by the formula:</p><p><span class="math display">\[\begin{align}Z_2 = 0.544 × (pop − \bar{pop}) − 0.839 × (ad − \bar{ad}).\end{align}\]</span></p><p>Figure 6.15. The fact that the second principal component scores are much closer to zero indicates that this component captures far less information.</p><h2 id="the-principal-components-regression-approach">The Principal Components Regression Approach</h2><p>The principal components regression (PCR) approach involves constructing principal components regression the first M principal components, Z1, . . ., ZM, and then using these components as the predictors in a linear regression model that is fit using least squares</p><p><strong>The key idea</strong></p><p>Often a small number of principal components suffice to explain most of the variability in the data, as well as the relationship with the response. In other words, we assume that <strong><em>the directions in which X1, . . .,Xp show the most variation are the directions that are associated with Y</em></strong></p><p><strong>Example</strong></p><p><img src="./10.png" width="650"></p><ul><li>Performing PCR with an appropriate choice of M can result in a substantial improvement over least squares</li><li>PCR does not perform as well as the two shrinkage methods</li><li><strong>Reason</strong>: The data were generated in such a way that many principal components are required in order to adequately model the response. In contrast, PCR will tend to do well in cases when the first few principal components are sufficient to capture most of the variation in the predictors as well as the relationship with the response.</li></ul><p><strong>Note</strong>: even though PCR provides a simple way to perform regression using M &lt; p predictors, it is not a <em>feature selection</em> method!</p><ul><li>This is because each of the M principal components used in the regression is a linear combination of all p of the original features.</li><li>PCR is more closely related to ridge regression than to the lasso. One can even think of ridge regression as a continuous version of PCR!</li></ul><p><strong>Cross-validation</strong>: In PCR, the number of principal components, M, is typically chosen by cross-validation.</p><p><img src="./11.png" width="650"></p><p><strong>Standardisation</strong>: When performing PCR, we generally recommend standardizing each predictor, prior to generating the principal components. - In the absence of standardization, the <em>high-variance variables</em> will tend to play a larger role in the principal components obtained, and the scale on which the variables are measured will ultimately have an effect on the final PCR model.</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;intro-to-dimension-reduction-methods&quot;&gt;Intro to Dimension Reduction Methods&lt;/h1&gt;
&lt;p&gt;Subset selection and shrinkage methods all use the original predictors, X1,X2, . . . , Xp.&lt;/p&gt;
&lt;p&gt;Dimension Reduction Methods &lt;strong&gt;&lt;em&gt;transform&lt;/em&gt;&lt;/strong&gt; the predictors and then fit a least squares model using the transformed variables.&lt;/p&gt;
&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;
&lt;p&gt;Let Z1,Z2, . . . ,ZM represent M &amp;lt; p linear combinations of our original p predictors. That is,&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[\begin{align}
Z_m=\sum_{j=1}^p\phi_{jm}X_j
\end{align}\]&lt;/span&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="PCA" scheme="https://nancyyanyu.github.io/tags/PCA/"/>
    
      <category term="Dimension Reduction" scheme="https://nancyyanyu.github.io/tags/Dimension-Reduction/"/>
    
  </entry>
  
  <entry>
    <title>ISLR Note - Shrinkage Methods</title>
    <link href="https://nancyyanyu.github.io/undefined/f9cdb6e/"/>
    <id>https://nancyyanyu.github.io/undefined/f9cdb6e/</id>
    <published>2019-06-09T05:20:41.572Z</published>
    <updated>2019-06-12T03:01:36.165Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Shrinkage Methods v.s. Subset Selection</strong>: - Subset selection methods described involve using least squares to fit a linear model that contains a subset of the predictors. - Shrinkage Methods fit a model containing all p predictors by constraining or regularizing the coefficient estimates, or equivalently, that shrinks the coefficient estimates towards zero.</p><a id="more"></a><h1 id="ridge-regression">Ridge Regression</h1><p>Recall least squares:</p><p><span class="math display">\[\begin{align}RSS=\sum_{i=1}^n\left( y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j \right)^2\end{align}\]</span></p><p><strong>Ridge regression</strong> coefficient estimates <span class="math inline">\(\hat{\beta}^R\)</span> are the values that minimize</p><p><span class="math display">\[\begin{align}\sum_{i=1}^n\left( y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j \right)^2+\lambda\sum_{j=1}^p\beta_j^2=RSS+\lambda\sum_{j=1}^p\beta_j^2\end{align}\]</span></p><p><strong>Trade-off:</strong> 1. Ridge regression seeks coefficient estimates that fit the data well, by making the RSS small. 2. <strong>shrinkage penalty</strong> <span class="math inline">\(\lambda\sum_{j=1}^p\beta_j^2\)</span> is small when β1, . . . , βp are close to zero, and so it has the effect of shrinking the estimates of βj towards zero</p><p><strong>Standardization</strong>:</p><ul><li><p><strong>scale equivariant</strong>: The standard least squares coefficient estimates are scale equivariant: multiplying Xj by a constant c simply leads to a scale scaling of the least squares coefficient estimates by a factor of 1/c.</p></li><li><p><span class="math inline">\(X_{j,\lambda}^\beta\)</span> will depend not only on the value of λ, but also on the scaling of the jth predictor, and the scaling of the other predictors. It is best to apply ridge regression after standardizing the predictors <span class="math display">\[\begin{align}\tilde{x}_{ij}=\frac{x_{ij}}{\sqrt{\frac{1}{n}\sum_{i=1}^n(x_{ij}-\bar{x}_j)^2}}\end{align}\]</span> The denominator is the estimated standard deviation of the jth predictor</p></li></ul><h2 id="ridge-regression-improves-over-least-squares">Ridge Regression Improves Over Least Squares</h2><ol type="1"><li><strong>bias-variance trade-off</strong></li></ol><ul><li>Ridge regression’s advantage over least squares is rooted in the bias-variance trade-off. As λ increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias.</li><li>At the least squares coefficient estimates, which correspond to ridge regression with λ = 0, the variance is high but there is no bias. But as λ increases, the shrinkage of the ridge coefficient estimates leads to a substantial reduction in the variance of the predictions, at the expense of a slight increase in bias. &lt;img src=&quot;./4.png&quot; width=&quot;600&gt;&quot; /</li></ul><blockquote><p>ridge regression works best in situations where the least squares estimates have high variance</p></blockquote><ol start="2" type="1"><li><strong>computational advantages over best subset selection</strong></li></ol><h1 id="the-lasso">The Lasso</h1><p>The lasso coefficients, <span class="math inline">\(\hat{\beta}_\lambda^L\)</span>, minimize the quantity <span class="math display">\[\begin{align}\sum_{i=1}^n\left( y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j \right)^2+\lambda\sum_{j=1}^p|\beta_j|=RSS+\lambda\sum_{j=1}^p|\beta_j|\end{align}\]</span></p><h2 id="another-formulation-for-ridge-regression-and-the-lasso">Another Formulation for Ridge Regression and the Lasso</h2><p>The lasso and ridge regression coefficient estimates solve the problems</p><p><span class="math display">\[\begin{align}minimize_\beta \left\{\sum_{i=1}^n\left( y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j \right)^2\right\}\,\, subject\, to \, \sum_{j=1}^p|\beta_j|\leq s \\minimize_beta \left\{\sum_{i=1}^n\left( y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j \right)^2\right\}\,\, subject\, to \, \sum_{j=1}^p\beta_j^2\leq s\end{align}\]</span></p><p>When we perform the lasso we are trying to find the set of coefficient estimates that lead to the smallest RSS, subject to the constraint that there is a budget s for how large <span class="math inline">\(\sum_{j=1}^p|\beta_j|\)</span> can be. When s is extremely large, then this budget is not very restrictive, and so the coefficient estimates can be large</p><p><strong>A close connection between the lasso, ridge regression, and best subset selection</strong>:</p><p>best subset selection is equivelant to : <span class="math display">\[\begin{align}minimize_beta \left\{\sum_{i=1}^n\left( y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j \right)^2\right\}\,\, subject\, to \, \sum_{j=1}^pI(\beta_j\neq 0)\leq s\end{align}\]</span></p><p>Therefore, we can interpret <strong>ridge regression</strong> and <strong>the lasso</strong> as computationally feasible alternatives to <strong>best subset selection</strong>.</p><h2 id="the-variable-selection-property-of-the-lasso">The Variable Selection Property of the Lasso</h2><p>The lasso and ridge regression coefficient estimates are given by the first point at which an ellipse contacts the constraint region.</p><p><strong>ridge regression</strong>: <strong>circular</strong> constraint with no sharp points, so the ridge regression coefficient estimates will be exclusively non-zero.</p><p><strong>the lasso</strong>: constraint has <strong>corners</strong> at each of the axes, and so the ellipse will often intersect the constraint region at an axis. - the <span class="math inline">\(l_1\)</span> penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter λ is sufficiently large. - Hence, much like best subset selection, the lasso performs <strong>variable selection</strong></p><blockquote><p>lasso yields <strong>sparse</strong> models</p></blockquote><p>&lt;img src=&quot;./5.png&quot; width=&quot;600&gt;&quot; /</p><h2 id="comparing-the-lasso-and-ridge-regression">Comparing the Lasso and Ridge Regression</h2><p><strong>SAME</strong>: Ridge &amp; Lasso all can yield a reduction in variance at the expense of a small increase in bias, and consequently can generate more accurate predictions.</p><p><strong>DIFFERENCES</strong>: - Unlike ridge regression, the <strong>lasso performs variable selection</strong>, and hence results in models that are easier to interpret. - ridge regression outperforms the lasso in terms of prediction error in this setting</p><p><strong>Suitable setting</strong>: - <strong>Lasso</strong>: perform better in a setting where a relatively small number of predictors have substantial coefficients, and the remaining predictors have coefficients that are very small or that equal zero. - <strong>Ridge regression</strong>: perform better when the response is a function of many predictors, all with coefficients of roughly equal size. - The number of predictors that is related to the response is never known a <strong>priori</strong> for real data sets. Cross-validation can be used in order to determine which approach is better on a particular data set.</p><p>&lt;img src=&quot;./6.png&quot; width=&quot;600&gt;&quot; /</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Shrinkage Methods v.s. Subset Selection&lt;/strong&gt;: - Subset selection methods described involve using least squares to fit a linear model that contains a subset of the predictors. - Shrinkage Methods fit a model containing all p predictors by constraining or regularizing the coefficient estimates, or equivalently, that shrinks the coefficient estimates towards zero.&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Ridge" scheme="https://nancyyanyu.github.io/tags/Ridge/"/>
    
      <category term="Lasso" scheme="https://nancyyanyu.github.io/tags/Lasso/"/>
    
  </entry>
  
  <entry>
    <title>ISLR Note - Subset Selection</title>
    <link href="https://nancyyanyu.github.io/undefined/8bf4bc53/"/>
    <id>https://nancyyanyu.github.io/undefined/8bf4bc53/</id>
    <published>2019-06-09T05:20:34.937Z</published>
    <updated>2019-06-12T03:01:31.434Z</updated>
    
    <content type="html"><![CDATA[<h1 id="best-subset-selection">Best Subset Selection</h1><p><strong>Approach</strong></p><ol type="1"><li><p>fit a separate least squares regression best subset for each possible combination of the p predictors. That is, we fit all p models selection that contain exactly one predictor, all <span class="math inline">\(\left(\begin{array}{c}p\\ 2\end{array}\right)= p(p−1)/2\)</span> models that contain exactly two predictors, and so forth.</p></li><li><p>We then look at all of the resulting models, with the goal of identifying the one that is best.</p></li></ol><a id="more"></a><p><img src="./1.png" width="600"></p><p><strong>Note</strong></p><ul><li><span class="math inline">\(RSS\)</span> of these p + 1 models decreases monotonically, and the <span class="math inline">\(R2\)</span> increases monotonically, as the number of features included in the models increases. Therefore, if we use these statistics to select the best model, then we will always end up with a model involving all of the variables</li><li>The problem of selecting the best model from among the <span class="math inline">\(2^p\)</span> possibilities considered by best subset selection is not trivial.</li></ul><h1 id="stepwise-selection">Stepwise Selection</h1><h2 id="forward-stepwise-selection">Forward Stepwise Selection</h2><p><strong>Approach</strong></p><ol type="1"><li><p><strong>Forward stepwise selection</strong> begins with a model containing no predictors, and then adds predictors to the model, one-at-a-time, until all of the predictors are in the model.</p></li><li><p>In particular, at each step the variable that gives the greatest additional improvement to the fit is added to the model.</p></li></ol><p><img src="./2.png" width="600"></p><p><strong>Forward Stepwise Selection V.S. Best Subset Selection</strong></p><ul><li>Forward stepwise selection’s computational advantage over best subset selection is clear.</li><li>Forward stepwise is not guaranteed to find the best possible model out of all <span class="math inline">\(2^p\)</span> models containing subsets of the p predictors.</li></ul><h2 id="backward-stepwise-selection">Backward Stepwise Selection</h2><p><strong>Approach</strong></p><ol type="1"><li><strong>Backward Stepwise Selection</strong> begins with the full least squares model containing all p predictors, and then iteratively removes the least useful predictor, one-at-a-time</li></ol><p><img src="./3.png" width="600"></p><p><strong>Backward Stepwise Selection V.S. Forward Stepwise Selection</strong>:</p><ul><li>Like forward stepwise selection, the backward selection approach searches through only 1+p(p+1)/2 models, and so can be applied in settings where p is too large to apply best subset selection.</li><li>Like forward stepwise selection, backward stepwise selection is not guaranteed to yield the best model containing a subset of the p predictors.</li><li>Backward selection requires that the number of samples n is larger than the number of variables p (so that the full model can be fit). In contrast, forward stepwise can be used even when n &lt; p, and so is the only viable subset method when p is very large.</li></ul><h2 id="hybrid-approaches">Hybrid Approaches</h2><p><strong>Approach</strong></p><ol type="1"><li>Variables are added to the model sequentially, in analogy to forward selection.</li><li>However, after adding each new variable, the method may also remove any variables that no longer provide an improvement in the model fit.</li></ol><p><strong>Note</strong></p><p>Such an approach attempts to more closely mimic best subset selection while retaining the computational advantages of forward and backward stepwise selection.</p><h1 id="choosing-the-optimal-model">Choosing the Optimal Model</h1><p>The training error can be a poor estimate of the test error. Therefore, RSS and R2 are not suitable for selecting the best model among a collection of models with different numbers of predictors.</p><p><strong>2 Methods</strong>:</p><ol type="1"><li><p><em>indirectly</em> estimate test error by making an adjustment to the training error to account for the bias due to overfitting.</p></li><li><p><em>directly</em> estimate the test error, using either a validation set approach or a cross-validation approach</p></li></ol><h2 id="c_p-aic-bic-adjusted-r2"><span class="math inline">\(C_p\)</span>, <span class="math inline">\(AIC\)</span>, <span class="math inline">\(BIC\)</span>, Adjusted <span class="math inline">\(R^2\)</span></h2><ul><li>the training set MSE is generally an underestimate of the test MSE. (Recall that MSE = RSS/n.)</li><li>the training error will decrease as more variables are included in the model, but the test error may not.</li><li>Therefore, training set RSS and training set R2 cannot be used to select from among a set of models with different numbers of variables.</li></ul><h3 id="c_p"><span class="math inline">\(C_p\)</span></h3><p><span class="math inline">\(C_p\)</span> estimate of test MSE:</p><p><span class="math display">\[\begin{align}C_p=\frac{1}{n}(RSS+2d\hat{\sigma}^2)\end{align}\]</span></p><p>where <span class="math inline">\(\hat{\sigma}^2\)</span> is an estimate of the variance of the error <span class="math inline">\(\epsilon\)</span></p><p><strong>Note</strong>: - The <span class="math inline">\(C_p\)</span> statistic adds a penalty of <span class="math inline">\(2d\hat{\sigma}^2\)</span> to the training RSS in order to adjust for the fact that the training error tends to underestimate the test error. - The penalty increases as the number of predictors in the model increases; this is intended to adjust for the corresponding decrease in training RSS. - If <span class="math inline">\(\hat{\sigma}^2\)</span> is an unbiased estimate of <span class="math inline">\(\sigma^2\)</span> in, then <span class="math inline">\(C_p\)</span> is an unbiased estimate of test MSE - When determining which of a set of models is best, we choose the model with the lowest <span class="math inline">\(C_p\)</span> value.</p><h3 id="aic">AIC</h3><p>The AIC criterion is defined for a large class of models fit by maximum likelihood. In the case of the model <span class="math inline">\(Y = β_0 + β_1X_1 + · · · + β_pX_p + \epsilon\)</span> with Gaussian errors, maximum likelihood and least squares are the same thing.</p><p>In this case AIC is given by</p><p><span class="math display">\[\begin{align}AIC=\frac{1}{n\hat{\sigma}^2}(RSS+2d\hat{\sigma}^2)\end{align}\]</span></p><p>For least squares models, Cp and AIC are proportional to each other</p><h3 id="bic">BIC</h3><p>For the least squares model with d predictors, the BIC is, up to irrelevant constants, given by</p><p><span class="math display">\[\begin{align}BIC=\frac{1}{n}(RSS+\log(n)d\hat{\sigma}^2)\end{align}\]</span></p><p>Since log(n) &gt; 2 for any n &gt; 7, the BIC statistic generally places a heavier penalty on models with many variables, and hence results in the selection of smaller models than Cp.</p><h3 id="adjusted-r2">Adjusted <span class="math inline">\(R^2\)</span></h3><p>Recall: <span class="math display">\[\begin{align}R^2=1 − RSS/TSS=1-\frac{RSS}{\sum(y_i-\bar{y})^2}\end{align}\]</span></p><p><strong>TSS</strong>: total sum of squares for the response</p><p>For a least squares model with d variables, <strong>the adjusted R2</strong> statistic is calculated as</p><p><span class="math display">\[\begin{align}Adjusted  \, R^2=1 − \frac{RSS/(n-d-1)}{TSS/(n-1)}\end{align}\]</span></p><p><strong>Note</strong>:</p><ul><li>a large value of adjusted R2 indicates a model with a small test error. Maximizing the adjusted R2 is equivalent to minimizing <span class="math inline">\(RSS/(n−d−1)\)</span></li><li><span class="math inline">\(RSS/(n−d−1)\)</span> may increase or decrease, due to the presence of d in the denominator.</li></ul><p><strong>Intuition</strong>: - once all of the correct variables have been included in the model, adding additional noise variables will lead to only a very small decrease in RSS - Unlike the R2 statistic, the adjusted R2 statistic pays a price for the inclusion of unnecessary variables in the model</p><h2 id="validation-and-cross-validation">Validation and Cross-Validation</h2><p>As an alternative to the approaches just discussed, we can compute the validation set error or the cross-validation error for each model under consideration, and then select the model for which the resulting estimated test error is smallest.</p><p><strong>Advantage over <span class="math inline">\(C_p, AIC, BIC\)</span></strong>: - Direct estimate of the test error, and makes fewer assumptions about the true underlying model. - Used in a wider range of model selection tasks, even in cases where it is hard to pinpoint the model degrees of freedom or hard to estimate the error variance σ2.</p><p><strong>One-standard-error rule</strong>: We first calculate the standard error of the estimated test MSE for each model size, and then select the smallest model for which the estimated test error is within one standard error of the lowest point on the curve. - <strong>Rationale</strong>: if a set of models appear to be more or less equally good, then we might as well choose the simplest model</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;best-subset-selection&quot;&gt;Best Subset Selection&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Approach&lt;/strong&gt;&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;fit a separate least squares regression best subset for each possible combination of the p predictors. That is, we fit all p models selection that contain exactly one predictor, all &lt;span class=&quot;math inline&quot;&gt;\(\left(\begin{array}{c}p\\ 2\end{array}\right)= p(p−1)/2\)&lt;/span&gt; models that contain exactly two predictors, and so forth.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We then look at all of the resulting models, with the goal of identifying the one that is best.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Model Selection" scheme="https://nancyyanyu.github.io/tags/Model-Selection/"/>
    
  </entry>
  
  <entry>
    <title>ISLR Note - Intro to Model Selection</title>
    <link href="https://nancyyanyu.github.io/undefined/a065f58f/"/>
    <id>https://nancyyanyu.github.io/undefined/a065f58f/</id>
    <published>2019-06-09T05:20:26.406Z</published>
    <updated>2019-06-12T03:01:54.130Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Setting:</strong> - In the regression setting, the standard linear model <span class="math inline">\(Y = β_0 + β_1X_1 + · · · + β_pX_p + \epsilon\)</span></p><ul><li>In the chapters that follow, we consider some approaches for extending the linear model framework.</li></ul><p><strong>Reason of using other fitting procedure than lease squares</strong>: - <strong><em>Prediction Accuracy:</em></strong> - Provided that the true relationship between the response and the predictors is approximately linear, the least squares estimates will have low bias. - If n <span class="math inline">\(\gg\)</span> p, least squares estimates tend to also have low variance <span class="math inline">\(\Rightarrow\)</span> perform well on test data. - If n is not much larger than p, least squares fit has large variance <span class="math inline">\(\Rightarrow\)</span> overfitting <span class="math inline">\(\Rightarrow\)</span> consequently poor predictions on test data - If p &gt; n, no more unique least squares coefficient estimate: the <strong>variance is infinite</strong> so the method cannot be used at all</p><p>By <strong>constraining</strong> or <strong>shrinking</strong> the estimated coefficients, we can often substantially reduce the variance at the cost of a negligible increase in bias.</p><a id="more"></a><ul><li><strong><em>Model Interpretability</em></strong>：</li><li>irrelevant variables leads to unnecessary complexity in the resulting model. By removing these variables—that is, by setting the corresponding coefficient estimates to zero—we can obtain a model that is more easily interpreted.</li><li>least squares is extremely unlikely to yield any coefficient estimates that are exactly zero <span class="math inline">\(\Rightarrow\)</span> <strong>feature selection</strong></li></ul><p><strong>Alternatives of lease squares:</strong></p><ol type="1"><li>Subset Selection</li><li>Shrinkage</li><li>Dimension Reduction</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Setting:&lt;/strong&gt; - In the regression setting, the standard linear model &lt;span class=&quot;math inline&quot;&gt;\(Y = β_0 + β_1X_1 + · · · + β_pX_p + \epsilon\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In the chapters that follow, we consider some approaches for extending the linear model framework.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Reason of using other fitting procedure than lease squares&lt;/strong&gt;: - &lt;strong&gt;&lt;em&gt;Prediction Accuracy:&lt;/em&gt;&lt;/strong&gt; - Provided that the true relationship between the response and the predictors is approximately linear, the least squares estimates will have low bias. - If n &lt;span class=&quot;math inline&quot;&gt;\(\gg\)&lt;/span&gt; p, least squares estimates tend to also have low variance &lt;span class=&quot;math inline&quot;&gt;\(\Rightarrow\)&lt;/span&gt; perform well on test data. - If n is not much larger than p, least squares fit has large variance &lt;span class=&quot;math inline&quot;&gt;\(\Rightarrow\)&lt;/span&gt; overfitting &lt;span class=&quot;math inline&quot;&gt;\(\Rightarrow\)&lt;/span&gt; consequently poor predictions on test data - If p &amp;gt; n, no more unique least squares coefficient estimate: the &lt;strong&gt;variance is infinite&lt;/strong&gt; so the method cannot be used at all&lt;/p&gt;
&lt;p&gt;By &lt;strong&gt;constraining&lt;/strong&gt; or &lt;strong&gt;shrinking&lt;/strong&gt; the estimated coefficients, we can often substantially reduce the variance at the cost of a negligible increase in bias.&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Model Selection" scheme="https://nancyyanyu.github.io/tags/Model-Selection/"/>
    
  </entry>
  
  <entry>
    <title>ISLR Note - Cross Validation</title>
    <link href="https://nancyyanyu.github.io/undefined/6d11b2f4/"/>
    <id>https://nancyyanyu.github.io/undefined/6d11b2f4/</id>
    <published>2019-06-09T05:10:44.954Z</published>
    <updated>2019-06-12T03:01:58.391Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Resampling methods</strong>:involve repeatedly drawing samples from a training set and refitting a mode of interest on each sample in order to obtain additional information about the fitted model.</p><p><strong>model assessment</strong>： The process of evaluating a model’s performance</p><p><strong>model selection</strong>：The process of selecting the proper level of flexibility for a model</p><p><strong>cross-validation</strong>: can be used to estimate the test error associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility.</p><p><strong>bootstrap</strong>:provide a measure of accuracy of a parameter estimate or of a given selection statistical learning method.</p><a id="more"></a><h1 id="the-validation-set-approach">The Validation Set Approach</h1><p><strong>The Validation Set Approach</strong>:</p><ol type="1"><li><p>Randomly dividing the available set of observations into two parts, a <strong>training set</strong> and a <strong>validation set</strong> or hold-out set.</p></li><li><p>The model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set.</p></li><li><p>The resulting validation set error rate—typically assessed using MSE in the case of a quantitative response—provides an estimate of the test error rate.</p></li></ol><p><strong>Disadvantage</strong>:</p><ol type="1"><li><p>The validation estimate of the test error rate can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set.</p></li><li><p>In the validation approach, only a subset of the observations—those that are included in the training set rather than in the validation set—are used to fit the model. Since statistical methods tend to perform worse when trained on <em>fewer observations</em>, this suggests that the validation set error rate may tend to overestimate the test error rate for the model fit on the entire data set.</p></li></ol><h1 id="k-fold-cross-validation">k-Fold Cross-Validation</h1><p><strong>Approach</strong>:</p><ol type="1"><li><p>Randomly k-fold CV dividing the set of observations into k groups, or folds, of approximately equal size.</p></li><li><p>The first fold is treated as a validation set, and the method is fit on the remaining k − 1 folds.</p></li><li><p>The mean squared error, MSE1, is then computed on the observations in the held-out fold. This procedure is repeated k times; each time, a different group of observations is treated as a validation set.</p></li><li><p>This process results in k estimates of the test error, MSE1,MSE2, . . . ,MSEk.</p></li><li><p>The k-fold CV estimate is computed by averaging these values,</p></li></ol><p><span class="math display">\[\begin{align}CV_{(k)}=\frac{1}{k}\sum_{i=1}^kMSE_i\end{align}\]</span></p><p><strong>Goal</strong>：</p><ol type="1"><li><p>Determine how well a given statistical learning procedure can be expected to perform on independent data</p></li><li><p>We are interested only in the location of the minimum point in the estimated test MSE curve. This is because we might be performing cross-validation on a number of statistical learning methods, or on a single method using different levels of flexibility, in order to identify the method that results in the lowest test error.</p></li></ol><h1 id="bias-variance-trade-off-for-k-fold-cross-validation">Bias-Variance Trade-Off for k-Fold Cross-Validation</h1><p><strong>Leave-One-Out Cross-Validation V.S. k-Fold Cross-Validation</strong>: - k-Fold more biased than LOOCV - LOOCV will give approximately unbiased estimates of the test error, since each training set contains n − 1 observations, which is almost as many as the number of observations in the full data set. - k-fold CV for, say, k = 5 or k = 10 will lead to an intermediate level of bias</p><ul><li>k-Fold less variance than LOOCV</li><li>When we perform LOOCV, we are in effect averaging the outputs of n fitted models, each of which is trained on an almost identical set of observations; therefore, these outputs are highly (positively) correlated with each other.</li><li>the mean of many highly correlated quantities has higher variance than does the mean of many quantities that are not as highly correlated</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Resampling methods&lt;/strong&gt;:involve repeatedly drawing samples from a training set and refitting a mode of interest on each sample in order to obtain additional information about the fitted model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;model assessment&lt;/strong&gt;： The process of evaluating a model’s performance&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;model selection&lt;/strong&gt;：The process of selecting the proper level of flexibility for a model&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;cross-validation&lt;/strong&gt;: can be used to estimate the test error associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;bootstrap&lt;/strong&gt;:provide a measure of accuracy of a parameter estimate or of a given selection statistical learning method.&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Model Selection" scheme="https://nancyyanyu.github.io/tags/Model-Selection/"/>
    
      <category term="Cross Validation" scheme="https://nancyyanyu.github.io/tags/Cross-Validation/"/>
    
  </entry>
  
  <entry>
    <title>ISLR Note - Assessing Model Accuracy</title>
    <link href="https://nancyyanyu.github.io/undefined/86b37baa/"/>
    <id>https://nancyyanyu.github.io/undefined/86b37baa/</id>
    <published>2019-06-09T05:10:24.388Z</published>
    <updated>2019-06-12T03:02:12.451Z</updated>
    
    <content type="html"><![CDATA[<p><strong><em>no free lunch in statistics</em></strong>: no one method dominates all others over all possible data sets.</p><ul><li>Explanation: On a particular data set, one specific method may work best, but some other method may work better on a similar but different data set. Hence it is an important task to decide for any given set of data which method produces the best results.</li></ul><h1 id="measuring-the-quality-of-fit">Measuring the Quality of Fit</h1><h2 id="mean-squared-error-mse">mean squared error (MSE)</h2><p><span class="math display">\[\begin{align}MSE=\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{f}(x_i))^2\end{align}\]</span></p><p><strong><em>overfitting</em></strong>: When a given method yields a small training MSE but a large test MSE. - Explanation: a less flexible model would have yielded a smaller test MSE. This happens because our statistical learning procedure is working too hard to find patterns in the training data, and may be picking up some patterns that are just caused by random chance rather than by true properties of the unknown function f.</p><a id="more"></a><p><img src="./1.png" width="500"></p><h1 id="the-bias-variance-trade-off">The Bias-Variance Trade-Off</h1><h2 id="decomposition">Decomposition</h2><p>The expected test MSE, for a given value <span class="math inline">\(x_0\)</span> can always be decomposed into the sum of three fundamental quantities: <strong>the variance of <span class="math inline">\(\hat{f}(x_0)\)</span>, the squared bias of <span class="math inline">\(\hat{f}(x_0)\)</span>, and the variance of the error variance terms <span class="math inline">\(\epsilon\)</span>.</strong></p><p><span class="math display">\[\begin{align}E(y_0-\hat{f}(x_0))^2=Var(\hat{f}(x_0))+[Bias(\hat{f}(x_0))]^2+Var(\epsilon)\end{align}\]</span></p><p>The overall expected test MSE can be computed by averaging <span class="math inline">\(E(y_0-\hat{f}(x_0))^2\)</span> over all possible values of x0 in the test set.</p><h3 id="variance">Variance</h3><p><strong><em>Variance</em></strong>: refers to the amount by which <span class="math inline">\(\hat{f}\)</span> would change if we estimated it using a different training data set. <strong><em>more flexible statistical methods have higher variance</em></strong> - Explanation: different training data sets will result in a different <span class="math inline">\(\hat{f}\)</span>. But ideally the estimate for f should not vary too much between training sets. However, if a method has high variance then small changes in the training data can result in large changes in <span class="math inline">\(\hat{f}\)</span></p><h3 id="bias">Bias</h3><p><strong><em>bias</em></strong>: refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model. - Explanation: As we increase the flexibility of a class of methods, the bias tends to initially decrease faster than the variance increases. Consequently, the expected test MSE declines. However, at some point increasing flexibility has little impact on the bias but starts to significantly increase the variance. When this happens the test MSE increases.</p><p><img src="./2.png" width="600"> <img src="./4.png" width="400"></p><hr><h3 id="math-explanation">Math Explanation</h3><p><strong>Math Explanation</strong>: If we assume that <span class="math inline">\(Y=f(X)+\epsilon\)</span> where <span class="math inline">\(E(\epsilon)=0\)</span>, and <span class="math inline">\(Var(\epsilon)=\sigma^2_\epsilon\)</span>, we can derive an expression for the expected prediction error of a regression fit <span class="math inline">\(\hat{f}(X)\)</span> at an input point X = x0, using squared-error loss:</p><p><span class="math display">\[\begin{align}Err(x_0)&amp;=E[(Y-\hat{f}(x_0))^2|X=x_0] \\&amp;=E[(f(x_0)+\epsilon-\hat{f}(x_0))^2] \\&amp;=E[\epsilon^2+(f(x_0)-\hat{f}(x_0))^2+2\epsilon(f(x_0)-\hat{f}(x_0))] \\&amp;=\sigma^2_\epsilon+E[f(x_0)^2+\hat{f}(x_0)^2-2f(x_0)\hat{f}(x_0)] \\&amp;=\sigma^2_\epsilon+E[\hat{f}(x_0)^2]+f(x_0)^2-2f(x_0)E[\hat{f}(x_0)]  \\&amp;=\sigma^2_\epsilon+(E[\hat{f}(x_0)])^2+f(x_0)^2-2f(x_0)E[\hat{f}(x_0)] +E[\hat{f}(x_0)^2]-(E[\hat{f}(x_0))^2 \\&amp;=\sigma^2_\epsilon+(E\hat{f}(x_0)-f(x_0))^2+Var(\hat{f}(x_0))\\&amp;=\sigma^2_\epsilon+Bias^2(\hat{f}(x_0))+Var(\hat{f}(x_0))\\&amp;= Irreducible Error+ Bias^2 + Variance\end{align}\]</span></p><ol type="1"><li>The first term is the variance of the target around its true mean f(x0), and cannot be avoided no matter how well we estimate f(x0), unless <span class="math inline">\(\sigma^2_\epsilon=0\)</span></li><li>The second term is the squared bias, the amount by which the average of our estimate differs from the true mean</li><li>The last term is the variance; the expected squared deviation of <span class="math inline">\(\hat{f}(x_0)\)</span> around its mean.</li></ol><blockquote><p>Typically the more complex we make the model <span class="math inline">\(\hat{f}\)</span>, the lower the (squared) bias but the higher the variance.</p></blockquote><h1 id="the-classification-setting">The Classification Setting</h1><p><strong><em>training error rate</em></strong>： <span class="math inline">\(\frac{1}{n}\sum_{i=1}^nI(y_i\neq\hat{y}_i)\)</span></p><p>Here <span class="math inline">\(\hat{y}_i\)</span> is the predicted class label for the ith observation using <span class="math inline">\(\hat{f}\)</span></p><p><strong><em>test error rate</em></strong>： <span class="math inline">\(Ave (I(y_0 \neq \hat{y}_0))\)</span></p><p>where <span class="math inline">\(\hat{y}_0\)</span> is the predicted class label that results from applying the classifier to the test observation with predictor x0. A good classifier is one for which the test error is smallest.</p><h2 id="the-bayes-classifier">The Bayes Classifier</h2><p><strong>Bayes classifier</strong>: <span class="math inline">\(Pr(Y=j|X=x_0)\)</span> - <strong>Explanation</strong>: The test error rate given in <span class="math inline">\(Ave (I(y_0 \neq \hat{y}_0))\)</span> is minimized, on average, by a very simple classifier that assigns each observation to the most likely class, given its predictor values. - <strong>Example</strong>: In a two-class problem where there are only two possible response values, say class 1 or class 2, the Bayes classifier corresponds to predicting class one if <span class="math inline">\(Pr(Y=1|X=x_0)\)</span>&gt; 0.5, and class two otherwise. - <strong>Disadvantage</strong>: For real data, we do not know the conditional distribution of Y given X, and so computing the Bayes classifier is impossible. Therefore, the Bayes classifier serves as an unattainable gold standard against which to compare other methods</p><p><strong>Bayes error rate</strong>: <span class="math inline">\(1-E\left(\max_jPr(Y=j|X)\right)\)</span></p><p>Since the Bayes classifier will always choose the class Bayes error <span class="math inline">\(Pr(Y=j|X=x_0)\)</span> is largest, the error rate at X = x0 will be <span class="math inline">\(1-\max_jPr(Y=j|X)\)</span></p><h2 id="k-nearest-neighbors">K-Nearest Neighbors</h2><p><strong>K-nearest neighbors (KNN) classifier</strong>: 1. Given a positive integer <span class="math inline">\(K\)</span> and a test observation <span class="math inline">\(x_0\)</span>, the KNN classifier first identifies the neighbors <span class="math inline">\(K\)</span> points in the training data that are closest to <span class="math inline">\(x_0\)</span>, represented by <span class="math inline">\(N_0\)</span>.</p><ol start="2" type="1"><li><p>It then estimates the conditional probability for class <span class="math inline">\(j\)</span> as the fraction of points in <span class="math inline">\(N_0\)</span> whose response values equal <span class="math inline">\(j\)</span>: <span class="math display">\[\begin{align}Pr(Y=j|X=x_0)=\frac{1}{K}\sum_{i\in N_0} I(y_i=j)\end{align}\]</span></p></li><li><p>Finally, KNN applies Bayes rule and classifies the test observation <span class="math inline">\(x_0\)</span> to the class with the largest probability.</p></li></ol><p><img src="./3.png" width="600"></p><p>As K grows, the method becomes less flexible and produces a decision boundary that is close to linear. This corresponds to a low-variance but high-bias classifier.</p><p>As we use more flexible classification methods, the training error rate will decline but the test error rate may not.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;&lt;em&gt;no free lunch in statistics&lt;/em&gt;&lt;/strong&gt;: no one method dominates all others over all possible data sets.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explanation: On a particular data set, one specific method may work best, but some other method may work better on a similar but different data set. Hence it is an important task to decide for any given set of data which method produces the best results.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;measuring-the-quality-of-fit&quot;&gt;Measuring the Quality of Fit&lt;/h1&gt;
&lt;h2 id=&quot;mean-squared-error-mse&quot;&gt;mean squared error (MSE)&lt;/h2&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[\begin{align}
MSE=\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{f}(x_i))^2
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;overfitting&lt;/em&gt;&lt;/strong&gt;: When a given method yields a small training MSE but a large test MSE. - Explanation: a less flexible model would have yielded a smaller test MSE. This happens because our statistical learning procedure is working too hard to find patterns in the training data, and may be picking up some patterns that are just caused by random chance rather than by true properties of the unknown function f.&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Model Assessment" scheme="https://nancyyanyu.github.io/tags/Model-Assessment/"/>
    
  </entry>
  
  <entry>
    <title>ISLR Note - The Bootstrap</title>
    <link href="https://nancyyanyu.github.io/undefined/56253a8a/"/>
    <id>https://nancyyanyu.github.io/undefined/56253a8a/</id>
    <published>2019-06-09T05:10:14.096Z</published>
    <updated>2019-06-12T03:01:28.814Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Approach</strong>:</p><ol type="1"><li><p>A data set, which we call Z, that contains n observations. We randomly select n observations from the data set in order to produce a bootstrap data set, <span class="math inline">\(Z^{∗1}\)</span>.</p></li><li>The sampling is performed with <strong>replacement</strong>, which means that the replacement same observation can occur more than once in the bootstrap data set.</li></ol><ul><li>In this example, <span class="math inline">\(Z^{∗1}\)</span> contains the third observation twice, the first observation once, and no instances of the second observation.</li><li>Note that if an observation is contained in <span class="math inline">\(Z^{∗1}\)</span>, then both its X and Y values are included.</li></ul><a id="more"></a><ol start="3" type="1"><li><p>We can use <span class="math inline">\(Z^{∗1}\)</span> to produce a new bootstrap estimate for α, which we call <span class="math inline">\(\alpha^{∗1}\)</span>. This procedure is repeated B times for some large value of B, in order to produce B different bootstrap data sets, <span class="math inline">\(Z^{∗1}\)</span>,<span class="math inline">\(Z^{∗2}\)</span>, . . . , <span class="math inline">\(Z^{∗B}\)</span>, and B corresponding α estimates, <span class="math inline">\(\alpha^{∗1}\)</span>, <span class="math inline">\(\alpha^{∗2}\)</span>, . . . , <span class="math inline">\(\alpha^{∗B}\)</span>.</p></li><li><p>We can compute the standard error of these bootstrap estimates using the formula <span class="math display">\[\begin{align}SE_B(\hat{\alpha})=\sqrt{\frac{1}{B-1}\sum_{i=1}^B\left( \hat{\alpha}^{*i}-\frac{1}{B}\sum^{B}_{j=1}\hat{\alpha}^{*j} \right)}\end{align}\]</span> This serves as an estimate of the standard error of <span class="math inline">\(\hat{\alpha}\)</span> estimated from the original data set.</p></li></ol><p><img src="./1.png" width="600"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Approach&lt;/strong&gt;:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;A data set, which we call Z, that contains n observations. We randomly select n observations from the data set in order to produce a bootstrap data set, &lt;span class=&quot;math inline&quot;&gt;\(Z^{∗1}\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;The sampling is performed with &lt;strong&gt;replacement&lt;/strong&gt;, which means that the replacement same observation can occur more than once in the bootstrap data set.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;In this example, &lt;span class=&quot;math inline&quot;&gt;\(Z^{∗1}\)&lt;/span&gt; contains the third observation twice, the first observation once, and no instances of the second observation.&lt;/li&gt;
&lt;li&gt;Note that if an observation is contained in &lt;span class=&quot;math inline&quot;&gt;\(Z^{∗1}\)&lt;/span&gt;, then both its X and Y values are included.&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Bootstrap" scheme="https://nancyyanyu.github.io/tags/Bootstrap/"/>
    
  </entry>
  
  <entry>
    <title>ESL Note - Bias, Variance</title>
    <link href="https://nancyyanyu.github.io/undefined/2a71b2a0/"/>
    <id>https://nancyyanyu.github.io/undefined/2a71b2a0/</id>
    <published>2019-06-09T05:09:26.121Z</published>
    <updated>2019-06-12T03:02:29.082Z</updated>
    
    <content type="html"><![CDATA[<h1 id="bias-variance-and-model-complexity">Bias, Variance and Model Complexity</h1><p><strong>Test error</strong> (generalization error): the prediction error over an independent test sample <span class="math display">\[\begin{align}Err_\tau=E[L(Y,\hat{f}(X))|\tau]\end{align}\]</span> Here the training set <span class="math inline">\(\tau\)</span> is fixed, and test error refers to the error for this specific training set.</p><a id="more"></a><p><strong>Expected test error: </strong> <span class="math display">\[\begin{align}Err=E[L(Y,\hat{f}(X)]=E[Err_\tau]\end{align}\]</span> This expectation averages over everything that is random, including the randomness in the training set that produced <span class="math inline">\(\hat{f}\)</span></p><p><strong>Training error</strong>: the average loss over the training sample <span class="math display">\[\begin{align}\bar{err}=\frac{1}{N}\sum_{i=1}^NL(y_i,\hat{f}(x_i))\end{align}\]</span></p><p><img src="./bv.PNG" width="370"></p><p><strong>Model selection:</strong> estimating the performance of different models in order to choose the best one.</p><p><strong>Model assessment:</strong> having chosen a final model, estimating its prediction error (generalization error) on new data.</p><p>Randomly divide the dataset into three parts: - a <strong>training set</strong>: fit the models - a <strong>validation set</strong>: estimate prediction error for model selection - a <strong>test set</strong>: assessment of the generalization error of the nal chosen model</p><p>A typical split might be 50% for training, and 25% each for validation and testing:</p><h1 id="the-bias-variance-decomposition">The Bias Variance Decomposition</h1><h2 id="general-model">General Model</h2><p>If we assume that <span class="math inline">\(Y=f(X)+\epsilon\)</span> where <span class="math inline">\(E(\epsilon)=0\)</span>, and <span class="math inline">\(Var(\epsilon)=\sigma^2_\epsilon\)</span>, we can derive an expression for the expected prediction error of a regression fit <span class="math inline">\(\hat{f}(X)\)</span> at an input point X = x0, using squared-error loss:</p><p><span class="math display">\[\begin{align}Err(x_0)&amp;=E[(Y-\hat{f}(x_0))^2|X=x_0] \\&amp;=E[(f(x_0)+\epsilon-\hat{f}(x_0))^2] \\&amp;=E[\epsilon^2+(f(x_0)-\hat{f}(x_0))^2+2\epsilon(f(x_0)-\hat{f}(x_0))] \\&amp;=\sigma^2_\epsilon+E[f(x_0)^2+\hat{f}(x_0)^2-2f(x_0)\hat{f}(x_0)] \\&amp;=\sigma^2_\epsilon+E[\hat{f}(x_0)^2]+f(x_0)^2-2f(x_0)E[\hat{f}(x_0)]  \\&amp;=\sigma^2_\epsilon+(E[\hat{f}(x_0)])^2+f(x_0)^2-2f(x_0)E[\hat{f}(x_0)] +E[\hat{f}(x_0)^2]-(E[\hat{f}(x_0))^2 \\&amp;=\sigma^2_\epsilon+(E\hat{f}(x_0)-f(x_0))^2+Var(\hat{f}(x_0))\\&amp;=\sigma^2_\epsilon+Bias^2(\hat{f}(x_0))+Var(\hat{f}(x_0))\\&amp;= Irreducible Error+ Bias^2 + Variance\end{align}\]</span></p><ol type="1"><li>The first term is the variance of the target around its true mean f(x0), and cannot be avoided no matter how well we estimate f(x0), unless <span class="math inline">\(\sigma^2_\epsilon=0\)</span></li><li>The second term is the squared bias, the amount by which the average of our estimate differs from the true mean</li><li>The last term is the variance; the expected squared deviation of <span class="math inline">\(\hat{f}(x_0)\)</span> around its mean.</li></ol><blockquote><p>Typically the more complex we make the model <span class="math inline">\(\hat{f}\)</span>, the lower the (squared) bias but the higher the variance.</p></blockquote><h2 id="knn-regression">KNN regression</h2><p>For the k-nearest-neighbor regression t, these expressions have the sim- ple form <span class="math display">\[\begin{align}Err(x_0)&amp;=E[(Y-\hat{f}_k(x_0))^2|X=x_0] \\\end{align}\]</span></p><p><img src="./bv2.PNG" width="470"></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;bias-variance-and-model-complexity&quot;&gt;Bias, Variance and Model Complexity&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Test error&lt;/strong&gt; (generalization error): the prediction error over an independent test sample &lt;span class=&quot;math display&quot;&gt;\[\begin{align}
Err_\tau=E[L(Y,\hat{f}(X))|\tau]
\end{align}\]&lt;/span&gt; Here the training set &lt;span class=&quot;math inline&quot;&gt;\(\tau\)&lt;/span&gt; is fixed, and test error refers to the error for this specific training set.&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Model Assessment" scheme="https://nancyyanyu.github.io/tags/Model-Assessment/"/>
    
      <category term="Model Selection" scheme="https://nancyyanyu.github.io/tags/Model-Selection/"/>
    
  </entry>
  
  <entry>
    <title>ESL Note - Logistic Regression</title>
    <link href="https://nancyyanyu.github.io/undefined/c759e252/"/>
    <id>https://nancyyanyu.github.io/undefined/c759e252/</id>
    <published>2019-06-09T05:07:28.846Z</published>
    <updated>2019-06-12T03:02:20.299Z</updated>
    
    <content type="html"><![CDATA[<h1 id="logistic-regression">Logistic Regression</h1><p>Reference: Trevor Hastie, Robert Tibshirani, Jerome Friedman The elements of statistical learning Data mining, inference, and prediction</p><p>The logistic regression model arises from the desire to model the posterior probabilities of the K classes via linear functions in x, while at the same time ensuring that they sum to one and remain in [0, 1]. The model has the form:</p><a id="more"></a><p><span class="math display">\[\begin{align}\log\frac{\text{Pr}(G=1|X=x)}{\text{Pr}(G=K|X=x)} &amp;= \beta_{10} + \beta_1^Tx \\\log\frac{\text{Pr}(G=2|X=x)}{\text{Pr}(G=K|X=x)} &amp;= \beta_{20} + \beta_2^Tx \\&amp;\vdots \\\log\frac{\text{Pr}(G=K-1|X=x)}{\text{Pr}(G=K|X=x)} &amp;= \beta_{(K-1)0} + \beta_{K-1}^Tx \\\end{align}\]</span></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;logistic-regression&quot;&gt;Logistic Regression&lt;/h1&gt;
&lt;p&gt;Reference: Trevor Hastie, Robert Tibshirani, Jerome Friedman The elements of statistical learning Data mining, inference, and prediction&lt;/p&gt;
&lt;p&gt;The logistic regression model arises from the desire to model the posterior probabilities of the K classes via linear functions in x, while at the same time ensuring that they sum to one and remain in [0, 1]. The model has the form:&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Logistic Regression" scheme="https://nancyyanyu.github.io/tags/Logistic-Regression/"/>
    
      <category term="Classification" scheme="https://nancyyanyu.github.io/tags/Classification/"/>
    
  </entry>
  
</feed>
