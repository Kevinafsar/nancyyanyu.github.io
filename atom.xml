<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Nancy&#39;s Notes</title>
  
  <subtitle>Code changes world!</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://nancyyanyu.github.io/"/>
  <updated>2019-06-27T23:06:55.360Z</updated>
  <id>https://nancyyanyu.github.io/</id>
  
  <author>
    <name>Nancy Yan</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>DFS,HDFS,Architecture,Scaling problem</title>
    <link href="https://nancyyanyu.github.io/undefined/2a71b2a0/"/>
    <id>https://nancyyanyu.github.io/undefined/2a71b2a0/</id>
    <published>2019-06-27T23:04:59.943Z</published>
    <updated>2019-06-27T23:06:55.360Z</updated>
    
    <content type="html"><![CDATA[<h1 id="section"></h1><h1 id="scaling-dfs">Scaling DFS</h1><h2 id="big-data-storage">Big data storage:</h2><ul><li>Scale up (vertical scaling): get yourself a bigger hard drive</li><li>Lower latency</li><li>Scale out (horizontal scaling)</li><li>Higher latency</li><li>Problem: one node get out of service 3 years averagely</li></ul><p>-&gt; Distributed file system <img src="./images/week_1.png" width="400"></p><h2 id="google-file-system">Google File System:</h2><p><strong>Keys:</strong> - components failures are a norm (→ replication) - even space utilisation: all files splited into blocks of fixed size, about 100mb - write-once-read-many: it's not allowed to modify in the middle, as it drastically simplifies API and internal implementation of a distributed file system.</p><p><strong>Replication:</strong> <img src="./images/week_2.png" width="400"> &gt; S.txt and B.txt. They are both split into equal sized blocks, and then distributed over a different machine, with replications. Storage machines are called <em>channel servers</em>, or <em>data nodes</em>.</p><p><strong>Metadata:</strong> include administrative information about creation time, access properties...</p><p><strong>Master node:</strong> stores all metadata in memory; enable to request metadata with minimal latency</p><h2 id="hadoop-distributed-file-system">Hadoop Distributed File System</h2><blockquote><p>an open source implementation of Google File System</p></blockquote><p><strong>Server roles</strong>: 1. Namenode: master node 2. Datanode</p><p><img src="./images/week_3.png" width="500"></p><p>HDFS client provides command line interface to communicate with that distributed file system -&gt;no need to write any code to access data.</p><h3 id="how-to-read-files-from-hdfs">How to read files from HDFS?</h3><p><img src="./images/week_4.png" width="400"></p><ol type="1"><li><p>Request name node to get information about file blocks' locations. &gt;These blocks are distributed over different machines, but all of this complexity is hidden behind HDFS API.</p></li><li><p>User only sees a continuous stream of data. &gt;If at some point a datanode you retrieve data from died you get this data from another replica without bothering users about it.</p></li><li><p>You will get data from the closest machine.</p></li></ol><h4 id="closeness">Closeness</h4><blockquote><p>data center topology; it depends on the physical distance and unpredictable system load such as metric overutilization</p></blockquote><p><img src="./images/week_5.png" width="400"></p><ul><li>d=0: request data from HDFS, and this data is available on the same machine, then you can use data locality to read data directly from hard drive without any extra RPC codes.</li><li>d=2: a datanode is located in the same rack</li><li>d=4: read data from another rack</li><li>d=6: the data is allocated in another data center</li></ul><h3 id="how-to-write-files-into-hdfs">How to write files into HDFS?</h3><h4 id="redundancy-model">Redundancy model:</h4><blockquote><p>When you write a block of data into HDFS, Hadoop distributes replicas over the storage.</p></blockquote><p><strong>first replica</strong>: located on the same node if write data from a DataNode machin; otw, the first DataNode to put replica is chosen by random.</p><p><strong>second replica</strong>: placed in a different rack. If this racks goes down (power supply problems), you will access data from another rack.</p><p><strong>third replica</strong>: located on a different machine in the same rack as the second replica. You don't pay for extra between rack network utilization as the third replica is copied from the second data node.</p><p><strong>further replicas</strong>: applies on the random nodes in the cluster</p><p><img src="./images/week_6.png" width="150"></p><h4 id="data-flow-of-writing-data">Data flow of writing data</h4><p><img src="./images/week_7.png" width="500"></p><ol type="1"><li><p>HDFS client request and name node via RPC protocol. &gt; The name node validates if you have rights to create a file and there are no naming conflicts.</p></li><li><p>HDFS client requests a list of datanodes to put a fraction of blocks of the file. &gt; These datanodes form a pipeline as your first client sends packets of data to the closest datanode. The later one transfers copies of packets through a datanode pipeline. As soon as packet is on all of the datanodes, datanodes send acknowledgment packets back.</p></li></ol><p><strong>If something goes wrong</strong> - then if the client closes the datanode pipeline, marks the misbehaving datanode bad and requests a replacement for the bad datanodes from a name node. So a new data node pipeline will be organized, and the process of writing the file to HDFS will continue.</p><h4 id="what-happens-with-failure-blocks">What happens with failure blocks?</h4><p><img src="./images/week_8.png" width="400"> Datanode serves a state machine for each block. Whenever a datanode recovers from its own failure, or failures of other datanodes in a pipeline, you can be sure that all the necessary replicas will be recovered. And unnecessary ones will be removed.</p><h1 id="block-and-replica-states">Block and Replica States</h1><p><strong>Replica</strong> : a physical data storage on a data node.There are usually several replicas with the same content on different data nodes.</p><p><strong>Block</strong>: a meta-information storage on a name node and provides information about replica's locations and their states.</p><p>Both replica and block have their own states. <img src="./images/week_9.png" width="250"></p><p><strong>Data node replica's states</strong>: Finalized, Replica Being Written to, Replica Under Recovery, Replica Waiting to be Recovered, Temporary</p><p><strong>Name node replica's states</strong>:</p><p><strong>Difference of Datanode &amp; Namenode</strong>: a block state is stored in memory, it doesn't persist on any disk.</p><h2 id="datanode-state-finalized">Datanode State: Finalized</h2><p><img src="./images/week_11.png" width="450"></p><p><strong>Finalized state</strong>: the content of this replica is frozen - Meaning: meta-information for this block on name node is aligned with all the corresponding replica's states and data.</p><ul><li><p><strong>Read consistency</strong>: you can safely read data from any data node and you will get exactly the same content.</p></li><li><strong>Generation Stamp(GS)</strong>: Each block of data has a version number called Generation Stamp. All of finalized replicas have the same GS number which can only increase over time. <img src="./images/week_10.png" width="300"></li><li><p>It happens during error <em>recovery process</em> or during <em>data appending to a block</em>.</p></li></ul><h2 id="datanode-state-replica-being-written">Datanode State: Replica Being Written</h2><p><img src="./images/week_12.png" width="450"></p><p><strong>RBW</strong>:the state of the last block of an open file or a file which was reopened for appending.</p><ul><li>Different data nodes can return to use a different set of bytes. In short, bytes that are acknowledged by the downstream data nodes in a pipeline are visible for a reader of this replica.</li><li>Data node on disk data and name node meta-information may not match during this state.</li><li><strong>Data Durability</strong>: In case of any failure data node will try to preserve as many bytes as possible.</li></ul><h2 id="datanode-state-replica-waiting-to-be-recovered">Datanode State: Replica Waiting to be Recovered</h2><p><img src="./images/week_13.png" width="450"></p><p><strong>RWR</strong>: a state of all Being Written replicas after data node failure and recovery after a system reboot or after Pacer.sys or BSOD,</p><ul><li>RWR replicas will not be in any data node pipeline and therefore will not receive any new data packets.</li><li>RWR either become <strong><em>outdated and should be discarded</em></strong>, or they will participate in a special recovery process called a <strong><em>lease recovery</em></strong> if the client also dies.</li></ul><p>HDFS client requests a <em>lease</em> from a name node to have an exclusive access to write or append data to a file. In case of HDFS client lease expiration, replica transition to a RUR state.</p><h2 id="datanode-state-replica-under-recovery">Datanode State: Replica Under Recovery</h2><p><img src="./images/week_14.png" width="450"></p><p><strong>RUR</strong> (Replica Under Recovery): HDFS client requests a <strong><em>lease</em></strong> from a name node to have an exclusive access to write or append data to a file. In case of HDFS client <strong><em>lease expiration</em></strong>(usually happens during the client's site failure), replica transition to a RUR state.</p><h2 id="datanode-state-temporary">Datanode State: Temporary</h2><p><img src="./images/week_15.png" width="450"></p><p><strong>Temporary</strong>: As data grows and different nodes are added or removed from a cluster, data can become unevenly distributed over the cluster nodes. A Hadoop administrator can spawn a process of data re-balancing or a data engineer can request increasing of the replication factor of data for the sake of durability. In these cases new generated replicas will be in a state called temporary.</p><ul><li>Similar to RBW except the fact that this data is not visible to user unless finalized.</li><li>In case of failure, the whole chunk of data is removed without any intermediate recovery state.</li></ul><h2 id="namenode-state-under-construction">Namenode State: Under Construction</h2><p><img src="./images/week_16.png" width="450"></p><p><strong>Under Construction</strong>: opens a file for writing, name node creates the corresponding block with the <em>under_construction</em> state; opens a file for append name node also transition this block to the state <em>under_construction</em>.</p><ul><li>always the last block of a file</li><li>it's length and generation stamp are mutable</li></ul><h2 id="namenode-state-under-recovery">Namenode State: Under Recovery</h2><p><img src="./images/week_17.png" width="450"></p><p>Name node block keeps track of write pipeline. It means that it contains information about all RBW and RWR replicas. Replicas transitions from RWR to recovery RUR state when the client dies. Even more generally it happens when a client's lease expires. Consequently, the corresponding block transitions from under_construction to under_recovery state. <img src="./images/week_18.png" width="450"></p><h2 id="namenode-state-committed">Namenode State: Committed</h2><p><img src="./images/week_19.png" width="450"> The under_construction block transitions to a committed state when a client successfully requests name node to close a file or to create a new consecutive block of data.</p><p>The committed state means that there are already some finalized replicas but not all of them. For this reason in order to serve a read request, the committed block needs to keep track of RBW replicas, until all the replicas are transitioned to the finalized state and HDFS client will be able to close the file.</p><h2 id="namenode-state-final-complete">Namenode State: Final Complete</h2><p><img src="./images/week_20.png" width="500"></p><p><strong>Final complete state</strong> of a block: a state where all the replicas are in the finalized state and therefore they have identical visible length and generation stamps.</p><ul><li>Only when all the blocks of a file are complete the file can be closed.</li></ul><h2 id="namenode-state-open-file">Namenode State: Open File</h2><p><strong>Open File state</strong>: In case of name node restart, it has to restore the open file state. All the blocks of the un-closed file are loaded as complete except the last block which is loaded as under_construction.</p><p>Then recovery procedures will start to work.</p><p><strong>Recovery</strong>: replica recovery, block recovery, lease recovery, and pipeline recovery.</p><h1 id="recovery-process">Recovery Process</h1><h2 id="block-recovery">Block Recovery</h2><p><strong>Goal</strong>: NameNode has to ensure that all of the corresponding replicas of a block will transition to a common state logically and physically.</p><p><strong>physically</strong>: all the correspondent replicas should have the same on disk content.</p><p>To accomplish it,</p><ol type="1"><li><p><strong>primary datanode(PD)</strong>: NameNode chooses a primary datanode called PD in a design document. PD should contain a replica for the target block. <img src="./images/week_21.png" width="400"></p></li><li><p>PD request from a NameNode, a new generation stamp, information and location of other replicas for recovery process.</p></li></ol><p><img src="./images/week_22.png" width="400"></p><ol start="3" type="1"><li>PD connects each relevant DataNodes to participate in the <strong>replica recovery process</strong>.During this phase, all the necessary information or data is propagated through the pipeline.</li></ol><p><strong>Replica recover process</strong> includes: - Aborting active clients right into a replica. - Aborting the previous replica of block recovery process, and participating in final replica size agreement process. <img src="./images/week_23.png" width="400"></p><ol start="4" type="1"><li>As the last step, PD notifies NameNode about the result, success or failure. In case of failure, NameNode could retry block recovery process. <img src="./images/week_24.png" width="400"></li></ol><h2 id="lease-recovery">Lease Recovery</h2><blockquote><p>Block recovery process could happen only as a part of the lease recovery process.</p></blockquote><p>Lease manager manages all the leases at the NameNode. HDFS clients request at least every time they would like to write, or append to a file. <img src="./images/week_25.png" width="400"></p><h3 id="conditions-of-starting-lease-recovery-process">Conditions of starting lease recovery process</h3><p>Lease manager maintains a soft and a hard limit. If a current lease holder doesn't renew his lease during the soft limit timeout, then another client will be able to take over this lease. In this case and in the case of reaching a hard limit, the process of lease recovery will begin. <img src="./images/week_26.png" width="400"> <img src="./images/week_28.png" width="400"></p><p><strong>Necessity:</strong> to close open files for the sake of the client.</p><p><strong>Gurantees to be reached</strong>:</p><ul><li><p><strong>concurrency control</strong>: Even if a client is still alive, it won't be able to write data to a file</p></li><li><p><strong>consistency guarantee</strong>: All replicas should draw back to a consistence state to have the same on-disk data and generation stamp.</p></li></ul><p><img src="./images/week_29.png" width="400"></p><h3 id="lease-recovery-process">Lease recovery process</h3><p><img src="./images/week_30.png" width="400"></p><ol type="1"><li>Lease recovery starts with a <strong>lease renew</strong>.</li></ol><p>New files lease holder should have the ability to take ownership of any other user's lease. The name of the super user is DFS. Therefore, all the other client request such as get new generation stamp, get new block, close file from other clients to this pass will be rejected.</p><ol start="2" type="1"><li><p>NameNode gets the lease of DataNodes which contains the last block of a file, and sends a primary DataNode and starts a block recovery process. <img src="./images/week_30.png" width="400"></p></li><li><p>As soon as block recovery process finishes, the NameNode is notified by PD about the outcome. Updates blocking for, and removes the lease for a file.</p></li></ol><p><img src="./images/week_31.png" width="400"></p><h2 id="pipeline-recovery">Pipeline Recovery</h2><h3 id="pipeline">Pipeline</h3><p>When you write to an HDFS file, HDFS client writes data block by block. Each block is constructed through a <strong>write pipeline</strong>, as the first client breaks down block into pieces called <strong>packets</strong>. These packets are propagated to the DataNodes through the pipeline.</p><blockquote><p>Three stages: Pipeline setup, data streaming, and close</p></blockquote><ul><li>bold lines: data packets</li><li>doted lines: acknowledge messages</li><li>regular lines: control messages.</li></ul><p><img src="./images/week_32.png" width="400"></p><h4 id="setup">Setup</h4><p>a clients sends a setup message down to the pipeline. Each DataNode opens a replica for writing and sends ack message back upstream with the pipeline. <img src="./images/week_33.png" width="400"></p><h4 id="data-streaming">Data streaming</h4><p>Data streaming stage is defined by time range from t1 to t2, where t1 is the time when a client to receives the acknowledgement message for the top stage. And t2 is the time when the client receives the acknowledgement message for all the block packets. <img src="./images/week_34.png" width="400"></p><ul><li><p>data is buffered on the client site to form a packet, then propagated through the DataNode pipeline.</p></li><li><p>Next packet can be sent even before the acknowledgment of the previous packet is received.</p></li></ul><p><strong>flush</strong>: synchronous packets and used as synchronization points for the DataNode right.</p><p><img src="./images/week_35.png" width="400"></p><h4 id="close">Close</h4><p>Finalize replicas and shut down the pipeline.</p><p><img src="./images/week_36.png" width="400"></p><ul><li>All of the DataNodes in the pipeline change the replica state to the finalized.</li><li>Report the state to a NameNode and send the acknowledgement message upstream.</li></ul><h3 id="pipeline-recovery-process">Pipeline recovery process</h3><p>Pipeline recovery can be initiated during each of these stages.</p><h4 id="setup-failure">Setup failure</h4><p>A failure happens during writing to a new file, abandon DataNode pipeline and request a new one from scratch <img src="./images/week_37.png" width="400"></p><p>In this case, some packets can be resent but they will not be extra disk IO overhead for DataNodes that already saved this packet on disk. Once the client detects a failure during close stage, it rebuilds a pipeline with good DataNodes. Bumps generation stamp and requests to finalize replicas.</p><h4 id="data-streaming-failure">Data streaming failure</h4><p>If DataNode is not able to continue process packets appropriately,then it allots the DataNode pipeline about it, by closing all the connections.</p><p>When HDFS client detects a fire, it stops sending new packets to the existing pipeline, request a new generation stamp from a NameNode, and rebuilds a pipeline from good DataNodes. &gt; In this case, some packets can be resent but there will not be extra disk IO overhead for DataNodes that already saved this packet on disk.</p><p>All DataNodes keep track of bytes received, bytes written to a disk and bytes acknowledged. <img src="./images/week_38.png" width="400"></p><h4 id="close-failure">Close failure</h4><p>Once the client detects a failure during close stage, it rebuilds a pipeline with good DataNodes. Bumps generation stamp and requests to finalize replicas. <img src="./images/week_39.png" width="400"></p><h1 id="hdfs-client">HDFS Client</h1><h1 id="namenode-architecture">Namenode Architecture</h1><p><strong>NameNode</strong>: a service responsible for keeping hierarchy of folders and files.</p><ul><li>NameNode stores all of this data in memory.</li></ul><p><img src="./images/week40.png" width="300"></p><h2 id="capacity">Capacity</h2><p>For example: 10 petabytes of data.</p><p><strong>Capacity for data nodes:</strong> - In HDFS, all data is usually stored with replication factor three. &gt; So, you can request to buy approximately 15,000 of two terabyte hard drives.</p><ul><li>On average, 15 hard drives will die every day. &gt;So, you should request at least 1000 extra hard drives for several months of research.</li></ul><p><strong>Capacity for meta information in memory:</strong></p><ul><li><p><strong>Small files problem:</strong> Storing lot of small files which are extremely smaller than the block size cannot be efficiently handled by HDFS. Reading through small files involve lots of seeks and lots of hopping between data node to data node, which is inturn inefficient data processing.</p></li><li><p><strong>128 megabyte</strong> of data was once chosen as a default block size.</p></li></ul><blockquote><p>WHY? It is one choice to have less than one percent overhead for reading the random block of data from a hard drive, and keeping block size small at the same time.</p></blockquote><p><img src="./images/week_41.png" width="400"></p><p>10 petabytes data will consume at least 35 gigabyte of RAM on the NameNode.</p><h2 id="failure">Failure</h2><p><strong>NameNode server is a single point of failure.</strong> &gt; In case this service goes down, the whole HDFS storage becomes unavailable, even for read-only operations.</p><p>Technical tricks to make NameNode decisions durable and to speed up NameNode recovery process: - write-ahead log (WAL): strategy to persist matter information modifications. This log is called the edit log. It can be replicated to different hard drives. It is also usually replicated to an NFS storage.</p><ul><li><p>NFS storage:you will be able to tolerate full NameNode crash</p></li><li><p><strong>fsimage</strong>: have a snapshot of memory at some point in time from which you can replay transaction stored in the edit log.</p></li></ul><p><img src="./images/week41.png" width="400"></p><p><strong>secondary NameNode</strong></p><blockquote><p>tackle the problem that edit log grows fast, replay of one week of transactions from edit log will take several hours to boot a NameNode</p></blockquote><p>Secondary NameNode, or better to say, checkpoint NameNodes, compacts the edit log by creating a new fsimage.</p><p>New fsimage is made of all the fsimage by applying all stored transactions in edit log.</p><ul><li>secondary NameNode consumes the same amount of RAM to build a new fsimage.</li><li>secondary NameNode was considered a badly named service.</li><li>secondary NameNode <span class="math inline">\(\neq\)</span> backup NameNode</li></ul><p><img src="./images/week42.png" width="400"></p><h2 id="evaluate">Evaluate</h2><p>Evaluate how long it takes to read 10 petabytes of data from a hard drive with similar reading speed:</p><p><img src="./images/week43.png" width="300"></p><p>The amount of drives in your cluster has a linear relation to the speed of data processing</p><h2 id="summary">Summary</h2><ol type="1"><li>explain and reason about HDFS Namenode architecture: (RAM; fsimage + edit log; block size)</li><li>estimate required resources for a Hadoop cluster</li><li>explain what small files problem is and where a bottleneck is</li><li>list differences between different types of Namenodes (Secondary / Checkpoint / Backup</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;section&quot;&gt;&lt;/h1&gt;
&lt;h1 id=&quot;scaling-dfs&quot;&gt;Scaling DFS&lt;/h1&gt;
&lt;h2 id=&quot;big-data-storage&quot;&gt;Big data storage:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Scale up (vertical sc
      
    
    </summary>
    
      <category term="Big Data" scheme="https://nancyyanyu.github.io/categories/Big-Data/"/>
    
    
      <category term="HDFS" scheme="https://nancyyanyu.github.io/tags/HDFS/"/>
    
      <category term="Model Selection" scheme="https://nancyyanyu.github.io/tags/Model-Selection/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning Basic Questions</title>
    <link href="https://nancyyanyu.github.io/undefined/b89c7c35/"/>
    <id>https://nancyyanyu.github.io/undefined/b89c7c35/</id>
    <published>2019-06-17T22:49:31.000Z</published>
    <updated>2019-06-18T21:46:20.896Z</updated>
    
    <content type="html"><![CDATA[<h3 id="can-you-state-tom-mitchells-definition-of-learning-and-discuss-t-p-and-e">1. Can you state Tom Mitchell's definition of learning and discuss T, P and E?</h3><p>Mitchell (1997) provides the definition “A computer program is said to learn from <strong>experience E</strong> with respect to some class of <strong>tasks T</strong> and <strong>performance measure P</strong>, if its performance at tasks in <strong>T</strong>, as measured by <strong>P</strong>, improves with experience <strong>E</strong>.</p><a id="more"></a><h3 id="what-can-be-different-types-of-tasks-encountered-in-machine-learning">2. What can be different types of tasks encountered in Machine Learning?</h3><p>Classification, regression, Machine translation, Anomaly detection, Density estimation or probability mass function estimation</p><h3 id="consider-linear-regression.-what-are-t-p-and-e">3. Consider linear regression. What are T, P and E?</h3><p>Task T : to predict y from <span class="math inline">\(x\)</span> by outputting <span class="math inline">\(\hat{y} = \mathbf{w}^T\mathbf{x}\)</span>.</p><p>Performance P: compute the mean squared error of the model on the test set. <span class="math display">\[MSE_{test}=\frac{1}{m}||\hat{y}^{test}-y^{test}||^2_2\]</span> Experience E: training set <span class="math inline">\((X^{train}, y^{train})\)</span>.</p><h3 id="what-are-supervised-unsupervised-semi-supervised-self-supervised-multi-instance-learning-and-reinforcement-learning">4. What are supervised, unsupervised, semi-supervised, self-supervised, multi-instance learning, and reinforcement learning?</h3><p><strong><em>Supervised learning:</em></strong> Training a model from input data and its corresponding labels.</p><p><strong><em>Unsupervised learning:</em></strong> Training a model to find patterns in a dataset, typically an unlabeled dataset.</p><p><strong><em>Semi-supervised learning:</em></strong> Training a model on data where some of the training examples have labels but others don’t. One technique for semi-supervised learning is to infer labels for the unlabeled examples, and then to train on the inferred labels to create a new model. Semi-supervised learning can be useful if labels are expensive to obtain but unlabeled examples are plentiful.</p><p><strong><em>Self-supervised learning:</em></strong> a relatively recent learning technique (in machine learning) where the <strong>training data is automatically labelled</strong>. It is still supervised learning, but the datasets do not need to be manually labelled by human, but they can e.g. be labelled by finding and exploiting the relations (or correlations) between different input signals (input coming e.g. from different sensor modalities).</p><p><strong><em>Multi-instance learning:</em></strong> a type of supervised learning. Instead of receiving a set of instances which are individually labeled, <strong>the learner receives a set of labeled <em>bags</em>, each containing many instances.</strong> In the simple case of multiple-instance <em>binary classification</em>, a bag may be labeled negative if all the instances in it are negative. On the other hand, a bag is labeled positive if there is at least one instance in it which is positive. From a collection of labeled bags, the learner tries to either (i) induce a concept that will label individual instances correctly or (ii) learn how to label bags without inducing the concept.</p><p><strong><em>Reinforcement learning:</em></strong> A machine learning approach to <strong>maximize an ultimate reward</strong> through feedback (rewards and punishments) after a sequence of actions. For example, the ultimate reward of most games is victory. Reinforcement learning systems can become expert at playing complex games by evaluating sequences of previous game moves that ultimately led to wins and sequences that ultimately led to losses.</p><p><strong><em>Reinforcement learning</em></strong> is learning what to do---how to map situations to actions---so as to maximize a numerical reward signal</p><h3 id="prove-that-for-linear-regression-mse-can-be-derived-from-maximal-likelihood-by-proper-assumptions.">5. Prove that for linear regression MSE can be derived from maximal likelihood by proper assumptions.</h3><p><strong>Probabilistic assumption</strong>:</p><ul><li>Assume that the target variables and the inputs are related via the equation:</li></ul><p><span class="math display">\[y^{(i)}=\theta^Tx^{(i)}+\epsilon^{(i)}\]</span></p><p>where <span class="math inline">\(\epsilon^{(i)}\)</span> is an error term that captures either unmodeled effects (such as if there are some features very pertinent to predicting housing price, but that we’d left out of the regression), or random noise.</p><ul><li><p>Assume <span class="math inline">\(\epsilon^{(i)}\)</span> are distributed IID (independently and identically distributed) according to a Gaussian distribution (also called a Normal distribution) mean zero and some variance <span class="math inline">\(\sigma^2\)</span></p><ul><li><p>The density of <span class="math inline">\(\epsilon^{(i)}\)</span> is: <span class="math display">\[p(\epsilon^{(i)})=\frac{1}{\sqrt{2\pi}\sigma}\exp \left(-\frac{(\epsilon^{(i)})^2}{2\sigma^2}\right)\]</span></p></li><li><p>This implies that:</p></li></ul><p><span class="math display">\[p(y^{(i)}|x^{(i)};\theta)=\frac{1}{\sqrt{2\pi}\sigma}\exp \left(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\right)\]</span></p></li></ul><p><strong>Likelihood function</strong>: <span class="math display">\[L(\theta)=L(\theta|\mathbf{X},\mathbf{y})=p(\mathbf{y}|\mathbf{X};\theta)\]</span> <span class="math inline">\(p(\mathbf{y}|\mathbf{X};\theta)\)</span>: This quantity is typically viewed a function of <span class="math inline">\(\mathbf{y}\)</span> (and perhaps X), for a fixed value of θ.</p><p>By the independence assumption on the <span class="math inline">\(\epsilon^{(i)}\)</span>’s (and hence also the <span class="math inline">\(y^{(i)}\)</span>’s given the <span class="math inline">\(x^{(i)}\)</span> ’s), this can also be written: <span class="math display">\[\begin{align}L(\theta)&amp;= \prod_{i=1}^n p(y^{(i)}|x^{(i)};\theta) \\&amp;=\prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma}\exp \left(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\right)\end{align}\]</span> The principal of <strong><em>maximum likelihood</em></strong> says that we should choose <span class="math inline">\(θ\)</span> so as to make the data as high probability as possible <span class="math inline">\(\rightarrow\)</span> maximize <span class="math inline">\(L(θ)\)</span>.</p><p>Instead of maximizing <span class="math inline">\(L(θ)\)</span>, we can also maximize any strictly increasing function of $L(θ) $ <span class="math inline">\(\rightarrow\)</span> <strong>log likelihood</strong> <span class="math inline">\(ℓ(θ)\)</span>: <span class="math display">\[\begin{align}ℓ(θ)=\log L(\theta)&amp;=\log \prod_{i=1}^n p(y^{(i)}|x^{(i)};\theta) \\&amp;=\sum_{i=1}^n \log \frac{1}{\sqrt{2\pi}\sigma}\exp \left(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\right) \\&amp;= n\log\frac{1}{\sqrt{2\pi}\sigma}-\frac{1}{2\sigma^2} \sum_{i=1}^n (y^{(i)}-\theta^Tx^{(i)})^2\end{align}\]</span> Hense, maximizing <span class="math inline">\(ℓ(θ)\)</span> gives the same answer as minimizing <span class="math display">\[\frac{1}{2}\sum_{i=1}^n(h_\theta(x^{(i)})-y^{(i)})^2 =J(\theta)\]</span> To summarize: Under the previous probabilistic assumptions on the data, least-squares regression corresponds to finding the maximum likelihood estimate of θ. Note also that, in our previous discussion, our final choice of θ did not depend on what was <span class="math inline">\(\sigma^2\)</span> , and indeed we’d have arrived at the same result even if <span class="math inline">\(\sigma^2\)</span> were unknown.</p><h3 id="derive-the-normal-equation-for-linear-regression.">6. Derive the normal equation for linear regression.</h3><p>Linear function: <span class="math display">\[h_θ(x) = θ_0 + θ_1x_1 + θ_2x_2=θ^Tx\]</span> Least-squares cost function: <span class="math display">\[J(\theta)=\frac{1}{2}\sum_{i=1}^n(h_\theta(x^{(i)})-y^{(i)})^2\]</span></p><p><span class="math display">\[\begin{align}\mathbf{X}&amp;=\begin{bmatrix}- (x^{(1)})^T -  \\- (x^{(2)})^T - \\ ...\\- (x^{(n)})^T -\end{bmatrix} \\\mathbf{y}&amp;=\begin{bmatrix} y^{(1)} \\y^{(2)} \\... \\y^{(n)}\end{bmatrix}   \\\mathbf{X}\theta-\mathbf{y}&amp;=\begin{bmatrix} (x^{(1)})^T\theta-y^{(1)} \\(x^{(2)})^T\theta-y^{(2)} \\... \\(x^{(n)})^T\theta-y^{(n)}\end{bmatrix} \\&amp;=\begin{bmatrix} (h_\theta(x^{(1)})-y^{(1)} \\h_\theta(x^{(2)})-y^{(2)} \\... \\h_\theta(x^{(1)})-y^{(n)}\end{bmatrix}\end{align}\]</span> For a vector <span class="math inline">\(z\)</span>, we have that: <span class="math inline">\(z^Tz=\sum_i z^2_i\)</span> <span class="math display">\[\frac{1}{2}(\mathbf{X}\theta-\mathbf{y})^T(\mathbf{X}\theta-\mathbf{y})=\frac{1}{2}\sum_{i=1}^n(h_\theta(x^{(i)})-y^{(i)})^2 =J(\theta)\]</span> We know that: <span class="math display">\[\begin{align}\frac{\partial f(A)}{\partial A^T}&amp;=(\frac{\partial f(A)}{\partial A})^T \\\frac{\partial \mathbf{y}^T\mathbf{A}\mathbf{x}}{\partial \mathbf{x}}&amp;=\mathbf{y}^T\mathbf{A} \\\frac{\partial \mathbf{y}^T\mathbf{A}\mathbf{x}}{\partial \mathbf{y}}&amp;=\frac{\partial \mathbf{x}^T\mathbf{A}^T\mathbf{y}}{\partial \mathbf{y}}=\mathbf{x}^T\mathbf{A}^T \\\frac{\partial \mathbf{x}^T\mathbf{A}\mathbf{x}}{\partial \mathbf{x}}&amp;=\mathbf{x}^T\mathbf{A}^T +\mathbf{x}^T\mathbf{A}=\mathbf{x}^T（\mathbf{A}^T +\mathbf{A}）\\\end{align}\]</span> To minimize <span class="math inline">\(J\)</span>, let’s find its derivatives with respect to <span class="math inline">\(θ\)</span>: <span class="math display">\[\begin{align}\frac{\partial J(\theta)}{\partial \theta}&amp;= \frac{\partial \frac{1}{2}(\mathbf{X}\theta-\mathbf{y})^T(\mathbf{X}\theta-\mathbf{y})}{\partial \theta}\\&amp;= \frac{1}{2}\frac{\partial (\theta^T\mathbf{X}^T\mathbf{X}\theta-\theta^T\mathbf{X}^T\mathbf{y}-\mathbf{y}^T\mathbf{X}\theta+\mathbf{y}^T\mathbf{y})}{\partial \theta} \\&amp;=\frac{1}{2}\frac{\partial (\theta^T\mathbf{X}^T\mathbf{X}\theta-\theta^T\mathbf{X}^T\mathbf{y}-\mathbf{y}^T\mathbf{X}\theta+\mathbf{y}^T\mathbf{y})}{\partial \theta} \\&amp;=\frac{1}{2} (\theta^T\mathbf{X}^T\mathbf{X}+\theta^T\mathbf{X}^T\mathbf{X}-\mathbf{y}^T\mathbf{X}-\mathbf{y}^T\mathbf{X}  )\\&amp;=\frac{1}{2}(\mathbf{X}^T\mathbf{X}\theta-2\mathbf{y}^T\mathbf{X}) \\&amp;=\mathbf{X}^T\mathbf{X}\theta-\mathbf{X}^T\mathbf{y}=0\end{align}\]</span> <strong><em>Normal Equation</em></strong>: <span class="math display">\[\theta=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\]</span></p><h3 id="why-is-a-validation-set-necessary">7. Why is a validation set necessary?</h3><p>Let's assume that you are training a model whose performance depends on a set of hyperparameters. In the case of a neural network, these parameters may be for instance the learning rate or the number of training iterations.</p><p>Given a choice of hyperparameter values, you use the <strong>training</strong> set to train the model. But, how do you set the values for the hyperparameters? That's what the <strong>validation</strong> set is for. You can use it to evaluate the performance of your model for different combinations of hyperparameter values (e.g. by means of a grid search process) and keep the best trained model.</p><p>But, how does your selected model compares to other different models? Is your neural network performing better than, let's say, a random forest trained with the same combination of training/test data? You cannot compare based on the validation set, because that validation set was part of the fitting of your model. You used it to select the hyperparameter values!</p><p>The <strong>test</strong> set allows you to compare different models in an unbiased way, by basing your comparisons in data that were not use in any part of your training/hyperparameter selection process.</p><h3 id="what-is-the-no-free-lunch-theorem-in-connection-to-machine-learning">8. What is the no free lunch theorem in connection to Machine Learning?</h3><p>The <strong><em>no free lunch theorem</em></strong> for machine learning (Wolpert, 1996) states that, <strong>averaged over</strong> <strong>all possible data generating distributions, every classification algorithm has the</strong> <strong>same error rate when classifying previously unobserved points</strong>. In other words, in some sense, <strong>no machine learning algorithm is universally any better than any</strong> <strong>other.</strong> The most sophisticated algorithm we can conceive of has the same average performance (over all possible tasks) as merely predicting that every point belongs to the same class.</p><p>Fortunately, these results hold only when we average over all possible data generating distributions. **If we make assumptions about the kinds of probability<em> distributions we encounter in real-world applications,</em> then we can design learning algorithms that perform well on these distributions.</p><p>This means that the goal of machine learning research is not to seek a universal learning algorithm or the absolute best learning algorithm. Instead, <em>our goal is to</em> <em>understand what kinds of distributions are relevant to the “real world” that an AI</em> <em>agent experiences,</em> and what kinds of machine learning algorithms perform well on data drawn from the kinds of data generating distributions we care about.</p><h3 id="discuss-training-error-test-error-generalization-error-overfitting-and-underfitting.">9. Discuss training error, test error, generalization error, overfitting, and underfitting.</h3><p><strong><em>overfitting:</em></strong> the gap between the training error and test error is too large</p><p><strong><em>underfitting:</em></strong> the model is not able to obtain a sufficiently low error value on the training set</p><p><strong><em>training error:</em></strong> when training a machine learning model, we have access to a training set, we can compute some error measure on the training set</p><p><strong><em>generalization error/test error:</em></strong> the expected value of the error on a new input. Here the expectation is taken across different possible inputs, drawn from the distribution of inputs we expect the system to encounter in practice</p><p><img src="./1.png" width="600"></p><h3 id="compare-representational-capacity-vs.-effective-capacity-of-a-model.">10. Compare representational capacity vs. effective capacity of a model.</h3><ul><li><strong>Representational capacity</strong> - the functions which the model <em>can</em> learn; The model specifies which <strong>family of functions</strong> the learning algorithm can choose from when varying the parameters in order to reduce a training objective.</li><li><strong>Effective capacity</strong> - in practice, a learning algorithm is not likely to find the <em>best</em> function out of the possible functions it can learn, though it can learn one that performs exceptionally well - those functions that the learning algorithm is capable of finding defines the model's <em>effective</em> capacity.</li></ul><p>These additional limitations, such as the imperfection of the optimization algorithm, mean that the learning algorithm’s <strong><em>effective capacity</em></strong> may be less than the <strong><em>representational capacity</em></strong> of the model family</p><h3 id="what-is-an-ideal-model-what-is-bayes-error-what-isare-the-sources-of-bayes-error-occur">11. What is an ideal model? What is Bayes error? What is/are the source(s) of Bayes error occur?</h3><p><strong><em>The ideal model</em></strong>: is an oracle that simply knows the true probability distribution that generates the data.</p><ul><li>Even such a model will still incur some error on many problems, because there may still be some noise in the distribution. In the case of supervised learning, the mapping from <span class="math inline">\(x\)</span> to <span class="math inline">\(y\)</span> may be inherently stochastic, or <span class="math inline">\(y\)</span> may be a deterministic function that involves other variables besides those included in <span class="math inline">\(x\)</span>.</li></ul><p><strong><em>Bayes error</em></strong>: the lowest possible prediction error that can be achieved and is the same as irreducible error. ; The error incurred by an oracle making predictions from the true distribution p(x, y).</p><p><strong><em>Source(s) of Bayes error occur</em></strong>: noise in the distribution if the process is random</p><h3 id="what-are-nonparametric-models-what-is-nonparametric-learning">12. What are nonparametric models? What is nonparametric learning?</h3><p>Parametric models: learn a function described by a parameter vector whose size is finite and fixed before any data is observed (linear regression)</p><p>Non-parametric models: assume that the data distribution cannot be defined in terms of a finite set of parameters. But they can often be defined by assuming an infinite dimensional <span class="math inline">\(\theta\)</span> . Usually we think of <span class="math inline">\(\theta\)</span> as a function (nearest neighbor regression)</p><h3 id="what-is-regularization-intuitively-what-does-regularization-do-during-the-optimization-procedure">13. What is regularization? Intuitively, what does regularization do during the optimization procedure?</h3><p><strong><em>Regularization</em></strong> is any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error.</p><p>We regularize a model that learns a function <span class="math inline">\(f(x; θ)\)</span> by adding a penalty called a <strong>regularizer</strong> to the cost function. <strong>Expressing preferences for one function over another</strong> implicitly and explicitly is a more general way of controlling a model’s capacity than including or excluding members from the hypothesis space.</p><h3 id="what-is-weight-decay-what-is-it-added">14. What is weight decay? What is it added?</h3><p><strong><em>Weight decay</em></strong> is an additional term that causes the weights to exponentially decay to zero.</p><p>To perform linear regression with <strong>weight decay</strong>, we minimize a sum comprising both the mean squared error on the training and a criterion <span class="math inline">\(J (w)\)</span> that expresses a preference for the weights to have smaller squared L2 norm. Specifically, <span class="math display">\[J(w) = MSE_{train} + λ\mathbf{w}^T\mathbf{w}\]</span> Minimizing <span class="math inline">\(J(w)\)</span> results in a choice of weights that make a tradeoff between fitting the training data and being small. This gives us solutions that have a smaller slope, or put weight on fewer of the features.</p><h3 id="what-is-a-hyperparameter-how-do-you-choose-which-settings-are-going-to-be-hyperparameters-and-which-are-going-to-be-learnt">15. What is a hyperparameter? How do you choose which settings are going to be hyperparameters and which are going to be learnt?</h3><p><strong>Hyperparameter</strong>: Most machine learning algorithms have several settings that we can use to control the behavior of the learning algorithm.</p><ul><li>The values of hyperparameters are not adapted by the learning algorithm itself</li></ul><p>Sometimes a setting is chosen to be a hyperparameter that the learning algorithm does not learn because it is <strong>difficult to optimize</strong> or it is not appropriate to learn that hyperparameter on the training set. This applies to all hyperparameters that control model capacity. If learned on the training set, such hyperparameters would always choose the maximum possible model capacity, resulting in <strong>overfitting</strong></p><h3 id="why-is-maximal-likelihood-the-preferred-estimator-in-ml">16. Why is maximal likelihood the preferred estimator in ML?</h3><p>The main appeal of the maximum likelihood estimator is that it can be shown to be the best estimator asymptotically, as the number of examples m → ∞, in terms of its rate of convergence as m increases</p><p>Under appropriate conditions, the maximum likelihood estimator has the property of :</p><ul><li><strong>consistency</strong>: as the number of training examples approaches infinity, the maximum likelihood estimate of a parameter converges to the true value of the parameter. <span class="math inline">\(\hat{\theta} \rightarrow^{n \rightarrow \infin} \theta\)</span>.</li><li><strong>efficiency</strong>: the Cramér-Rao lower bound (Rao, 1945; Cramér, 1946) shows that no consistent estimator has a lower mean squared error <span class="math inline">\(Var(\hat{\theta}_n)\)</span> than the maximum likelihood estimator</li></ul><p>When the number of examples is small enough to yield overfitting behavior, regularization strategies such as weight decay may be used to obtain a biased version of maximum likelihood that has less variance when training data is limited.</p><h3 id="under-what-conditions-do-the-maximal-likelihood-estimator-guarantee-consistency">17. Under what conditions do the maximal likelihood estimator guarantee consistency?</h3><ol type="1"><li>The true distribution <span class="math inline">\(p_{data}\)</span> must lie within the model family <span class="math inline">\(p_{model}(·; θ)\)</span>. Otherwise, no estimator can recover <span class="math inline">\(p_{data}\)</span>.</li><li>The true distribution <span class="math inline">\(p_{data}\)</span> must correspond to exactly one value of <span class="math inline">\(θ\)</span>. Otherwise, maximum likelihood can recover the correct <span class="math inline">\(p_{data}\)</span> , but will not be able to determine which value of <span class="math inline">\(θ\)</span> was used by the data generating processing.</li></ol><h3 id="what-do-you-mean-by-affine-transformation-discuss-affine-vs.-linear-transformation.">18. What do you mean by affine transformation? Discuss affine vs. linear transformation.</h3><p>A function 𝑓 is linear if <span class="math inline">\(𝑓(𝑎𝑥+𝑏𝑦)=𝑎𝑓(𝑥)+𝑏𝑓(𝑦)\)</span> for all relevant values of 𝑎, 𝑏, 𝑥 and 𝑦.</p><p>A function 𝑔 is affine if <span class="math inline">\(𝑔(𝑥)=𝑓(𝑥)+𝑐\)</span> for some linear function 𝑓 and constant 𝑐. Note that we allow 𝑐=0, which implies that every linear function is an affine function.</p><ol type="1"><li>All linear transformations are affine transformations.</li><li>Not all affine transformations are linear transformations.</li><li>It can be shown that any affine transformation 𝐴:𝑈→𝑉 can be written as 𝐴(𝑥)=𝐿(𝑥)+𝑣0, where 𝑣0 is some vector from 𝑉 and 𝐿:𝑈→𝑉 is a linear transformation.</li></ol><p>Discuss VC dimension.</p><p>The VC dimension measures the capacity of a binary classifier. The VC dimension is defined as being the largest possible value of m for which there exists a training set of m different x points that the classifier can label arbitrarily.</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;can-you-state-tom-mitchells-definition-of-learning-and-discuss-t-p-and-e&quot;&gt;1. Can you state Tom Mitchell&#39;s definition of learning and discuss T, P and E?&lt;/h3&gt;
&lt;p&gt;Mitchell (1997) provides the definition “A computer program is said to learn from &lt;strong&gt;experience E&lt;/strong&gt; with respect to some class of &lt;strong&gt;tasks T&lt;/strong&gt; and &lt;strong&gt;performance measure P&lt;/strong&gt;, if its performance at tasks in &lt;strong&gt;T&lt;/strong&gt;, as measured by &lt;strong&gt;P&lt;/strong&gt;, improves with experience &lt;strong&gt;E&lt;/strong&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Interview" scheme="https://nancyyanyu.github.io/tags/Interview/"/>
    
  </entry>
  
  <entry>
    <title>ESL Note: Model Averaging and Stacking</title>
    <link href="https://nancyyanyu.github.io/undefined/4f6e00ef/"/>
    <id>https://nancyyanyu.github.io/undefined/4f6e00ef/</id>
    <published>2019-06-17T21:21:28.000Z</published>
    <updated>2019-06-18T17:33:58.897Z</updated>
    
    <content type="html"><![CDATA[<h2 id="bayesian-model-averaging">Bayesian Model Averaging</h2><p>We have a set of candidate models <span class="math inline">\(M_m\)</span>; m = 1,…,M for our training set <span class="math inline">\(Z\)</span>.</p><p><strong>Suppose</strong> <span class="math inline">\(\zeta\)</span> is some quantity of interest, for example, a prediction f(x) at some fixed feature value <span class="math inline">\(x\)</span>. The <strong><em>posterior distribution</em></strong> of <span class="math inline">\(\zeta\)</span> is <span class="math display">\[\Pr(\zeta|\mathbf{Z})=\sum_{i=1}^M\Pr(\zeta|M_m,\mathbf{Z})\Pr(M_m| \mathbf{Z})\]</span> with <strong><em>posterior mean</em></strong>: <span class="math display">\[E(\zeta|\mathbf{Z})=\sum_{i=1}^ME(\zeta|M_m,\mathbf{Z})\Pr(M_m| \mathbf{Z})\]</span> This Bayesian prediction is a weighted average of the individual predictions, with weights proportional to the posterior probability of each model.</p><h3 id="frequentist-viewpoint">Frequentist Viewpoint</h3><p>Given predictions <span class="math inline">\(\hat{f}_1(x); \hat{f}_2(x),…, \hat{f}_M(x)\)</span>, under squared-error loss, we can seek the weights $w = (w_1, w_2,…, w_M) $ such that <span class="math display">\[\hat{w}=\arg \min_w E_P[Y-\sum_{i=1}^Mw_m\hat{f}_m(x)]^2\]</span> Here the input value <span class="math inline">\(x\)</span> is fixed and the <span class="math inline">\(N\)</span> observations in the dataset <span class="math inline">\(Z\)</span> (and the target <span class="math inline">\(Y\)</span> ) are distributed according to <span class="math inline">\(P\)</span>. The solution is the population linear regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(\hat{F}(x)^T=[\hat{f}_1(x); \hat{f}_2(x),…, \hat{f}_M(x)]\)</span> : <span class="math display">\[\hat{w}=E_P[\hat{F}(x)\hat{F}(x)^T]^{-1}E_P[\hat{F}(x)Y]\]</span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;bayesian-model-averaging&quot;&gt;Bayesian Model Averaging&lt;/h2&gt;
&lt;p&gt;We have a set of candidate models &lt;span class=&quot;math inline&quot;&gt;\(M_m\)&lt;/span
      
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Model Inference" scheme="https://nancyyanyu.github.io/tags/Model-Inference/"/>
    
  </entry>
  
  <entry>
    <title>Deep Learning Questions Part I: UAT, Motivation</title>
    <link href="https://nancyyanyu.github.io/undefined/c7bd9d66/"/>
    <id>https://nancyyanyu.github.io/undefined/c7bd9d66/</id>
    <published>2019-06-17T20:22:02.000Z</published>
    <updated>2019-06-18T21:46:15.099Z</updated>
    
    <content type="html"><![CDATA[<h2 id="universal-approximation-of-neural-networks">Universal Approximation of neural networks</h2><h3 id="state-the-universal-approximation-theorem-what-is-the-technique-used-to-prove-that">1. State the universal approximation theorem? What is the technique used to prove that?</h3><p><strong>Universal approximation theorem</strong> (Hornik et al., 1989; Cybenko, 1989) states that a feedforward network with a linear output layer and at least one hidden layer with any “squashing” activation function (such as the logistic sigmoid activation function) can approximate any Borel measurable function from one finite-dimensional space to another with any desired non-zero amount of error, provided that the network is given enough hidden units.</p><p>The universal approximation theorem means that <strong>regardless of what function we are trying to learn, we know that a large MLP will be able to represent this function.</strong></p><p>However, we are not guaranteed that the training algorithm will be able to learn that function. Even if the MLP is able to represent the function, learning can fail for two different reasons.</p><ol type="1"><li>The <strong>optimization algorithm</strong> used for training may not be able to find the value of the parameters that corresponds to the desired function.</li><li>The training algorithm might <strong>choose the wrong function due to overfitting</strong></li></ol><p>The universal approximation theorem says that there exists a network large enough to achieve any degree of accuracy we desire, but the theorem does not say how large this network will be.</p><h3 id="what-is-a-borel-measurable-function">2. What is a Borel measurable function?</h3><p>Any continuous function on a closed and bounded subset of <span class="math inline">\(R^n\)</span> is Borel measurable and therefore may be approximated by a neural network.</p><a id="more"></a><h2 id="deep-learning-motivation">Deep Learning motivation</h2><ol type="1"><li>What is the mathematical motivation of Deep Learning as opposed to standard Machine Learning techniques?</li><li>In standard Machine Learning vs. Deep Learning, how is the order of number of samples related to the order of regions that can be recognized in the function space?</li><li>What are the reasons for choosing a deep model as opposed to shallow model? (1. Number of regions O(2^k) vs O(k) where k is the number of training examples 2. # linear regions carved out in the function space depends exponentially on the depth. )</li><li>How Deep Learning tackles the curse of dimensionality?</li></ol><h2 id="general-questions">General questions</h2><ol type="1"><li>How will you implement dropout during forward and backward pass?</li><li>What do you do if Neural network training loss/testing loss stays constant? (ask if there could be an error in your code, going deeper, going simpler…)</li><li>Why do RNNs have a tendency to suffer from exploding/vanishing gradient? How to prevent this? (Talk about LSTM cell which helps the gradient from vanishing, but make sure you know why it does so. Talk about gradient clipping, and discuss whether to clip the gradient element wise, or clip the norm of the gradient.)</li><li>Do you know GAN, VAE, and memory augmented neural network? Can you talk about it?</li><li>Does using full batch means that the convergence is always better given unlimited power? (Beautiful explanation by Alex Seewald: https://www.quora.com/Is-full-batch-gradient-descent-with-unlimited-computer-power-always-better-than-mini-batch-gradient-descent)</li><li>What is the problem with sigmoid during backpropagation? (Very small, between 0.25 and zero.)</li><li>Given a black box machine learning algorithm that you can’t modify, how could you improve its error? (you can transform the input for example.)</li><li>How to find the best hyper parameters? (Random search, grid search, Bayesian search (and what it is?))</li><li>What is transfer learning?</li><li>Compare and contrast L1-loss vs. L2-loss and L1-regularization vs. L2-regularization.</li></ol><p><strong>Ref</strong>:</p><p><a href="https://github.com/Sroy20/machine-learning-interview-questions" target="_blank" rel="noopener">machine-learning-interview-questions</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;universal-approximation-of-neural-networks&quot;&gt;Universal Approximation of neural networks&lt;/h2&gt;
&lt;h3 id=&quot;state-the-universal-approximation-theorem-what-is-the-technique-used-to-prove-that&quot;&gt;1. State the universal approximation theorem? What is the technique used to prove that?&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Universal approximation theorem&lt;/strong&gt; (Hornik et al., 1989; Cybenko, 1989) states that a feedforward network with a linear output layer and at least one hidden layer with any “squashing” activation function (such as the logistic sigmoid activation function) can approximate any Borel measurable function from one finite-dimensional space to another with any desired non-zero amount of error, provided that the network is given enough hidden units.&lt;/p&gt;
&lt;p&gt;The universal approximation theorem means that &lt;strong&gt;regardless of what function we are trying to learn, we know that a large MLP will be able to represent this function.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;However, we are not guaranteed that the training algorithm will be able to learn that function. Even if the MLP is able to represent the function, learning can fail for two different reasons.&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;The &lt;strong&gt;optimization algorithm&lt;/strong&gt; used for training may not be able to find the value of the parameters that corresponds to the desired function.&lt;/li&gt;
&lt;li&gt;The training algorithm might &lt;strong&gt;choose the wrong function due to overfitting&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The universal approximation theorem says that there exists a network large enough to achieve any degree of accuracy we desire, but the theorem does not say how large this network will be.&lt;/p&gt;
&lt;h3 id=&quot;what-is-a-borel-measurable-function&quot;&gt;2. What is a Borel measurable function?&lt;/h3&gt;
&lt;p&gt;Any continuous function on a closed and bounded subset of &lt;span class=&quot;math inline&quot;&gt;\(R^n\)&lt;/span&gt; is Borel measurable and therefore may be approximated by a neural network.&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Interview" scheme="https://nancyyanyu.github.io/tags/Interview/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning Questions - Part IV: Clustering &amp; Bayesian</title>
    <link href="https://nancyyanyu.github.io/undefined/25b6d1fa/"/>
    <id>https://nancyyanyu.github.io/undefined/25b6d1fa/</id>
    <published>2019-06-17T20:22:00.000Z</published>
    <updated>2019-06-18T21:45:53.718Z</updated>
    
    <content type="html"><![CDATA[<h2 id="clustering">Clustering</h2><h3 id="describe-the-k-means-algorithm.">1. Describe the k-means algorithm.</h3><p><strong>K-means clustering</strong> is a simple and elegant approach for partitioning a data set into K distinct, <strong><em>non-overlapping</em></strong> clusters.</p><p>The idea behind <strong>K-means clustering</strong> is that a <em>good</em> clustering is one for which the <strong><em>within-cluster</em></strong> <strong><em>variation</em></strong> is as small as possible.</p><p>The <strong>within-cluster variation</strong> for cluster <span class="math inline">\(C_k\)</span> is a measure <span class="math inline">\(W(C_k)\)</span> of the amount by which the observations within a cluster differ from each other.</p><a id="more"></a><p><strong>Define the within-cluster variation</strong>: <strong><em>Euclidean distance</em></strong>: <span class="math display">\[W(C_k)=\frac{1}{|C_k|}\sum_{i,i^{&#39;}\in C_k}\sum_{j=1}^p(x_{ij}-x_{i^{&#39;}j})^2\]</span></p><ul><li>where <span class="math inline">\(|C_k|\)</span> denotes the number of observations in the kth cluster.</li><li>The within-cluster variation for the kth cluster is <em>the sum of all of the pairwise squared Euclidean distances between the observations in the kth cluster</em>, divided by the total number of observations in the kth cluster.</li></ul><p><strong>Objective funtion</strong>: <span class="math display">\[\min_{C_1,...,C_K}\left\{ \sum_{i=1}^K\frac{1}{|C_k|}\sum_{i,i^{&#39;}\in C_k}\sum_{j=1}^p(x_{ij}-x_{i^{&#39;}j})^2\right\}\]</span></p><ul><li>partition the observations into K clusters such that the total within-cluster variation, summed over all K clusters, is <em>as small as possible</em>.</li></ul><p><strong>Algorithm</strong></p><p><img src="./1.png" width="600"></p><p>In the algorithm above, <span class="math inline">\(k\)</span> (a parameter of the algorithm) is the number of clusters we want to find; and the cluster <strong>centroids</strong> <span class="math inline">\(\mu_j\)</span> represent our current guesses for the positions of the centers of the clusters. To initialize the cluster centroids, we could choose <span class="math inline">\(k\)</span> training examples randomly, and set the cluster centroids to be equal to the values of these k examples. (Other initialization methods are also possible.)</p><p>The inner-loop of the algorithm repeatedly carries out two steps:</p><ul><li>“Assigning” each training example <span class="math inline">\(x^{(i)}\)</span> to the closest cluster centroid <span class="math inline">\(\mu_j\)</span>, and</li><li>Moving each cluster centroid <span class="math inline">\(\mu_j\)</span> to the mean of the points assigned to it.</li></ul><p><strong>Local optimum</strong> : This means that as the algorithmis run, the clustering obtained will continually improve until the result no longer changes; the objective will never increase.</p><ul><li>It is important to run the algorithm multiple times from different random initial configurations, because the results obtained will depend on the initial (random) cluster assignmentof each observation in Step 1 of Algorithm 10.1</li></ul><h3 id="what-is-distortion-function-is-it-convex-or-non-convex">2. What is distortion function? Is it convex or non-convex?</h3><p><strong>Distortion function</strong>: <span class="math display">\[J(c,\mu)=\sum_{i=1}^n ||x^{(i)}-\mu_{c^{(i)}}||^2\]</span> <span class="math inline">\(J\)</span> measures the sum of squared distances between each training example <span class="math inline">\(x^{(i)}\)</span> and the cluster centroid <span class="math inline">\(\mu_{c^{(i)}}\)</span> to which it has been assigned.</p><ul><li>k-means is exactly <em>coordinate descent</em> 坐标下降 on <span class="math inline">\(J\)</span>. Specifically, the inner-loop of k-means repeatedly minimizes <span class="math inline">\(J\)</span> with respect to <span class="math inline">\(c\)</span> while holding <span class="math inline">\(\mu\)</span> fixed, and then minimizes <span class="math inline">\(J\)</span> with respect to <span class="math inline">\(\mu\)</span> while holding <span class="math inline">\(c\)</span> fixed. Thus,<span class="math inline">\(J\)</span> must monotonically decrease, and the value of <span class="math inline">\(J\)</span> must converge.</li></ul><p>The distortion function <span class="math inline">\(J\)</span> is a <strong><em>non-convex</em></strong> function, and so coordinate descent on <span class="math inline">\(J\)</span> is not guaranteed to converge to the global minimum.</p><h3 id="describe-the-em-algorithm-intuitively.">3. Describe the EM algorithm intuitively.</h3><ol start="4" type="1"><li>What is the Gaussian Mixture Model?</li><li>What are the two steps of the EM algorithm</li><li>Compare GMM vs GDA.</li></ol><h2 id="bayesian-machine-learning">Bayesian Machine Learning</h2><h3 id="what-are-the-differences-between-bayesian-and-freqentist-approach-for-machine-learning">1. What are the differences between “Bayesian” and “Freqentist” approach for Machine Learning?</h3><p>The <strong>Bayesian</strong> approach differs from the standard (&quot;<strong>frequentist</strong>&quot;) method for inference in its use of a <em>prior distribution</em> to express the uncertainty present before seeing the data, and to allow the uncertainty remaining after seeing the data to be expressed in the form of a <em>posterior distribution</em>.</p><p>Given a specific set of data, the <strong>frequentist</strong> believes that there is a true, underlying distribution from which said data was generated. The inability to get the exact parameters is a function of finite sample size. The <strong>Bayesian</strong>, on the other hand, think that we start with some assumption about the parameters (even if unknowingly) and use the data to refine our opinion about those parameters.</p><ul><li><p>The <strong>Bayesian</strong> probability measures a &quot;degree of belief&quot;. It pretty much matches our every-day intuitive understanding of probability,</p></li><li><p>The <strong>frequentists</strong> interpretation needs some explanation. Frequentists can assign probabilities only to events/obervations that come from repeatable experiments. With &quot;<em>probability of an event</em>&quot; they mean the relative frequency of the event occuring in an infinitively long series of repetitions. For instance, when a frequentists says that the probability for &quot;heads&quot; in a coin toss is 0.5 (50%) he means that in infinititively many such coin tosses, 50% of the coins will show &quot;head&quot;.</p></li></ul><p><strong>Frequentist:</strong> best suited to falsify a hypothesis <strong>Bayesian:</strong> best suited to (re)allocate the credibility of a statement</p><h4 id="downsides-of-frequentists">Downsides of Frequentists</h4><ul><li><p>Frequentists approach relies on data more than Bayesian as we totally ignore our knowledge or logical thinking which have been introduced in a form of prior probability.</p></li><li><p>P-value does not provide the probability of your hypothesis to be collect. It only avoids the most extreme value that seems to be rare. It sometimes make the situation difficult as you may find it challenging to explain the actual meaning of value. Whereas, the posterior probability describes in percentage how likely your hypothesis is correct based on our prior knowledge.</p></li></ul><h4 id="downsides-of-bayesians">Downsides of Bayesians</h4><ul><li>Bayesian Statistic requires more mathematical knowledge since the formula requires us to deduce two probability distributions.</li><li>What if your prior has become meaningless as the logic we have is no longer valid? (Some articles suggest that the prior at early stage can be any number as it can be updated as more information comes in)</li></ul><h3 id="when-will-you-use-bayesian-methods-instead-of-frequentist-methods">2. When will you use Bayesian methods instead of Frequentist methods?</h3><p>Small dataset, large feature set</p><h3 id="compare-maximum-likelihood-and-maximum-a-posteriori-estimation.">3. Compare maximum likelihood and maximum a posteriori estimation.</h3><p><strong><em>MLE</em></strong>:</p><p>Likelihood function: <span class="math inline">\(P(X|\theta)\)</span></p><p>MLE for θ, the parameter we want to infer: <span class="math display">\[\begin{align}\theta_{MLE}&amp;=\arg \max_\theta P(X|\theta) \\&amp;=\arg \max_\theta \prod_i P(x_i|\theta)\end{align}\]</span> As taking a product of some numbers less than 1 would approaching 0 as the number of those numbers goes to infinity, it would be not practical to compute, because of computation underflow. Hence, we will instead work in the log space, as logarithm is monotonically increasing, so maximizing a function is equal to maximizing the log of that function. <span class="math display">\[\begin{align}\theta_{MLE}&amp;=\arg \max_\theta \log P(X|\theta) \\&amp;=\arg \max_\theta \log \prod_i P(x_i|\theta) \\&amp;=\arg \max_\theta \sum_i \log  P(x_i|\theta)\end{align}\]</span> To use this framework, we just need to derive the log likelihood of our model, then maximizing it with regard of θθ using our favorite optimization algorithm like Gradient Descent.</p><p><strong><em>MAP</em></strong>:</p><p>MAP usually comes up in Bayesian setting. Because, as the name suggests, it works on a posterior distribution, not only the likelihood.</p><p>Bayes’ rule: <span class="math display">\[P(\theta|X)=\frac{P(X|\theta)P(\theta)}{P(X)} \\\propto P(X|\theta)P(\theta)\]</span></p><p><span class="math display">\[\begin{align}\theta_{MAP}&amp;=\arg \max_\theta P(X|\theta) P(\theta) \\&amp;=\arg \max_\theta \log P(X|\theta) P(\theta) \\&amp;=\arg \max_\theta \log \prod_i P(x_i|\theta) P(\theta)\\&amp;=\arg \max_\theta \sum_i \log  P(x_i|\theta)P(\theta) \end{align}\]</span></p><p>Comparing both MLE and MAP equation, the only thing differs is the <strong>inclusion of prior P(θ)</strong> in MAP. What it means is that, <strong>the likelihood is now weighted with some weight coming from the prior</strong>.</p><p>Let’s consider what if we use the simplest prior in our MAP estimation, i.e. <strong><em>uniform prior.</em></strong> This means, we assign equal weights everywhere, on all possible values of the θ. For example ,our prior P(θ) is <span class="math inline">\(\frac{1}{6}\)</span> <span class="math display">\[\begin{align}\theta_{MAP} &amp;=\arg \max_\theta \sum_i \log  P(x_i|\theta)P(\theta) \\&amp;= \arg \max_\theta \sum_i \log  P(x_i|\theta) const \\&amp;= \arg \max_\theta \sum_i \log  P(x_i|\theta) \\&amp;= \theta_{MLE}\end{align}\]</span></p><p><strong>Ref</strong>:</p><p><a href="https://github.com/Sroy20/machine-learning-interview-questions" target="_blank" rel="noopener">machine-learning-interview-questions</a></p><p><a href="%5Bhttp://cs229.stanford.edu/notes-spring2019/cs229-notes7a.pdf%5D(http://cs229.stanford.edu/notes-spring2019/cs229-notes7a.pdf)">CS229 Lecture notes: Unsupervised Learning, k-means clustering</a></p><p><a href="%5Bhttp://cs229.stanford.edu/notes-spring2019/cs229-notes7b.pdf%5D(http://cs229.stanford.edu/notes-spring2019/cs229-notes7b.pdf)">CS229 Lecture notes: Mixtures of Gaussians and the EM algorithm</a></p><p><a href="https://medium.com/@yongddeng/a-meaningless-debate-frequentists-vs-bayesians-7317317b458f" target="_blank" rel="noopener">A Meaningless Debate: Frequentists vs Bayesians</a></p><p><a href="https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading20.pdf" target="_blank" rel="noopener">Comparison of frequentist and Bayesian inference</a></p><p><a href="https://wiseodd.github.io/techblog/2017/01/01/mle-vs-map/" target="_blank" rel="noopener">MLE vs MAP: the connection between Maximum Likelihood and Maximum A Posteriori Estimation</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;clustering&quot;&gt;Clustering&lt;/h2&gt;
&lt;h3 id=&quot;describe-the-k-means-algorithm.&quot;&gt;1. Describe the k-means algorithm.&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;K-means clustering&lt;/strong&gt; is a simple and elegant approach for partitioning a data set into K distinct, &lt;strong&gt;&lt;em&gt;non-overlapping&lt;/em&gt;&lt;/strong&gt; clusters.&lt;/p&gt;
&lt;p&gt;The idea behind &lt;strong&gt;K-means clustering&lt;/strong&gt; is that a &lt;em&gt;good&lt;/em&gt; clustering is one for which the &lt;strong&gt;&lt;em&gt;within-cluster&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;variation&lt;/em&gt;&lt;/strong&gt; is as small as possible.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;within-cluster variation&lt;/strong&gt; for cluster &lt;span class=&quot;math inline&quot;&gt;\(C_k\)&lt;/span&gt; is a measure &lt;span class=&quot;math inline&quot;&gt;\(W(C_k)\)&lt;/span&gt; of the amount by which the observations within a cluster differ from each other.&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Interview" scheme="https://nancyyanyu.github.io/tags/Interview/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning Questions Part III: SVM</title>
    <link href="https://nancyyanyu.github.io/undefined/c8f688ba/"/>
    <id>https://nancyyanyu.github.io/undefined/c8f688ba/</id>
    <published>2019-06-17T18:07:59.000Z</published>
    <updated>2019-06-18T21:46:08.638Z</updated>
    
    <content type="html"><![CDATA[<h2 id="support-vector-machine">Support Vector Machine</h2><h3 id="svm-v.s.-logistic-regression">1. SVM v.s. Logistic Regression</h3><p><strong>SVM Optimization problem</strong>: <span class="math display">\[\max_{\beta_0,...\beta_p,\epsilon_1,..,\epsilon_n} M \\s.t.  \sum_{j=1}^p \beta_j^2=1,  \quad (9.13) \\ y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2},...+\beta_px_{ip})&gt;M(1-\epsilon_i) \quad \forall i=1,..,n.  \quad (9.14) \\ \epsilon_i\geq0,\sum_{i=1}^p\epsilon_i \leq C, \quad (9.15)\]</span> Rewrite the criterion (9.12)–(9.15) for fitting the support vector classifier <span class="math inline">\(f(X) = β_0 + β_1X_1 + . . . + β_pX_p\)</span> as <span class="math display">\[\min_{\beta_0,...,\beta_p}\left\{ \sum_{i=1}^n\max[0,1-y_if(x_i)]+\lambda\sum_{j=1}^p\beta_j^2 \right\}\]</span></p><ul><li>λ is small: few violations to the margin ; high-variance, low-bias; <span class="math inline">\(\Leftrightarrow\)</span> small <span class="math inline">\(C\)</span>;</li></ul><a id="more"></a><p><strong>“Loss + Penalty” form</strong>: <span class="math display">\[\min_{\beta_0,...,\beta_p}\left\{ L(\mathbf{X},\mathbf{y},\beta)+\lambda P(\beta) \right\}\]</span></p><ul><li><span class="math inline">\(L(\mathbf{X},\mathbf{y},\beta)\)</span> : loss function</li><li><span class="math inline">\(P(\beta)\)</span>: penalty function</li></ul><p><strong>Ridge regression and the lasso</strong>: <span class="math display">\[L(\mathbf{X},\mathbf{y},\beta)=\sum_{i=1}^n \left( y_i-\beta_0-\sum_{j=1}^p x_{ij}\beta_j \right)^2 \\P(\beta) = \sum_{j=1}^p \beta_j^2 \quad ridge \, regression \\P(\beta) = \sum_{j=1}^p |\beta_j| \quad lasso\]</span> <strong>SVM</strong>: <strong><em>hindge loss</em></strong> <span class="math display">\[L(\mathbf{X},\mathbf{y},\beta)=\sum_{i=1}^n \max[0,1-y_i(\beta_0+\beta_1x_{i1}+,,,+\beta_px_{ip})]\]</span> <strong>Optimization problems of linear SVM and (regularized) LR</strong>: <span class="math display">\[\min_\beta \lambda||\beta||^2+\sum_{i=1}^n \max[0,1-y_i(\beta_0+\beta_1x_{i1}+,,,+\beta_px_{ip})] \\\min_\beta \lambda||\beta||^2+\sum_{i=1}^n \log(1+\exp(1-y_i(\beta_0+\beta_1x_{i1}+,,,+\beta_px_{ip})))\]</span> That is, they only differ in the loss function — <strong>SVM minimizes hinge loss while logistic regression minimizes logistic loss.</strong></p><ul><li>Logistic loss diverges faster than hinge loss. So, in general, it will be more sensitive to outliers.</li><li>Logistic loss does not go to zero even if the point is classified sufficiently confidently. This might lead to minor degradation in accuracy.</li></ul><p><strong>Main Difference</strong>:</p><ul><li>SVM try to maximize the margin between the closest support vectors while LR the posterior class probability. Thus, SVM find a solution which is as fare as possible for the two categories while LR has not this property.</li></ul><p><img src="./1.png" width="500"></p><ul><li>LR is more sensitive to outliers than SVM because the cost function of LR diverges faster than those of SVM. So putting an outlier on above picture would give below picture:</li></ul><p><img src="./2.png" width="500"></p><ul><li>Logistic Regression produces probabilistic values while SVM produces 1 or 0. So in a few words LR makes not absolute prediction and it does not assume data is enough to give a final decision. This maybe be good property when what we want is an estimation or we do not have high confidence into data.<ul><li>In order to get discrete values <strong>1 or 0</strong> for the LR we can say that when a function value is greater than a threshold we classify as 1 and when a function value is smaller than the threshold we classify as 0.</li></ul></li></ul><p><strong>When to use which one?</strong></p><p><img src="./4.png" width="500"></p><h3 id="what-is-a-large-margin-classifier">2. What is a large margin classifier?</h3><p><strong><em>Margin</em></strong>: the smallest (perpendicular) distance from each training observation to a given separating hyperplane <span class="math inline">\(\Rightarrow\)</span> the minimal distance from the observations to the hyperplane.</p><p><strong><em>Maximal margin hyperplane</em></strong>: the separating hyperplane that is farthest from the training observations.</p><ul><li>The maximal margin hyperplane is the separating hyperplane for which the <em>margin</em> is <strong>largest</strong></li><li>Overfitting when <span class="math inline">\(p\)</span> is large.</li></ul><p><strong><em>Maximal margin classifier</em></strong>: classify a test observation based on which side of the maximal margin hyperplane it lies.</p><p>The <strong>maximal margin hyperplane</strong> is the solution to the optimization problem <span class="math display">\[\max_{\beta_0,...\beta_p} M \\s.t.  \sum_{j=1}^p \beta_j^2=1,  \quad (9.10) \\ y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2},...+\beta_px_{ip})&gt;M \quad \forall i=1,..,n.  \quad (9.11)\]</span></p><ul><li>The constraint in (9.11) in fact requires that each observation be on the correct side of the hyperplane, with some cushion, provided that <strong>margin</strong> <span class="math inline">\(M\)</span> is positive.)</li><li>The constraint in (9.10) makes sure the perpendicular distance from the i-th observation to the hyperplane is given by</li></ul><p><span class="math display">\[y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2},...+\beta_px_{ip})\]</span></p><h3 id="why-svm-is-an-example-of-a-large-margin-classifier">3. Why SVM is an example of a large margin classifier?</h3><p><strong><em>Support Vector Classifier (Soft Margin Classifier)</em></strong>: Rather than seeking the largest possible margin that every observation is not only on the correct side of the hyperplane but also on the correct side of the margin, we instead allow some observationsto be on the incorrect side of the margin, or even the incorrect side of the hyperplane.</p><p>​ <strong>Optimization problem</strong>: <span class="math display">\[\max_{\beta_0,...\beta_p,\epsilon_1,..,\epsilon_n} M \\s.t.  \sum_{j=1}^p \beta_j^2=1,  \quad (9.13) \\ y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2},...+\beta_px_{ip})&gt;M(1-\epsilon_i) \quad \forall i=1,..,n.  \quad (9.14) \\ \epsilon_i\geq0,\sum_{i=1}^p\epsilon_i \leq C, \quad (9.15)\]</span></p><ul><li><strong><em>Slack variables</em></strong>: <span class="math inline">\(\epsilon_1,..,\epsilon_n\)</span> - allow individual observations to be on the wrong side of the margin or the hyperplane<ul><li><span class="math inline">\(\epsilon_i=0\)</span>: the i-th observation is on the correct side of the <em>margin</em></li><li><span class="math inline">\(\epsilon_i &gt;0\)</span>: the i-th observation is on the wrong side of the <em>margin</em> <span class="math inline">\(\Rightarrow\)</span> i-th observation <strong><em>violated</em></strong> the margin.</li><li><span class="math inline">\(\epsilon_i &gt;1\)</span>: the i-th observation is on the wrong side of the <em>hyperplane</em></li></ul></li><li>Classify the test observation based on the sign of <span class="math inline">\(f(x^∗) = \beta_0+\beta_1x_{1}^*+\beta_2x_{2}^*,...+\beta_px_{p}^*\)</span>.</li></ul><p>The <strong><em>support vector machine (SVM)</em></strong> is an extension of the support vector classifier that results from enlarging the feature space using <strong>kernels</strong>.</p><h3 id="svm-being-a-large-margin-classifier-is-it-influenced-by-outliers">4. SVM being a large margin classifier, is it influenced by outliers?</h3><p>Yes, if C is large, otherwise not</p><h3 id="what-is-the-role-of-c-in-svm">5. What is the role of C in SVM?</h3><p><strong><em>Tuning parameter C</em></strong>: <span class="math inline">\(C\)</span> bounds the sum of the <span class="math inline">\(\epsilon_i\)</span>'s, and so it determines the number and severity of the violationsto the margin(and to the hyperplane) that we will tolerate.</p><ul><li><strong><em>budget</em></strong> for the amount that the margin can be violated by the <span class="math inline">\(n\)</span> observations.</li><li>Generally chosen via <em>cross-validation</em>.</li><li><span class="math inline">\(C\)</span> controls the <strong>bias-variance trade-off</strong> of the support vector classifier.<ul><li>C is small: highly fit to the data, fewer support vectors <span class="math inline">\(\Rightarrow\)</span> low bias , high variance;</li><li>C is large: margin wider, many support vectors <span class="math inline">\(\Rightarrow\)</span> high bias , low variance;</li></ul></li></ul><h3 id="what-is-a-kernel-in-svm-why-do-we-use-kernels-in-svm">6. What is a kernel in SVM? Why do we use kernels in SVM?</h3><p><strong>Kernel</strong>: Kernel is a function that quantifies the similarity of two observations.</p><ul><li><strong><em>Linear kernel</em></strong>: <span class="math inline">\(K(x_i,x_{i^{&#39;}})=\sum_{j=1}^px_{ij}x_{i^{&#39;}j}\)</span><ul><li>Linear kernel essentially quantifies the similarity of a pair of observations using <strong>Pearson</strong> (standard) correlation.</li></ul></li><li><strong><em>Polynomial kernel</em></strong> of degree d: <span class="math inline">\(K(x_i,x_{i^{&#39;}})=(1+\sum_{j=1}^px_{ij}x_{i^{&#39;}j})^d\)</span><ul><li>fitting a support vector classifier in a higher-dimensional space involving polynomials of degree <span class="math inline">\(d\)</span>.</li></ul></li><li><strong><em>Radial kernel</em></strong>: <span class="math inline">\(K(x_i,x_{i^{&#39;}})=\exp(-\gamma \sum_{j=1}^p(x_{ij}-x_{i^{&#39;}j})^2)\)</span><ul><li>Radial kernel has very <em>local</em> behavior: only nearby training observations have an effect on the class label of a test observation<ul><li>If a given test observation <span class="math inline">\(x^∗ = (x^∗_1 . . .x^∗_p)^T\)</span> is far from a training observation <span class="math inline">\(x_i\)</span> in terms of <strong><em>Euclidean distance</em></strong>; <span class="math inline">\(\Rightarrow\)</span> $ _{j=1}<sup>p(x_{ij}-x_{i</sup>{'}j})^2 $ will be large <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(K(x_i,x_{i^{&#39;}})=\exp(-\gamma \sum_{j=1}^p(x_{ij}-x_{i^{&#39;}j})^2)\)</span> will be very tiny. <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(x_i\)</span> will play virtually no role in <span class="math inline">\(f(x^∗)\)</span>.</li></ul></li></ul></li></ul><p><strong><em>Support Vector Machine</em></strong>: When the support vector classifieris combined with a non-linear kernel, the resulting classifier is known as a support vector machine. <span class="math display">\[f(x)=\beta_0+\sum_{i \in S}^n \alpha_i K(x,x_i)\]</span> In machine learning, a <strong>“kernel”</strong> is usually used to refer to the kernel trick, a method of <strong><em>using a linear classifier to solve a non-linear problem</em></strong>. The kernel function is what is applied on each data instance to map the original non-linear observations into a higher-dimensional space in which they become <em>separable</em>.</p><p><strong>Advantage of Kernel over enlarging the feature space using functions of the original features: </strong></p><ul><li><strong><em>Computational</em></strong>: one need only compute <span class="math inline">\(K(x_i,x_{i^{&#39;}})\)</span> for all <span class="math inline">\(\left(\begin{array}{c}n\\ 2\end{array}\right)\)</span> distinct pairs <span class="math inline">\(i, i^{&#39;}\)</span>. This can bedone without explicitly working in the <em>enlarged feature space.</em><ul><li><strong>Curse of dimensionality</strong>: for some kernels, such as the radial kernel, the feature space is implicit and infinite-dimensional.</li></ul></li></ul><h3 id="can-we-apply-the-kernel-trick-to-logistic-regression-why-is-it-not-used-in-practice-then">7. Can we apply the kernel trick to logistic regression? Why is it not used in practice then?</h3><p>While converting the primal SVM formulation into its dual form (which gives us the kernel version of the SVM), we notice that one of the equations we get is, <span class="math display">\[\beta=\sum_{i=1}^n \alpha_iy_ix_i\]</span> This, in the kernelized version where the kernel <span class="math inline">\(k\)</span> has an implicit representation for points given by x <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(\phi(x)\)</span>, <span class="math display">\[\beta=\sum_{i=1}^n \alpha_iy_i\phi(x_i)\]</span> Surprisingly, one can also get this form from the representer theorem [2]. This suggests something general about the classifiers we learn.</p><p>Now, let us get back to logistic regression, which is modeled as, <span class="math display">\[p(y=1|x)=\frac{1}{1+\exp(-\beta^Tx)}\]</span> First of all, let us map the x to the space of implicit representation of the kernel. So, our model in the implicit representation space would look like, <span class="math display">\[p(y=1|x)=\frac{1}{1+\exp(-\beta^T\phi(x))}\]</span> Next, let us use the form of <span class="math inline">\(\beta\)</span>, we observed from SVM and the representer theorem. This will give us, <span class="math display">\[p(y=1|x)=\frac{1}{1+\exp(-\sum_{i=1}^n\alpha_iy_i\phi(x_i)^T\phi(x))} \\=\frac{1}{1+\exp(-\sum_{i=1}^n\alpha_iy_iK(x_i,x))}\]</span> This gives us the kernelized logistic regression model.</p><p><strong>Why is it not used in practice then?</strong>: logistic regression with kernels is merely an <strong>SVM</strong> without maximum margins</p><h3 id="how-does-the-svm-guassian-kernel-parameter-lambda-affect-the-biasvariance-trade-off">8. How does the SVM Guassian kernel parameter <span class="math inline">\(\lambda\)</span> affect the bias/variance trade off?</h3><p><strong>Larger RBF kernel bandwidths (i.e. smaller 𝛾γ) produce smoother decision boundaries because they produce smoother feature space mappings</strong>. Smoother mappings produce simpler decision boundaries:</p><p><img src="./5.png" width="700"> <span class="math display">\[K(x_i,x_j)=\exp(-\lambda ||x_i-x_j||^2)\]</span></p><ul><li><strong>Small <span class="math inline">\(\lambda\)</span>:</strong> a Gaussian with a <em>large variance</em> so the influence of <span class="math inline">\(x_j\)</span> is more, i.e. if <span class="math inline">\(x_j\)</span> is a support vector, the class of this support vector will have influence on deciding the class of the vector <span class="math inline">\(x_i\)</span> even if the distance between them is large <span class="math inline">\(\rightarrow\)</span> <strong>Larger RBF kernel bandwidths</strong> <span class="math inline">\(\rightarrow\)</span> smoother decision boundaries $  $ <strong>high bias , low variance</strong></li><li><strong>Large <span class="math inline">\(\lambda\)</span>:</strong> variance is small <span class="math inline">\(\rightarrow\)</span> support vector does not have wide-spread influence. Technically speaking <span class="math inline">\(\rightarrow\)</span> <strong>low bias , high variance</strong>.</li></ul><p><strong>Ref</strong>:</p><p><a href="https://github.com/Sroy20/machine-learning-interview-questions" target="_blank" rel="noopener">machine-learning-interview-questions</a></p><p><a href="https://towardsdatascience.com/support-vector-machine-vs-logistic-regression-94cc2975433f" target="_blank" rel="noopener">Support Vector Machine vs Logistic Regression</a></p><p><a href="https://towardsdatascience.com/kernel-function-6f1d2be6091" target="_blank" rel="noopener">Kernel Functions</a></p><p><a href="https://www.quora.com/What-are-kernels-in-machine-learning-and-SVM-and-why-do-we-need-them" target="_blank" rel="noopener">What are kernels in machine learning and SVM and why do we need them?</a></p><p><a href="https://www.quora.com/How-can-one-use-kernels-utilizing-the-kernel-trick-in-logistic-regression" target="_blank" rel="noopener">How can one use kernels (utilizing the kernel trick) in logistic regression?</a></p><p><a href="https://www.quora.com/What-are-C-and-gamma-with-regards-to-a-support-vector-machine" target="_blank" rel="noopener">What are C and gamma with regards to a support vector machine?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;support-vector-machine&quot;&gt;Support Vector Machine&lt;/h2&gt;
&lt;h3 id=&quot;svm-v.s.-logistic-regression&quot;&gt;1. SVM v.s. Logistic Regression&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;SVM Optimization problem&lt;/strong&gt;: &lt;span class=&quot;math display&quot;&gt;\[
\max_{\beta_0,...\beta_p,\epsilon_1,..,\epsilon_n} M \\
s.t.  \sum_{j=1}^p \beta_j^2=1,  \quad (9.13) \\
 y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2},...+\beta_px_{ip})&amp;gt;M(1-\epsilon_i) \quad \forall i=1,..,n.  \quad (9.14) \\
 \epsilon_i\geq0,\sum_{i=1}^p\epsilon_i \leq C, \quad (9.15)
\]&lt;/span&gt; Rewrite the criterion (9.12)–(9.15) for fitting the support vector classifier &lt;span class=&quot;math inline&quot;&gt;\(f(X) = β_0 + β_1X_1 + . . . + β_pX_p\)&lt;/span&gt; as &lt;span class=&quot;math display&quot;&gt;\[
\min_{\beta_0,...,\beta_p}\left\{ \sum_{i=1}^n\max[0,1-y_if(x_i)]+\lambda\sum_{j=1}^p\beta_j^2 \right\}
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;λ is small: few violations to the margin ; high-variance, low-bias; &lt;span class=&quot;math inline&quot;&gt;\(\Leftrightarrow\)&lt;/span&gt; small &lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Interview" scheme="https://nancyyanyu.github.io/tags/Interview/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning Questions Part II: COD, Reg, Model Evaluation, Dimensionality Reduction</title>
    <link href="https://nancyyanyu.github.io/undefined/a2f8a358/"/>
    <id>https://nancyyanyu.github.io/undefined/a2f8a358/</id>
    <published>2019-06-15T10:53:38.000Z</published>
    <updated>2019-06-18T21:37:04.828Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h2 id="curse-of-dimensionality">Curse of dimensionality</h2><h3 id="describe-the-curse-of-dimensionality-with-examples.">1. Describe the curse of dimensionality with examples.</h3><p><strong><em>Curse of dimensionality:</em></strong> as the dimensionality of the features space increases, the number configurations can grow exponentially, and thus the number of configurations covered by an observation decreases.</p><p><strong>As the number of feature or dimensions grows, the amount of data we need to generalise accurately grows exponentially.</strong></p><p>（fun example: It's easy to hunt a dog and maybe catch it if it were running around on the plain (two dimensions). It's much harder to hunt birds, which now have an extra dimension they can move in. If we pretend that ghosts are higher-dimensional beings ）</p><a id="more"></a><h3 id="what-is-local-constancy-or-smoothness-prior-or-regularization">2. What is local constancy or smoothness prior or regularization?</h3><p>(See DL Book 5.11.2)</p><p><strong>Smoothness prior</strong> or <strong>local constancy prior</strong>: This prior states that the function we learn should not change very much within a small region.</p><p>Many simpler algorithms rely exclusively on this prior to generalize well, and as a result they fail to scale to the statistical challenges involved in solving AIlevel tasks.</p><ul><li>KNN, decision trees, local kernel</li></ul><p>All of these different methods are designed to encourage the learning process to learn a function <span class="math inline">\(f^*\)</span> that satisfies the condition <span class="math display">\[f^*(x)\approx f^*(x+\epsilon)\]</span> In other words, if we know a good answer for an input x (for example, if x is a labeled training example) then that answer is probably good in the neighborhood of x.</p><p>Assuming only <strong>smoothness</strong> of the underlying function will not allow a learnerto represent a complex function that has many more regions to be distinguished than the number of training examples</p><h2 id="regularization">Regularization</h2><h3 id="what-is-l1-regularization">1. What is L1 regularization?</h3><p>L1 lasso penalty: <span class="math inline">\(\sum_{j=1}^p |\beta_j|\)</span></p><p>A type of regularization that penalizes weights in proportion to the <strong>sum of the absolute values</strong> of the weights. In models relying on <strong>sparse features</strong>, L1 regularization helps drive the weights of irrelevant or barely relevant features to exactly 0, which removes those features from the model.</p><h3 id="what-is-l2-regularization">2. What is L2 regularization?</h3><p>L2 ridge penalty : <span class="math inline">\(\sum_{j=1}^p\beta_j^2\)</span></p><p>A type of regularization that penalizes weights in proportion to the sum of the squares of the weights. L2 regularization helps drive outlier weights (those with high positive or low negative values) closer to 0 but not quite to 0. L2 regularization always improves generalization in linear models.</p><h3 id="compare-l1-and-l2-regularization.">3. Compare L1 and L2 regularization.</h3><p><strong>SAME</strong>: Ridge &amp; Lasso all can yield a reduction in variance at the expense of a small increase in bias, and consequently can generate more accurate predictions.</p><p><strong>DIFFERENCES</strong>:</p><ul><li>Unlike ridge regression, the <strong>lasso performs variable selection</strong>, and hence results in models that are easier to interpret.</li><li>ridge regression outperforms the lasso in terms of prediction error in this setting</li></ul><p><strong>Suitable setting</strong>:</p><ul><li><strong>Lasso</strong>: perform better in a setting where a relatively small number of predictors have substantial coefficients, and the remaining predictors have coefficients that are very small or that equal zero.</li><li><strong>Ridge regression</strong>: perform better when the response is a function of many predictors, all with coefficients of roughly equal size.</li><li>The number of predictors that is related to the response is never known a <strong>priori</strong> for real data sets. Cross-validation can be used in order to determine which approach is better on a particular data set.</li></ul><h3 id="why-does-l1-regularization-result-in-sparse-models">4. Why does L1 regularization result in sparse models?</h3><p>The lasso and ridge regression coefficient estimates are given by the first point at which an ellipse contacts the constraint region.</p><p><strong>Ridge regression</strong>: <strong>circular</strong> constraint with no sharp points, so the ridge regression coefficient estimates will be exclusively non-zero.</p><p><strong>The lasso</strong>: constraint has <strong>corners</strong> at each of the axes, and so the ellipse will often intersect the constraint region at an axis.</p><ul><li>The <span class="math inline">\(l_1\)</span> penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter λ is sufficiently large.</li><li>Hence, much like best subset selection, the lasso performs <strong>variable selection</strong></li></ul><blockquote><p>Lasso yields <strong>sparse</strong> models</p></blockquote><p><img src="./1.png" width="600"></p><h2 id="evaluation-of-machine-learning-systems">Evaluation of Machine Learning systems</h2><h3 id="what-are-accuracy-sensitivity-specificity-roc-auc-confusion-matrix-f1-score">1. What are accuracy, sensitivity, specificity, ROC, AUC, Confusion matrix, F1-Score?</h3><p>(see ISLR Note - Linear Discriminant Analysis, ROC &amp; AUC, Confusion Matrix)</p><p><strong><em>Confusion matrix</em></strong>: An NxN table that summarizes how successful a <a href="https://developers.google.com/machine-learning/glossary/#classification_model" target="_blank" rel="noopener"><strong>classification model's</strong></a> predictions were</p><p><img src="./2.png"></p><p><strong><em>Accuracy</em></strong>: The fraction of predictions that a classification model got right.</p><ul><li>In multi-class classification, accuracy is defined as follows: <span class="math display">\[Accuracy=\frac{Correct Predictions}{Total Number Of Observations}\]</span></li></ul><p>In binary classification, accuracy has the following definition: <span class="math display">\[Accuracy=\frac{TruePositives+TrueNegatives}{Total Number Of Observations}\]</span> <strong><em>Sensitivity/ Recall/ TPR</em></strong>: A metric for classification models that answers the following question: Out of all the possible <em>positive labels</em>(positive under true condition), how many did the model correctly identify? <span class="math display">\[Sensitivity=\frac{TruePositives}{TruePositives+FalseNegatives}\]</span> <strong><em>Specificity/ Selectivity/ TNR</em></strong>: Out of all the possible <em>negative labels</em> (negative under true condition), how many did the model correctly identify? <span class="math display">\[Specificity=\frac{TrueNegatives}{TrueNegatives+FalsePositives}\]</span> <strong><em>Precision/ PPV</em></strong>: <span class="math display">\[Precision=\frac{TruePositives}{TruePositives+FalsePositives}\]</span> <strong><em>Type I and Type II Errors</em></strong>:</p><p>According to me, the null hypothesis in this case is that this call is a hoax. As a matter of fact, if Jack would have believed the stranger and provided his bank details, and the call was in fact a hoax, he would have committed a type I error, also known as a false positive. On the other hand, had he ignored the stranger’s request, but later found out that he actually had won the lottery and the call was not a hoax, he would have committed a Type II error, or a false negative.</p><p><strong>Recall v.s. Precision</strong>:</p><ul><li>Recall refers to the percentage of total relevant results correctly classified by your algorithm</li><li>Precision means the percentage of your results which are relevant.</li></ul><p><img src="./3.png" width="800"></p><blockquote><p><em>…Feeling a bit panicky, Jack called up his bank to ensure his existing accounts were safe and all his credits were secure. After listening to Jack’s story, the bank executive informed Jack that all his accounts were safe. However, in order to ensure that there is no future risk, the bank manager asked Jack to recall all instances in the last six months wherein he might have shared his account details with another person for any kind of transaction, or may have accessed his online account from a public system, etc…</em></p></blockquote><p>What are the chances that Jack will be able to <em>recall</em> all such instances <em>precisely</em>?</p><ul><li><p>If Jack had let’s say 10 such instances in reality, and he narrated 20 instances to finally spell out the 10 correct instances, then his <em>recall</em> will be a 100%, but his <em>precision</em> will only be 50%.</p></li><li><p><strong>Trade-off</strong>: If you have to <em>recall</em> everything, you will have to keep generating results which are not accurate, hence lowering your <em>precision</em>.</p></li></ul><p><strong><em>AUC - ROC curve</em></strong> is a performance measurement for classification problem at <strong>various thresholds settings</strong>.</p><p><strong><em>ROC</em></strong> is a probability curve and <strong><em>AUC</em></strong> represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. By analogy, Higher the AUC, better the model is at distinguishing between patients with disease and no disease. <span class="math display">\[Recall=\frac{TP}{TP+FN} \\Specificity=\frac{TN}{FP+TN} \\FPR=1-Specificity=\frac{FP}{FP+TN}\]</span></p><ul><li>An ideal ROC curve will hug the top left corner, so the larger the AUC the better the classifier. We expect a classifier that performs no better than chance to <em>have an AUC of 0.5</em></li><li>ROC curves are useful for comparing different classifiers, since they take into account all possible thresholds.</li></ul><p><img src="./5.png" width="600"></p><p><strong><em>F1-Score</em></strong>: <span class="math display">\[Recall=\frac{2 \cdot Recall \cdot Precision}{Recall+Precision}\]</span></p><ul><li><strong>F1 Score</strong> : the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account.<ul><li>needed when you want to <em>seek a balance between Precision and Recall</em>, and there is an <strong>uneven class distribution</strong> <em>(large number of Actual Negatives</em>)</li></ul></li><li><strong>Difference between F1 Score and Accuracy</strong> : Accuracy can be largely contributed by a large number of <em>True Negatives</em> which in most business circumstances, we do not focus on much whereas False Negative and False Positive usually has business costs (tangible &amp; intangible) .Accuracy works best if false positives and false negatives have similar cost. If the cost of false positives and false negatives are very different, it’s better to look at both Precision and Recall.</li></ul><h3 id="describe-t-test-in-the-context-of-machine-learning.">2. Describe t-test in the context of Machine Learning.</h3><p><strong>Hypothesis Tests</strong>:</p><p>The most common hypothesis test involves testing the <strong>null test hypothesis</strong> of</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">H_0: There is no relationship between X and Y or β1=0</span><br></pre></td></tr></table></figure><p>versus the <strong>alternative hypothesis</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">H_a : There is some relationship between X and Y or β1≠0</span><br></pre></td></tr></table></figure><p>To test the null hypothesis, we need to determine whether <span class="math inline">\(\hat{\beta_1}\)</span>, our estimate for <span class="math inline">\(\beta_1\)</span>, is sufficiently far from zero that we can be confident that <span class="math inline">\(\beta_1\)</span> is non-zero <span class="math inline">\(\Rightarrow\)</span> it depends on SE( <span class="math inline">\(\hat{\beta_1}\)</span>)</p><ul><li>If SE( <span class="math inline">\(\hat{\beta_1}\)</span>) is small, then even relatively small values of <span class="math inline">\(\hat{\beta_1}\)</span> may provide strong evidence that <span class="math inline">\(\beta_1 \neq 0\)</span>, and hence that there is a relationship between X and Y</li></ul><p><strong>t-statistic</strong> <span class="math display">\[\begin{align}t=\frac{\hat{\beta_1}-0}{SE(\hat{\beta_1})} \end{align}\]</span> which measures <strong>the number of standard deviations that <span class="math inline">\(\hat{\beta_1}\)</span> is away from 0.</strong>If there really is no relationship between X and Y , then we expect it will have a t-distribution with n−2 degrees of freedom.</p><p><strong>Standard error of <span class="math inline">\(\hat{\mu}\)</span> (SE(<span class="math inline">\(\hat{\mu}\)</span>)</strong>): average amount that this estimate <span class="math inline">\(\hat{\mu}\)</span> differs from the actual value of μ. <span class="math display">\[\begin{align}Var(\hat{\mu})=SE(\hat{\mu})^2=\frac{\sigma^2}{n}\end{align}\]</span> where σ is the standard deviation of each of the realizations <span class="math inline">\(y_i\)</span> of <span class="math inline">\(Y\)</span> provided that the <span class="math inline">\(n\)</span> observations are <strong>uncorrelated</strong>.</p><p><strong>Standard Deviation V.S. Standard Error</strong></p><ul><li>The standard deviation (SD) measures the amount of variability, or dispersion, for a subject set of data from the mean</li><li>The standard error of the mean (SEM) measures how far the sample mean of the data is likely to be from the true population mean.</li></ul><p><img src="./6.png" width="300"></p><p><strong>P-value</strong></p><ul><li>The probability of observing any value <span class="math inline">\(≥ t\)</span> or <span class="math inline">\(≤ -t\)</span>, assuming <span class="math inline">\(β_1 = 0\)</span>.</li></ul><p><img src="./7.png" width="300"> (Here <span class="math inline">\(|t|=2.17\)</span>, p-value<span class="math inline">\(=0.015\)</span>.The area in red is <span class="math inline">\(0.015 + 0.015 = 0.030\)</span>, <span class="math inline">\(3\%\)</span>. If we had chosen a significance level of <span class="math inline">\(5\%\)</span>, this would mean that we had achieved statistical significance. We would reject the null hypothesis in favor of the alternative hypothesis.)</p><ul><li><strong>Interpretation</strong>:a small p-value indicates<ul><li>It is unlikely to observe such a substantial association between the predictor and the response due to LUCK, in the absence of any real association between the predictor and the response.</li><li>There is an association between the predictor and the response.</li><li>We reject the null hypothesis—that is, we declare a relationship to exist between X and Y</li></ul></li></ul><h2 id="dimensionality-reduction">Dimensionality Reduction</h2><h3 id="why-do-we-need-dimensionality-reduction-techniques">1. Why do we need dimensionality reduction techniques?</h3><p>Because the <strong>curse of dimensionality</strong> demands that we do.</p><ol type="1"><li>Less misleading data means <em>model accuracy</em> improves.</li><li>Less dimensions mean <em>less computing</em>. Less data means that <em>algorithms train faster</em>.</li><li>Less data means <em>less storage space</em> required.</li><li>Less dimensions allow usage of algorithms unfit for a large number of dimensions</li><li><p>Removes redundant features and noise.</p></li><li><p>PCA also serves as a tool for data visualization (visualization of the observations or visualization of the variables).</p></li></ol><h3 id="what-do-we-need-pca-and-what-does-it-do">2. What do we need PCA and what does it do?</h3><p><strong><em>PCA</em></strong> : find a lower dimensional surface such the <strong>sum of the squared projection error</strong> is minimized</p><p><strong>PCA</strong> :finds a low-dimensional representation of a data set that contains as much as possible of the <strong>variation</strong></p><p>Each of the dimensions found by PCA is a linear combination of the <span class="math inline">\(p\)</span> features.</p><p><strong><em>The first principal component</em></strong> of a set of features <span class="math inline">\(X_1,X_2, . . . , X_p\)</span> is the <strong>normalized</strong> linear combination of the features <span class="math display">\[\begin{align}Z_1=\phi_{11}X_1+\phi_{21}X_2+,,,+\phi_{p1}X_p\end{align}\]</span> that has the <strong>largest variance</strong>.</p><!--more--><p><strong>Normalized</strong>: <span class="math inline">\(\sum_{j=1}^p \phi_{j1}^2=1\)</span></p><p><strong>Loadings</strong>: <span class="math inline">\(\phi_{11}, . . . , \phi_{p1}\)</span> the loadings of the first principal component;</p><ul><li>Together, the loadings make up the principal component loading vector, <span class="math inline">\(\phi_1=(\phi_{11},\phi_{21},...,\phi_{p1})^T\)</span></li></ul><h4 id="compute-the-first-principal-component">Compute the first principal component</h4><ul><li><p>Assume that each of the variables in <span class="math inline">\(X\)</span> has been centered to have mean zero. We then look for the linear combination of the sample feature values of the form <span class="math display">\[\begin{align}z_{i1}=\phi_{11}x_{i1}+\phi_{21}x_{i2}+,,,+\phi_{p1}x_{ip} \quad \quad i=1,2,...,n\end{align}\]</span> that has largest sample variance, subject to the constraint that <span class="math inline">\(\sum_{j=1}^p \phi_{j1}^2=1\)</span></p></li><li><p>The first principal component loading vector solves the optimization problem <span class="math display">\[\begin{align}\max_{\phi_{11},...,\phi_{p1}}{\left\{ \frac{1}{n} \sum_{i=1}^n \left( \sum_{j=1}^p \phi_{j1}x_{ij}   \right)^2 \right\}} \, subject \, to \, \sum_{j=1}^p \phi_{j1}^2=1\end{align}\]</span></p></li><li><p>Since <span class="math inline">\(\sum_{i=1}^nx_{ij}/n=1\)</span>, the average of the <span class="math inline">\(z_{11}, . . . , z_{n1}\)</span> will be zero as well. Hence the objective that we are maximizing is just the <strong>sample variance</strong> of the <span class="math inline">\(n\)</span> values of zi1</p></li><li><p><strong>Scores</strong>: We refer to <span class="math inline">\(z_{11}, . . . , z_{n1}\)</span> as the scores of the first principal component.</p></li></ul><p><strong>Geometric interpretation</strong>: for the first principal component: The loading vector <span class="math inline">\(\phi_1\)</span> with elements <span class="math inline">\(\phi_{11},\phi_{21},...,\phi_{p1}\)</span> defines a direction in feature space along which the data <strong>vary the most</strong>. If we project the <span class="math inline">\(n\)</span> data points <span class="math inline">\(x_1, . . . , x_n\)</span> onto this direction, the projected values are the principal component scores <span class="math inline">\(z_{11}, . . . , z_{n1}\)</span> themselves.</p><h4 id="compute-the-second-principal-component">Compute the second principal component</h4><p><strong>The second principal component <span class="math inline">\(Z_2\)</span></strong>: the linear combination of <span class="math inline">\(X_1,X_2, . . . , X_p\)</span> that has maximal variance out of all linear combinations that are <strong>uncorrelated with <span class="math inline">\(Z_1\)</span></strong>.</p><p>The second principal component scores <span class="math inline">\(z_{12}, . . . , z_{n2}\)</span> take the form <span class="math display">\[\begin{align}z_{i2}=\phi_{12}x_{i1}+\phi_{22}x_{i2}+,,,+\phi_{p2}x_{ip} \quad \quad i=1,2,...,n\end{align}\]</span> where <span class="math inline">\(\phi_2\)</span> is the second principal component <strong>loading</strong> vector, with elements <span class="math inline">\(\phi_{12},\phi_{22},...,\phi_{p2}\)</span>.</p><p>It turns out that constraining <span class="math inline">\(Z_2\)</span> to be uncorrelated with <span class="math inline">\(Z_1\)</span> is equivalent to constraining the direction <span class="math inline">\(\phi_2\)</span> to be <strong>orthogonal</strong> (perpendicular) to the direction <span class="math inline">\(\phi_1\)</span>.</p><p>To find <span class="math inline">\(\phi_2\)</span>, we solve a problem similar to (10.3) with <span class="math inline">\(\phi_2\)</span> replacing <span class="math inline">\(\phi_1\)</span>, and with the additional constraint that <span class="math inline">\(\phi_2\)</span> is orthogonal to <span class="math inline">\(\phi_1\)</span></p><p><img src="./8.png" width="600"></p><p><strong>Interpretation:</strong></p><ul><li>1st loading vector places approximately equal weight on Assault, Murder, and Rape, with much less weight UrbanPop. Hence this component roughly corresponds to a measure of overall rates of serious crimes.</li><li>Overall, we see that the crime-related variables (Murder, Assault, and Rape) are located close to each other, and that the UrbanPop variable is far from the other three.</li><li>This indicates that the crime-related variables are correlated with each other—states with high murder rates tend to have high assault and rape rates—and that the UrbanPop variable is less correlated with the other three.</li></ul><h3 id="what-is-the-difference-between-logistic-regression-and-pca">3. What is the difference between logistic regression and PCA?</h3><ul><li>PCA will <strong>NOT consider</strong> the response variable but only the variance of the independent variables.</li><li>Logistic Regression will <strong>consider</strong> how each independent variable impact on response variable.</li></ul><h3 id="what-are-the-two-pre-processing-steps-that-should-be-applied-before-doing-pca">4. What are the two pre-processing steps that should be applied before doing PCA?</h3><p>Mean normalization, feature scaling: Before PCA is performed, the variables should be <strong>centered to have mean zero</strong>. Furthermore, the results obtained when we perform PCA will also depend on whether the variables have been <strong>individually scaled</strong> to have standard deviation one. (each multiplied by a different constant)</p><p><img src="./9.png" width="600"></p><p><strong>Ref</strong>:</p><p><a href="https://github.com/Sroy20/machine-learning-interview-questions" target="_blank" rel="noopener">machine-learning-interview-questions</a></p><p><a href="https://towardsdatascience.com/precision-vs-recall-386cf9f89488" target="_blank" rel="noopener">Precision vs Recall</a></p><p><a href="https://towardsdatascience.com/dimensionality-reduction-for-machine-learning-80a46c2ebb7e" target="_blank" rel="noopener">A beginner’s guide to dimensionality reduction in Machine Learning</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id=&quot;curse-of-dimensionality&quot;&gt;Curse of dimensionality&lt;/h2&gt;
&lt;h3 id=&quot;describe-the-curse-of-dimensionality-with-examples.&quot;&gt;1. Describe the curse of dimensionality with examples.&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Curse of dimensionality:&lt;/em&gt;&lt;/strong&gt; as the dimensionality of the features space increases, the number configurations can grow exponentially, and thus the number of configurations covered by an observation decreases.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;As the number of feature or dimensions grows, the amount of data we need to generalise accurately grows exponentially.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;（fun example: It&#39;s easy to hunt a dog and maybe catch it if it were running around on the plain (two dimensions). It&#39;s much harder to hunt birds, which now have an extra dimension they can move in. If we pretend that ghosts are higher-dimensional beings ）&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Interview" scheme="https://nancyyanyu.github.io/tags/Interview/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning Questions Part I: Learning Theory &amp; Model Selection</title>
    <link href="https://nancyyanyu.github.io/undefined/6bd38994/"/>
    <id>https://nancyyanyu.github.io/undefined/6bd38994/</id>
    <published>2019-06-15T03:57:50.000Z</published>
    <updated>2019-06-18T21:46:04.891Z</updated>
    
    <content type="html"><![CDATA[<h2 id="learning-theory">Learning Theory</h2><h3 id="describe-bias-and-variance-with-examples.">1. Describe bias and variance with examples.</h3><p><strong><em>Variance</em></strong>: refers to the amount by which <span class="math inline">\(\hat{f}\)</span> would change if we estimated it using a different training data set. <strong><em>more flexible statistical methods have higher variance</em></strong></p><ul><li>Explanation: different training data sets will result in a different <span class="math inline">\(\hat{f}\)</span>. But ideally the estimate for f should not vary too much between training sets. However, if a method has high variance then small changes in the training data can result in large changes in <span class="math inline">\(\hat{f}\)</span></li></ul><p><strong><em>Bias</em></strong>: refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model.</p><ul><li>Explanation: As we increase the flexibility of a class of methods, the bias tends to initially decrease faster than the variance increases. Consequently, the expected test MSE declines. However, at some point increasing flexibility has little impact on the bias but starts to significantly increase the variance. When this happens the test MSE increases.</li></ul><p><strong><em>Decomposition</em></strong>：The expected test MSE, for a given value <span class="math inline">\(x_0\)</span> can always be decomposed into the sum of three fundamental quantities: <strong>the variance of <span class="math inline">\(\hat{f}(x_0)\)</span>, the squared bias of <span class="math inline">\(\hat{f}(x_0)\)</span>, and the variance of the error variance terms <span class="math inline">\(\epsilon\)</span>.</strong> <span class="math display">\[\begin{align}E(y_0-\hat{f}(x_0))^2=Var(\hat{f}(x_0))+[Bias(\hat{f}(x_0))]^2+Var(\epsilon)\end{align}\]</span> The overall expected test MSE can be computed by averaging <span class="math inline">\(E(y_0-\hat{f}(x_0))^2\)</span> over all possible values of <span class="math inline">\(x_0\)</span> in the test set.</p><a id="more"></a><p><img src="./2.png" width="600"></p><h3 id="what-is-empirical-risk-minimization">2. What is Empirical Risk Minimization?</h3><p><strong><em>ERM</em></strong>: the function that minimizes loss on the training set.</p><p>In many machine learning task, we have data <span class="math inline">\(Z\)</span> from some distribution <span class="math inline">\(p\)</span> and the task is to minimize the risk: <span class="math display">\[R_(f)=E_{Z \sim p}[loss(f(Z),Z)]\]</span> <strong>Loss function</strong>: In classification <span class="math inline">\(Z = (X, Y )\)</span> and we use 0/1 loss <span class="math inline">\(loss(f(Z), Z) = I_{f(X) \neq Y}\)</span> , in regression $Z = (X, Y ) $and we use squared error <span class="math inline">\(loss(f(Z), Z) = (f(X) − Y )^2\)</span> and in density estimation <span class="math inline">\(Z = X\)</span> and we use negative log likelihood loss <span class="math inline">\(loss(f(Z), Z) = − \log f(X)\)</span>. We are interested in finding the optimal predictor <span class="math display">\[f^*=\arg\min_f R(f)\]</span> In practice, we compute the empirical risk: <span class="math display">\[\hat{R}(f)=\frac{1}{n} \sum_{i=1}^n loss(f(X_i),Y_I)\]</span> We choose the <span class="math inline">\(\hat{f}\)</span> that minimizes the empirical risk over some class <span class="math inline">\(F\)</span>, such as parametric models, histogram classifiers, decision trees or linear/polynomial functions, etc. <span class="math display">\[\hat{f}^{ERM}=\arg \min_{f \in F} \hat{R}(f)\]</span></p><h3 id="write-the-formulae-for-training-error-and-generalization-error.-point-out-the-differences.">3. Write the formulae for training error and generalization error. Point out the differences.</h3><p><strong><em>Generalization error</em></strong> is defined as the expected value of the error on a new input. Here the expectation is taken across different possible inputs, drawn from the distribution of inputs we expect the system to encounter in practice. [DL Book]</p><p><strong><em>Training error:</em></strong> when training a machine learning model, we have access to a training set, we can compute some error measure on the training set called the training error, and we reduce this training error.</p><p>In our linear regression example, we trained the model by minimizing the <em>training error</em>: <span class="math display">\[\frac{1}{m^{(train)}}||\mathbf{X}^{(train)}\mathbf{w}-\mathbf{y}^{(train)}||^2_2\]</span> but we actually care about the <em>test error</em>: <span class="math display">\[\frac{1}{m^{(test)}}||\mathbf{X}^{(test)}\mathbf{w}-\mathbf{y}^{(test)}||^2_2\]</span> <strong>Error v.s. Loss v.s. Risk</strong>:</p><p><strong><em>Error</em></strong> is the difference between the actual / true value (𝜃) and the predicted / estimated value (𝜃̂ ) <span class="math display">\[Error=\theta-\hat{\theta}\]</span> <strong><em>Loss (𝐿)</em></strong> is a measurement of how well our model perform against your <strong>training data</strong>.</p><ul><li><p><strong>Regression Losses</strong></p><ul><li><strong>Mean Square Error/Quadratic Loss/L2 Loss</strong></li></ul><p><span class="math display">\[MSE=\frac{\sum_{i=1}^n(y_i-\hat{y}_i)^2}{n}\]</span></p><ul><li><strong>Mean Absolute Error/L1 Loss</strong></li></ul><p><span class="math display">\[MAE=\frac{\sum_{i=1}^n|y_i-\hat{y}_i|}{n}\]</span></p><ul><li><strong>Mean Bias Error</strong> <span class="math display">\[MBE=\frac{\sum_{i=1}^n(y_i-\hat{y}_i)}{n}\]</span></li></ul></li><li><p><strong>Classification Losses</strong></p><ul><li><strong>Hinge Loss/Multi class SVM Loss</strong> <span class="math display">\[SVM Loss=\sum_{j \neq y_i} \max(0,s_j-s_{y_i}+1) \\L(\mathbf{X},\mathbf{y},\beta)=\sum_{i=1}^n \max[0,1-y_i(\beta_0+\beta_1x_{i1}+,,,+\beta_px_{ip})]\]</span></li></ul></li><li><p><strong>Cross Entropy Loss/Negative Log Likelihood</strong></p></li></ul><p><span class="math display">\[CrossEntropyLoss=-\left[y_i\log(\hat{y_i})+(1-y_i)\log (1-y_i)\right]\]</span></p><p><strong><em>Risk</em></strong> is the average measure of loss, or expected loss, across your whole data distribution. <span class="math display">\[R(\theta,\hat{\theta})=E(L(\theta,\hat{\theta}))\]</span></p><p><strong><em>Empirical Risk:</em></strong> when we train our model, we do not have the full distribution of the data. This may be because some of our data is used for validation and testing, or that new data points are produced in real-time. The best we can do is to pick our training data in a random way and assume that our training data is representative of the real data.</p><p>Therefore, because we don't have all the data, the best we can do is to minimize the empirical risk, from data that we do have (our training data), and use regularization techniques to generalize (i.e. avoid overfitting). This is why minimizing loss and minimizing empirical risk are roughly the same thing.</p><h3 id="what-is-the-bias-variance-trade-off-theorem">4. What is the bias-variance trade-off theorem?</h3><p><strong><em>Bias-variance trade-off</em></strong>: If our model is too simple and has very few parameters then it may have <strong>high bias and low variance</strong>. On the other hand if our model has large number of parameters then it’s going to have <strong>high variance and low bias</strong>. So we need to find the right/good balance without overfitting and underfitting the data.</p><p><img src="./1.png" width="600"></p><p><em>Bias and variance using bulls-eye diagram:</em></p><p><img src="./3.png" width="300"></p><p><strong><em>Underfitting</em></strong> happens when a model unable to capture the underlying pattern of the data <span class="math inline">\(\Rightarrow\)</span> high bias, low variance.</p><p><strong><em>Overfitting</em></strong> happens when our model captures the noise along with the underlying pattern in data. It happens when we train our model a lot over noisy dataset <span class="math inline">\(\Rightarrow\)</span> low bias and high variance.</p><p><img src="./5.png" width="600"></p><h3 id="what-is-the-vc-dimension">5. What is the VC dimension?</h3><p>State the uniform convergence theorem and derive it.</p><p>What is sample complexity bound of uniform convergence theorem?</p><p>What is error bound of uniform convergence theorem?</p><p>From the bias-variance trade-off, can you derive the bound on training set size?What does the training set size depend on for a finite and infinite hypothesis set? Compare and contrast.What is the VC dimension for an n-dimensional linear classifier?How is the VC dimension of a SVM bounded although it is projected to an infinite dimension?Considering that Empirical Risk Minimization is a NP-hard problem, how does logistic regression and SVM loss work?</p><h2 id="model-and-feature-selection">Model and feature selection</h2><h3 id="why-are-model-selection-methods-needed">1. Why are model selection methods needed?</h3><ul><li><p><strong><em>Model Interpretability</em></strong>：Irrelevant variables leads to unnecessary complexity in the resulting model. By removing these variables—that is, by setting the corresponding coefficient estimates to zero—we can obtain a model that is more easily interpreted.</p></li><li>We want to estimate the generalization performance, the predictive performance of our model on future (unseen) data.</li><li>We want to increase the Prediction Accuracy by tweaking the learning algorithm and selecting the best performing model from a given hypothesis space.</li><li>We want to identify the machine learning algorithm that is best-suited for the problem at hand; thus, we want to compare different algorithms, selecting the best-performing one as well as the best performing model from the algorithm’s hypothesis space.</li><li>Fast (to train and test)</li><li><p>Scalable (it can be applied to a large dataset)</p></li></ul><p>The idea of model selection method is intuitive. It answers the following question:</p><blockquote><p><strong><em>How to select the right input variables for an optimal model?</em></strong></p></blockquote><p><strong>An Optimal model is a model that fits the data with best values for the evaluation metrics.</strong></p><p>(Feature selection simplifies a machine learning problem by choosing which subset of the available features should be used.)</p><h3 id="how-do-you-do-a-trade-off-between-bias-and-variance">2. How do you do a trade-off between bias and variance?</h3><p><strong>Shrinkage methods:</strong> By <strong>constraining</strong> or <strong>shrinking</strong> the estimated coefficients, we can often substantially reduce the variance at the cost of a negligible increase in bias.</p><p><strong>Bagging</strong> and other <strong>resampling techniques</strong> can be used to reduce the variance in model predictions.</p><h3 id="why-is-cross-validation-required">3. Why is cross-validation required?</h3><p><strong>Cross-validation</strong>: can be used to estimate the test error associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility.</p><p>Cross Validation is <strong>a very useful technique for assessing the effectiveness of your model, particularly in cases where you need to mitigate overfitting.</strong> You need some kind of <strong>assurance that your model has got most of the patterns from the data correct, and its not picking up too much on the noise, or in other words its low on bias and variance.</strong></p><p>Evaluation of residuals only gives us an idea about how well our model does on data used to train it. So, the problem with this evaluation technique is that <strong>it does not give an indication of how well the learner will generalize to an independent/ unseen data set.</strong> Getting this idea about our model is known as Cross Validation.</p><p>It is <strong>also of use in determining the hyper parameters of your model</strong>, in the sense that which parameters will result in lowest test error.</p><h3 id="describe-different-cross-validation-techniques.">4. Describe different cross-validation techniques.</h3><h4 id="what-is-hold-out-cross-validation-what-are-its-advantages-and-disadvantages">What is hold-out cross validation? What are its advantages and disadvantages?</h4><p><strong>Hold-out cross validation</strong>:</p><ol type="1"><li>Randomly dividing the available set of observations into two parts, a <strong>training set</strong> and a <strong>validation set</strong> or hold-out set.</li><li>The model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set.</li><li>The resulting validation set error rate—typically assessed using MSE in the case of a quantitative response—provides an estimate of the test error rate.</li></ol><p><strong>Advantage</strong>: this method doesn’t take any overhead to compute and is better than traditional validation</p><p><strong>Disadvantage</strong>:</p><ul><li><strong>High variance</strong>: because it is not certain which data points will end up in the validation set and the result might be entirely different for different sets.</li><li>Removing a part of it for validation poses a problem of <strong>underfitting</strong></li></ul><h4 id="what-is-k-fold-cross-validation-what-are-its-advantages-and-disadvantages">What is k-fold cross validation? What are its advantages and disadvantages?</h4><p><strong>K-fold cross validation</strong>:</p><ol type="1"><li>Randomly k-fold CV dividing the set of observations into k groups, or folds, of approximately equal size.</li><li>The first fold is treated as a validation set, and the method is fit on the remaining k − 1 folds.</li><li>The mean squared error, MSE1, is then computed on the observations in the held-out fold. This procedure is repeated k times; each time, a different group of observations is treated as a validation set.</li><li>This process results in k estimates of the test error, MSE1,MSE2, . . . ,MSEk.</li><li>The k-fold CV estimate is computed by averaging these values,</li></ol><p><span class="math display">\[\begin{align}CV_{(k)}=\frac{1}{k}\sum_{i=1}^kMSE_i\end{align}\]</span></p><p><strong>Advantage</strong>: significantly <strong><em>reduces bias</em></strong> as we are using most of the data for fitting, and also significantly <strong><em>reduces variance</em></strong> as most of the data is also being used in validation set.</p><p><strong>Disadvantage</strong>: the training algorithm has to be rerun from scratch <em>k</em> times, which means it takes <em>k</em> times as much computation to make an evaluation.</p><h4 id="what-is-leave-one-out-cross-validation-what-are-its-advantages-and-disadvantages">What is leave-one-out cross validation? What are its advantages and disadvantages?</h4><p><strong>Leave-one-out cross validation</strong>: leaves 1 data points out of training data as validation set. The statistical learning method is fit on the n − 1 training observations, and a prediction <span class="math inline">\(y_1\)</span> is made for the excluded observation, using its value <span class="math inline">\(x_1\)</span>. and then the error is averaged for all trials, to give overall effectiveness.</p><p>Since $ (x_1, y_1)$ was not used in the fitting process, <span class="math inline">\(MSE_1 =(y_1 − \hat{y}_1)^2\)</span> provides an approximately <strong><em>unbiased</em></strong> estimate for the test error.But even though <span class="math inline">\(MSE_1\)</span> is unbiased for the test error, it is a poor estimate because it is <strong><em>highly variable</em></strong>, since it is based upon a single observation $ (x_1, y_1)$.</p><p><strong>Advantages and disadvantages</strong>:</p><ul><li><p>k-Fold more biased than LOOCV; k-Fold less variance than LOOCV</p></li><li>When we perform LOOCV, we are in effect averaging the outputs of n fitted models, each of which is trained on an almost identical set of observations; therefore, these outputs are highly (positively) correlated with each other.</li><li><p>very expensive to compute</p></li></ul><h3 id="why-is-feature-selection-required">5. Why is feature selection required?</h3><ul><li><p><strong>Reduces Overfitting</strong>: Less redundant data means less opportunity to make decisions based on noise.</p></li><li><p><strong>Improves Accuracy:</strong> Less misleading data means modeling accuracy improves.</p></li><li><p><strong>Reduces Training Time</strong>: fewer data points reduce algorithm complexity and algorithms train faster.</p></li></ul><h3 id="describe-some-feature-selection-methods.">6. Describe some feature selection methods.</h3><ul><li><strong>Filter Methods:</strong> apply a statistical measure to assign a scoring to each feature. The features are ranked by the score and either selected to be kept or removed from the dataset.<ul><li>distance metrics, correlation, mutual information, and consistency metrics</li></ul></li><li><strong>Wrapper Methods:</strong> consider the selection of a set of features as a search problem, where different combinations are prepared, evaluated and compared to other combinations. A predictive model is used to evaluate a combination of features and assign a score based on model accuracy.<ul><li>Best Subset, forward and backward, recursive feature elimination</li></ul></li><li><strong>Embedded Methods:</strong> learn which features best contribute to the accuracy of the model while the model is being created.<ul><li>regularization methods: LASSO, Elastic Net and Ridge Regression</li></ul></li></ul><h3 id="what-is-forward-feature-selection-method-what-are-its-advantages-and-disadvantages">7. What is forward feature selection method? What are its advantages and disadvantages?</h3><p><strong>Forward stepwise selection</strong> starts with the intercept, and then sequentially adds into the model the predictor that most improves the ﬁt.</p><p><strong>Advantages and Disadvantages</strong>:</p><ul><li><strong>Computational</strong>: for large p we cannot compute the best subset sequence, but we can always compute the forward stepwise sequence</li><li><strong>Statistical</strong>: a price is paid in variance for selecting the best subset of each size; forward stepwise is a more constrained search, and will have lower variance, but perhaps more bias</li></ul><p><img src="./6.png" width="600"></p><h3 id="what-is-backward-feature-selection-method-what-are-its-advantages-and-disadvantages">8. What is backward feature selection method? What are its advantages and disadvantages?</h3><p><strong>Backward-stepwise selection</strong> starts with the full model, and sequentially deletes the predictor that has the least impact on the ﬁt. The candidate for dropping is the variable with the smallest Z-score</p><p><strong>Advantages and Disadvantages</strong>:</p><ul><li><p>Backward-stepwise selection can only be used when N&gt;p, while forward stepwise can always be used.</p></li><li><p>Like forward stepwise selection, backward stepwise selection is not guaranteed to yield the best model containing a subset of the p predictors.</p></li></ul><p><img src="./7.png" width="600"></p><h3 id="what-is-filter-feature-selection-method-and-describe-two-of-them">9. What is filter feature selection method and describe two of them?</h3><p><strong>Filter Methods:</strong> apply a statistical measure to assign a scoring to each feature. The features are ranked by the score and either selected to be kept or removed from the dataset.</p><ul><li><p><a href="http://nlp.stanford.edu/IR-book/html/htmledition/feature-selectionchi2-feature-selection-1.html" target="_blank" rel="noopener"><strong>Chi-squared</strong></a> <em>:</em>Chi-square test is used for categorical features in a dataset. We calculate Chi-square between each feature and the target and select the desired number of features with best Chi-square scores. to evaluate how likely it is that any observed difference between the sets arose by chance or if the association between two categorical variables of the sample would reflect their real association in the population.</p></li><li><p><a href="http://www.ime.unicamp.br/~wanderson/Artigos/correlation_based_feature_selection.pdf" target="_blank" rel="noopener"><strong>Correlation</strong></a> <strong>:</strong> The Correlation Feature Selection (CFS) measure evaluates subsets of features on the basis of the following hypothesis: &quot;Good feature subsets contain features highly correlated with the classification, yet uncorrelated to each other</p></li><li><p><a href="https://hal.archives-ouvertes.fr/hal-00617969/document" target="_blank" rel="noopener"><strong>Entropy</strong></a> <strong>:</strong> Entropy measures the amount of information in a random variable; it’s the average length of the message needed to transmit an outcome of that variable using the optimal code.</p><ul><li><p><strong>Information content</strong>: <span class="math inline">\(I(E)=-\log [\Pr(E)]=-\log (P)\)</span></p></li><li><p>Define the <strong>entropy</strong> as the expected value of information: <span class="math display">\[H(X)=E[I(X)]=E[-\log (P(X))]=-\sum_{i=1}^nP(x_i)\log (P(x_i))\]</span></p></li></ul></li><li><p><a href="http://www.saedsayad.com/oner.htm" target="_blank" rel="noopener"><strong>One-attribute-rule(OneR)</strong></a> <strong>:</strong> The idea of the OneR (one-attribute-rule) algorithm is to find the one attribute to use that makes fewest prediction errors.</p></li></ul><h3 id="what-is-cross-entropy-and-kl-divergencedescribe-kl-divergence-intuitively.">10. What is Cross Entropy and KL divergence?Describe KL divergence intuitively.</h3><p><strong>Cross entropy</strong> is, at its core, a way of measuring the “distance” between two probability distributions P and Q. As you observed, <strong>entropy</strong> on its own is just a measure of a single probability distribution. As such, if we are trying to find a way to model a <strong>true probability distribution, P,</strong> using, say, a neural network to produce an <strong>approximate probability distribution Q</strong>, then there is the need for some sort of distance or difference measure which can be minimized.</p><p><strong>Cross entropy function</strong>: <span class="math display">\[H(p,q)=H(p)+D_{KL}(p||q)\]</span></p><ul><li>The first term, the <strong>entropy</strong> of the true probability distribution <em>p</em>, during optimization is <strong><em>fixed</em></strong> – it reduces to an additive constant during optimization.</li><li>Only the parameters of the second, approximation distribution, <em>q</em> that can be varied during optimization – and hence <em>the core of the cross entropy measure</em> of distance is the <strong>KL divergence</strong> function.</li></ul><p><strong>KL divergence</strong> is an expression of “surprise” – under the assumption that <em>P</em> and <em>Q</em> are <em>close,</em> it is <em>surprising</em> if it turns out that they are NOT CLOSE, hence in those cases the KL divergence will be high. If they are CLOSE together, then the KL divergence will be low.</p><p><strong>KL divergence</strong> is the i<em>nformation gained</em> when we move from a prior distribution <em>Q</em> to a posterior distribution <em>P</em>.</p><p><strong>Derivation of KL divergence</strong>:</p><p>The expression for KL divergence can also be derived by using a likelihood ratio approach.</p><p><strong>The likelihood ratio</strong> <span class="math display">\[LR=\frac{p(x)}{q(x)}\]</span></p><ul><li><strong>Interpretation</strong>: if a value x is sampled from some unknown distribution, the likelihood ratio expresses how much more likely the sample has come from distribution p than from distribution q. If it is more likely from p, the LR &gt; 1, otherwise if it is more likely from q, the LR &lt; 1.</li></ul><p>Let’s say we have lots of independent samples and we want to estimate the likelihood function taking into account all this evidence – it then becomes: <span class="math display">\[LR=\prod_{i=1}^n\frac{p(x_i)}{q(x_i)}\]</span> If we convert the ratio to <span class="math inline">\(log\)</span>, it’s possible to turn the product in the above definition to a summation: <span class="math display">\[LR=\sum_{i=1}^n\log \frac{p(x_i)}{q(x_i)}\]</span> So now we have the likelihood ratio as a <em>summation</em>. Let’s say we want to answer the question of how much, on average, each sample gives evidence of <span class="math inline">\(p(x)\)</span> over <span class="math inline">\(q(x)\)</span> . To do this, we can take the <em>expected value</em> of the likelihood ratio and arrive at: <span class="math display">\[D_{KL}(P||Q)=\sum_{i=1}^n p(x_i)\log \frac{p(x_i)}{q(x_i)}\]</span> The expression above is the definition of KL divergence. It is basically the expected value of the likelihood ratio – where the likelihood ratio expresses how much more likely the sampled data is from distribution P instead of distribution Q. Another way of expressing the above definition is as follows (using log rules): <span class="math display">\[D_{KL}(P||Q)=\sum_{i=1}^n p(x_i)\log p(x_i)-\sum_{i=1}^n p(x_i)\log q(x_i)\]</span></p><ul><li>The first term in the above equation is the <strong><em>entropy</em></strong> of the distribution P. As you can recall it is the expected value of the <strong><em>information content</em></strong> of P.</li><li>The second term is the information content of Q, but instead weighted by the distribution P.</li><li>This yields the <em>interpretation of the KL divergence</em> to be something like the following: if P is the “true” distribution, then the KL divergence is the amount of <strong>information “lost”</strong> when expressing it via Q.</li></ul><p><strong>Cross entropy</strong>: <span class="math display">\[\begin{align}H(p,q)&amp; =H(p)+D_{KL}(p||q) \\&amp;= -\sum_{i=1}^nP(x_i)\log (P(x_i))+\sum_{i=1}^n p(x_i)\log p(x_i)-\sum_{i=1}^n p(x_i)\log q(x_i) \\&amp;= -\sum_{i=1}^n p(x_i)\log q(x_i)\end{align}\]</span></p><blockquote><p>From DL Book</p></blockquote><p>If we have two separate probability distributions <span class="math inline">\(P(x)\)</span> and <span class="math inline">\(Q(x)\)</span> over the same random variable <span class="math inline">\(x\)</span>, we can measure how different these two distributions are using the <strong>Kullback-Leibler (KL) divergence</strong>: <span class="math display">\[D_{KL}(P||Q)=E_{x \sim P}[\log \frac{P(x)}{Q(x)}]=E_{x \sim P}[\log P(x)- \log Q(x)]\]</span> The <strong>KL divergence</strong> has many useful properties</p><ul><li><strong>Nonnegative</strong>. The KL divergence is 0 if and only if P and Q are the same distribution in the case of discrete variables, or equal “almost everywhere” in the case of continuous variables.</li><li><strong>Asymmetric</strong>: <span class="math inline">\(D_{KL}(P||Q) \neq D_{KL}(Q||P)\)</span>, this asymmetry means that there are important consequences to the choice of whether to use $D_{KL}(P||Q) $ or <span class="math inline">\(D_{KL}(Q||P)\)</span>.</li></ul><p><strong><em>Cross-entropy:</em></strong> $H(P, Q) = H(P) +D_{KL}(P||Q) $ <span class="math display">\[H(P,Q)=-E_{x \sim P} \log Q(x)\]</span> Minimizing the <strong>cross-entropy</strong> with respect to <span class="math inline">\(Q\)</span> is equivalent to minimizing the <strong>KL divergenc</strong>e, because <span class="math inline">\(Q\)</span> does not participate in the omitted term.</p><p><strong>Ref:</strong></p><p><a href="https://github.com/Sroy20/machine-learning-interview-questions" target="_blank" rel="noopener">machine-learning-interview-questions</a></p><p><a href="https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23" target="_blank" rel="noopener">Common Loss functions in machine learning</a></p><p><a href="https://datascience.stackexchange.com/questions/35928/whats-the-difference-between-error-risk-and-loss" target="_blank" rel="noopener">What's the difference between Error, Risk and Loss?</a></p><p><a href="https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229" target="_blank" rel="noopener">Understanding the Bias-Variance Tradeoff</a></p><p><a href="https://adventuresinmachinelearning.com/cross-entropy-kl-divergence/" target="_blank" rel="noopener">An introduction to entropy, cross entropy and KL divergence in machine learning</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;learning-theory&quot;&gt;Learning Theory&lt;/h2&gt;
&lt;h3 id=&quot;describe-bias-and-variance-with-examples.&quot;&gt;1. Describe bias and variance with examples.&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Variance&lt;/em&gt;&lt;/strong&gt;: refers to the amount by which &lt;span class=&quot;math inline&quot;&gt;\(\hat{f}\)&lt;/span&gt; would change if we estimated it using a different training data set. &lt;strong&gt;&lt;em&gt;more flexible statistical methods have higher variance&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explanation: different training data sets will result in a different &lt;span class=&quot;math inline&quot;&gt;\(\hat{f}\)&lt;/span&gt;. But ideally the estimate for f should not vary too much between training sets. However, if a method has high variance then small changes in the training data can result in large changes in &lt;span class=&quot;math inline&quot;&gt;\(\hat{f}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Bias&lt;/em&gt;&lt;/strong&gt;: refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explanation: As we increase the flexibility of a class of methods, the bias tends to initially decrease faster than the variance increases. Consequently, the expected test MSE declines. However, at some point increasing flexibility has little impact on the bias but starts to significantly increase the variance. When this happens the test MSE increases.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Decomposition&lt;/em&gt;&lt;/strong&gt;：The expected test MSE, for a given value &lt;span class=&quot;math inline&quot;&gt;\(x_0\)&lt;/span&gt; can always be decomposed into the sum of three fundamental quantities: &lt;strong&gt;the variance of &lt;span class=&quot;math inline&quot;&gt;\(\hat{f}(x_0)\)&lt;/span&gt;, the squared bias of &lt;span class=&quot;math inline&quot;&gt;\(\hat{f}(x_0)\)&lt;/span&gt;, and the variance of the error variance terms &lt;span class=&quot;math inline&quot;&gt;\(\epsilon\)&lt;/span&gt;.&lt;/strong&gt; &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
E(y_0-\hat{f}(x_0))^2=Var(\hat{f}(x_0))+[Bias(\hat{f}(x_0))]^2+Var(\epsilon)
\end{align}
\]&lt;/span&gt; The overall expected test MSE can be computed by averaging &lt;span class=&quot;math inline&quot;&gt;\(E(y_0-\hat{f}(x_0))^2\)&lt;/span&gt; over all possible values of &lt;span class=&quot;math inline&quot;&gt;\(x_0\)&lt;/span&gt; in the test set.&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Interview" scheme="https://nancyyanyu.github.io/tags/Interview/"/>
    
  </entry>
  
  <entry>
    <title>ISLR Note - SVM: Support Vector Machines</title>
    <link href="https://nancyyanyu.github.io/undefined/a1139410/"/>
    <id>https://nancyyanyu.github.io/undefined/a1139410/</id>
    <published>2019-06-11T23:03:07.000Z</published>
    <updated>2019-06-17T21:03:48.248Z</updated>
    
    <content type="html"><![CDATA[<h1 id="svms-with-kernel">SVMs with Kernel</h1><p>The <strong><em>support vector machine (SVM)</em></strong> is an extension of the support vector classifier that results from enlarging the feature space using <strong>kernels</strong>.</p><p>The <strong><em>solution to the support vector classifier problem</em></strong> involves only the <strong><em>inner products</em></strong> of the observations: <span class="math display">\[\langle x_i,x_{i^{&#39;}} \rangle =\sum_{j=1}^px_{ij}x_{i^{&#39;}j}\]</span> (Details won't be discussed in this note)</p><p>The <strong>linear support vector classifier</strong> can be represented as <span class="math display">\[f(x)=\beta_0+\sum_{i=1}^n \alpha_i \langle x,x_i \rangle\]</span></p><ul><li><span class="math inline">\(α_i\)</span> is nonzero only for the support vectors in the solution—that is, if a training observation is not a support vector, then its <span class="math inline">\(α_i\)</span>equals zero.</li></ul><a id="more"></a><p>So if <span class="math inline">\(S\)</span> is the collection of indices of these support points: <span class="math display">\[f(x)=\beta_0+\sum_{i \in S}^n \alpha_i \langle x,x_i \rangle\]</span> <strong>Generalization</strong>: <em>Kernel</em> <span class="math display">\[K(x_i,x_{i^{&#39;}})\]</span> <strong>Kernel</strong>: Kernel is a function that quantifies the similarity of two observations.</p><ul><li><strong><em>Linear kernel</em></strong>: <span class="math inline">\(K(x_i,x_{i^{&#39;}})=\sum_{j=1}^px_{ij}x_{i^{&#39;}j}\)</span><ul><li>Linear kernel essentially quantifies the similarity of a pair of observations using <strong>Pearson</strong> (standard) correlation.</li></ul></li><li><strong><em>Polynomial kernel</em></strong> of degree d: <span class="math inline">\(K(x_i,x_{i^{&#39;}})=(1+\sum_{j=1}^px_{ij}x_{i^{&#39;}j})^d\)</span><ul><li>fitting a support vector classifier in a higher-dimensional space involving polynomials of degree <span class="math inline">\(d\)</span>.</li></ul></li><li><strong><em>Radial kernel</em></strong>: <span class="math inline">\(K(x_i,x_{i^{&#39;}})=\exp(-\gamma \sum_{j=1}^p(x_{ij}-x_{i^{&#39;}j})^2)\)</span><ul><li>Radial kernel has very <em>local</em> behavior: only nearby training observations have an effect on the class label of a test observation<ul><li>If a given test observation <span class="math inline">\(x^∗ = (x^∗_1 . . .x^∗_p)^T\)</span> is far from a training observation <span class="math inline">\(x_i\)</span> in terms of <strong><em>Euclidean distance</em></strong>; <span class="math inline">\(\Rightarrow\)</span> $ _{j=1}<sup>p(x_{ij}-x_{i</sup>{'}j})^2 $ will be large <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(K(x_i,x_{i^{&#39;}})=\exp(-\gamma \sum_{j=1}^p(x_{ij}-x_{i^{&#39;}j})^2)\)</span> will be very tiny. <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(x_i\)</span> will play virtually no role in <span class="math inline">\(f(x^∗)\)</span>.</li></ul></li></ul></li></ul><p><strong><em>Support Vector Machine</em></strong>: When the support vector classifieris combined with a non-linear kernel, the resulting classifier is known as a support vector machine. <span class="math display">\[f(x)=\beta_0+\sum_{i \in S}^n \alpha_i K(x,x_i)\]</span> <img src="./SVM1.png"></p><p><strong>Advantage of Kernel over enlarging the feature space using functions of the original features: </strong></p><ul><li><strong><em>Computational</em></strong>: one need only compute <span class="math inline">\(K(x_i,x_{i^{&#39;}})\)</span> for all <span class="math inline">\(\left(\begin{array}{c}n\\ 2\end{array}\right)\)</span> distinct pairs <span class="math inline">\(i, i^{&#39;}\)</span>. This can bedone without explicitly working in the <em>enlarged feature space.</em><ul><li><strong>Curse of dimensionality</strong>: for some kernels, such as the radial kernel, the feature space is implicit and infinite-dimensional.</li></ul></li></ul><h1 id="svms-with-more-than-two-classes">SVMs with More than Two Classes</h1><h2 id="one-versus-one-classification">One-Versus-One Classification</h2><p>A <strong><em>one-versus-one</em></strong> or <strong><em>all-pairs</em></strong> approach constructs <span class="math inline">\(\left(\begin{array}{c}K\\ 2\end{array}\right)\)</span> SVMs, each of which compares a pair of classes</p><ol type="1"><li>One such SVM might compare the <span class="math inline">\(k\)</span>-th class, coded as +1, to the <span class="math inline">\(k^{&#39;}\)</span>-th class, codedas −1.</li><li>We classify a test observation using each of the <span class="math inline">\(\left(\begin{array}{c}K\\ 2\end{array}\right)\)</span> classifiers.</li><li>We tally the number of times that the test observation is assigned to each of the K classes.</li><li>The final classification is performed by assigning the test observation to the class to which it was most frequently assigned in these <span class="math inline">\(\left(\begin{array}{c}K\\ 2\end{array}\right)\)</span> pairwise classifications.</li></ol><h2 id="one-versus-all-classification">One-Versus-All Classification</h2><p>The <strong><em>one-versus-all</em></strong> approach:</p><ol type="1"><li>We fit <span class="math inline">\(K\)</span> SVMs, each time comparing one of all the K classes to the remaining K − 1 classes.</li><li>Let <span class="math inline">\(β_{0k}, β_{1k}, . . . , β_{pk}\)</span> denote the parameters that result from fitting an SVM comparing the kth class(coded as +1) to the others (coded as −1).</li><li>Let $ x^∗$ denote a test observation. We assign the observation to the class for which <span class="math inline">\(β_{0k}x_1^*+β_{1k}x_2^*+, . . . ,+ β_{pk}x_p^*\)</span> is largest, as this amounts to a high level of confidence that the test observation belongs to the kth class rather than to any of the other classes.</li></ol><h1 id="relationship-to-logistic-regression">Relationship to Logistic Regression</h1><p>Rewrite the criterion (9.12)–(9.15) [look at last post] for fitting the support vector classifier <span class="math inline">\(f(X) = β_0 + β_1X_1 + . . . + β_pX_p\)</span> as</p><p><span class="math display">\[\min_{\beta_0,...,\beta_p}\left\{ \sum_{i=1}^n\max[0,1-y_If(x_i)]+\lambda\sum_{j=1}^p\beta_j^2 \right\}\]</span></p><ul><li>λ is small: few violations to the margin ; high-variance, low-bias; <span class="math inline">\(\Leftrightarrow\)</span> small <span class="math inline">\(C\)</span>;</li></ul><p><strong>“Loss + Penalty” form</strong>: <span class="math display">\[\min_{\beta_0,...,\beta_p}\left\{ L(\mathbf{X},\mathbf{y},\beta)+\lambda P(\beta) \right\}\]</span></p><ul><li><span class="math inline">\(L(\mathbf{X},\mathbf{y},\beta)\)</span> : loss function</li><li><span class="math inline">\(P(\beta)\)</span>: penalty function</li></ul><p><strong>Ridge regression and the lasso</strong>: <span class="math display">\[L(\mathbf{X},\mathbf{y},\beta)=\sum_{i=1}^n \left( y_i-\beta_0-\sum_{j=1}^p x_{ij}\beta_j \right)^2 \\P(\beta) = \sum_{j=1}^p \beta_j^2 \quad ridge \, regression \\P(\beta) = \sum_{j=1}^p |\beta_j| \quad lasso\]</span> <strong>SVM</strong>: <strong><em>hindge loss</em></strong> <span class="math display">\[L(\mathbf{X},\mathbf{y},\beta)=\sum_{i=1}^n \max[0,1-y_i(\beta_0+\beta_1x_{i1}+,,,+\beta_px_{ip})]\]</span> <img src="SVM2.png"></p><ul><li>Due to thesimilarities between their loss functions, logistic regression and the supportvector classifier often give very similar results.</li><li>When the classes are well separated, SVMs tend to behave better than logistic regression</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;svms-with-kernel&quot;&gt;SVMs with Kernel&lt;/h1&gt;
&lt;p&gt;The &lt;strong&gt;&lt;em&gt;support vector machine (SVM)&lt;/em&gt;&lt;/strong&gt; is an extension of the support vector classifier that results from enlarging the feature space using &lt;strong&gt;kernels&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;&lt;em&gt;solution to the support vector classifier problem&lt;/em&gt;&lt;/strong&gt; involves only the &lt;strong&gt;&lt;em&gt;inner products&lt;/em&gt;&lt;/strong&gt; of the observations: &lt;span class=&quot;math display&quot;&gt;\[
\langle x_i,x_{i^{&amp;#39;}} \rangle =\sum_{j=1}^px_{ij}x_{i^{&amp;#39;}j}
\]&lt;/span&gt; (Details won&#39;t be discussed in this note)&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;linear support vector classifier&lt;/strong&gt; can be represented as &lt;span class=&quot;math display&quot;&gt;\[
f(x)=\beta_0+\sum_{i=1}^n \alpha_i \langle x,x_i \rangle
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(α_i\)&lt;/span&gt; is nonzero only for the support vectors in the solution—that is, if a training observation is not a support vector, then its &lt;span class=&quot;math inline&quot;&gt;\(α_i\)&lt;/span&gt;equals zero.&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="SVM" scheme="https://nancyyanyu.github.io/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>ISLR Note - SVM: Support Vector Classifiers</title>
    <link href="https://nancyyanyu.github.io/undefined/e6eddd64/"/>
    <id>https://nancyyanyu.github.io/undefined/e6eddd64/</id>
    <published>2019-06-11T22:29:11.000Z</published>
    <updated>2019-06-17T20:45:22.502Z</updated>
    
    <content type="html"><![CDATA[<h1 id="overview-of-the-support-vector-classifier">Overview of the Support Vector Classifier</h1><p><strong>Maximal margin hyperplane</strong> is extremely sensitive to a change in a single observation suggests that it may have <strong><em>overfit</em></strong> the training data.</p><p>In this case, we might be willing to consider a classifier based on a hyperplane that does not perfectly separate the two classes, in the interest of</p><ul><li>Greater <em>robustness</em> to individual observations, and</li><li>Better classification of most of the training observations.</li></ul><p><strong><em>Support Vector Classifier (Soft Margin Classifier)</em></strong>: Rather than seeking the largest possible margin that every observation is not only on the correct side of the hyperplane but also on the correct side of the margin, we instead allow some observationsto be on the incorrect side of the margin, or even the incorrect side of the hyperplane.</p><a id="more"></a><p><img src="SVC1.png"></p><h1 id="details-of-the-support-vector-classifier">Details of the Support Vector Classifier</h1><p><strong>Optimization problem</strong>: <span class="math display">\[\max_{\beta_0,...\beta_p,\epsilon_1,..,\epsilon_n} M \\s.t.  \sum_{j=1}^p \beta_j^2=1,  \quad (9.13) \\ y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2},...+\beta_px_{ip})&gt;M(1-\epsilon_i) \quad \forall i=1,..,n.  \quad (9.14) \\ \epsilon_i\geq0,\sum_{i=1}^p\epsilon_i \leq C, \quad (9.15)\]</span></p><ul><li><strong><em>Slack variables</em></strong>: <span class="math inline">\(\epsilon_1,..,\epsilon_n\)</span> - allow individual observations to be on the wrong side of the margin or the hyperplane<ul><li><span class="math inline">\(\epsilon_i=0\)</span>: the i-th observation is on the correct side of the <em>margin</em></li><li><span class="math inline">\(\epsilon_i &gt;0\)</span>: the i-th observation is on the wrong side of the <em>margin</em> <span class="math inline">\(\Rightarrow\)</span> i-th observation <strong><em>violated</em></strong> the margin.</li><li><span class="math inline">\(\epsilon_i &gt;1\)</span>: the i-th observation is on the wrong side of the <em>hyperplane</em></li></ul></li><li>Classify the test observation based on the sign of <span class="math inline">\(f(x^∗) = \beta_0+\beta_1x_{1}^*+\beta_2x_{2}^*,...+\beta_px_{p}^*\)</span>.</li><li><strong><em>Tuning parameter C</em></strong>: <span class="math inline">\(C\)</span> bounds the sum of the <span class="math inline">\(\epsilon_i\)</span>'s, and so it determines the number and severity of the violationsto the margin(and to the hyperplane) that we will tolerate. $<ul><li><strong><em>budget</em></strong> for the amount that the margin can be violated by the <span class="math inline">\(n\)</span> observations.</li><li>Generally chosen via <em>cross-validation</em>.</li><li><span class="math inline">\(C\)</span> controls the <strong>bias-variance trade-off</strong> of the support vector classifier.<ul><li>C is small: a classifier highly fit to the data, fewersupport vectors <span class="math inline">\(\Rightarrow\)</span> low bias , high variance;</li><li>C is large: margin wider, many support vectors <span class="math inline">\(\Rightarrow\)</span> high bias , low variance;</li></ul></li></ul></li></ul><p><img src="SVC2.png"></p><p><strong>Properties</strong>:</p><ul><li>Only observations that either <em>lie on the margin or that violate the margin</em> (<strong>support vectors</strong>) will affect the hyperplane, and hence the classifier obtained.</li><li>The fact that the support vector classifier’s decision rule is based only on a potentially small subset of the training observations (the <strong><em>support vectors</em></strong>) means that it is quite <strong>robust</strong> to the behavior of observations that are far away from the hyperplane.<ul><li>Different from LDA which depends on the mean of <em>all</em> of the observations within each class, as well as the <em>within-class covariance matrix</em> computed using all of the observations</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;overview-of-the-support-vector-classifier&quot;&gt;Overview of the Support Vector Classifier&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Maximal margin hyperplane&lt;/strong&gt; is extremely sensitive to a change in a single observation suggests that it may have &lt;strong&gt;&lt;em&gt;overfit&lt;/em&gt;&lt;/strong&gt; the training data.&lt;/p&gt;
&lt;p&gt;In this case, we might be willing to consider a classifier based on a hyperplane that does not perfectly separate the two classes, in the interest of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Greater &lt;em&gt;robustness&lt;/em&gt; to individual observations, and&lt;/li&gt;
&lt;li&gt;Better classification of most of the training observations.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Support Vector Classifier (Soft Margin Classifier)&lt;/em&gt;&lt;/strong&gt;: Rather than seeking the largest possible margin that every observation is not only on the correct side of the hyperplane but also on the correct side of the margin, we instead allow some observationsto be on the incorrect side of the margin, or even the incorrect side of the hyperplane.&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="SVM" scheme="https://nancyyanyu.github.io/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>ISLR Note - SVM: Maximal Margin Classifier</title>
    <link href="https://nancyyanyu.github.io/undefined/bc53b72b/"/>
    <id>https://nancyyanyu.github.io/undefined/bc53b72b/</id>
    <published>2019-06-11T21:48:50.000Z</published>
    <updated>2019-06-17T20:42:35.085Z</updated>
    
    <content type="html"><![CDATA[<h1 id="what-is-a-hyperplane">What Is a Hyperplane?</h1><p><strong>Hyperplane</strong>: In a p-dimensional space, a hyperplane is a flat affine subspace of dimension <span class="math inline">\(p − 1\)</span>.</p><ul><li>e.g. in two dimensions, a hyperplane is a flat one-dimensional subspace—in other words, a line.</li></ul><p><strong>Mathematical definition of a hyperplane</strong>: <span class="math display">\[\beta_0+\beta_1X_1+\beta_2X_2,...+\beta_pX_p=0, \quad (9.1)\]</span></p><ul><li>Any <span class="math inline">\(X = (X_1,X_2,…X_p)^T\)</span> for which (9.1) holds is a point on the hyperplane.</li></ul><a id="more"></a><h1 id="classification-using-a-separating-hyperplane">Classification Using a Separating Hyperplane</h1><p><strong>Setting</strong>:</p><ul><li><span class="math inline">\(n \times p\)</span> data matrix <span class="math inline">\(X\)</span> that consists of <span class="math inline">\(n\)</span> training observations in p-dimensional space</li><li>These observations fall into two classes—that is, $y_1, . . . , y_n {−1, 1} $.</li><li>Test observation: a p-vector of observed features <span class="math inline">\(x^∗ =\{x^∗_1 . . . x^∗_p\}^T\)</span>.</li></ul><p><strong>Separating hyperplane</strong> has the property that: <span class="math display">\[y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2},...+\beta_px_{ip})&gt;0, \quad (9.8)\]</span></p><ul><li>for all i=1,…,n</li></ul><p>We classify the test observation <span class="math inline">\(x^*\)</span> based on the sign of <span class="math inline">\(f(x^∗) = \beta_0+\beta_1x_{1}^*+\beta_2x_{2}^*,...+\beta_px_{p}^*\)</span>.</p><ul><li>If <span class="math inline">\(f(x^∗)\)</span> is positive, then we assign the test observation to class 1, and if f(x∗) is negative, then we assign it to class −1.</li><li><strong>Magnitude</strong> of <span class="math inline">\(f(x^∗)\)</span>. If <span class="math inline">\(f(x^∗)\)</span>is far from zero, then this means that <span class="math inline">\(x^∗\)</span> lies far from the hyperplane,and so we can be confident about our class assignment for <span class="math inline">\(x^∗\)</span>.</li></ul><p><img src="./S1.png" width="600" aign="middle"></p><h1 id="the-maximal-margin-classifier">The Maximal Margin Classifier</h1><p><strong><em>Margin</em></strong>: the smallest (perpendicular) distance from each training observation to a given separating hyperplane <span class="math inline">\(\Rightarrow\)</span> the minimal distance from the observations to the hyperplane.</p><p><strong><em>Maximal margin hyperplane</em></strong>: the separating hyperplane that is farthest from the training observations.</p><ul><li>The maximal margin hyperplane is the separating hyperplane for which the <em>margin</em> is <strong>largest</strong></li><li>Overfitting when <span class="math inline">\(p\)</span> is large.</li></ul><p><strong><em>Maximal margin classifier</em></strong>: classify a test observation based on which side of the maximal margin hyperplane it lies.</p><p><strong><em>Support vectors</em></strong>: training observations that are equidistant from the maximal margin hyperplane that indicate <em>the width of the margin</em>.</p><ul><li>They “support” the maximal margin hyperplane in the sense vector that if these points were moved slightly then the maximal margin hyperplanewould move as well.</li><li><em>The maximal margin hyperplane depends directly on the support vectors, but not on the other observations</em></li></ul><p><img src="./S2.png" width="600"></p><h1 id="construction-of-the-maximal-margin-classifier">Construction of the Maximal Margin Classifier</h1><p>The maximal margin hyperplane is the solution to the optimization problem <span class="math display">\[\max_{\beta_0,...\beta_p} M \\s.t.  \sum_{j=1}^p \beta_j^2=1,  \quad (9.10) \\ y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2},...+\beta_px_{ip})&gt;M \quad \forall i=1,..,n.  \quad (9.11)\]</span></p><ul><li>The constraint in (9.11) in fact requires that each observation be on the correct side of the hyperplane, with some cushion, provided that <strong>margin</strong> <span class="math inline">\(M\)</span> is positive.)</li><li>The constraint in (9.10) makes sure the perpendicular distance from the i-th observation to the hyperplane is given by</li></ul><p><span class="math display">\[y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2},...+\beta_px_{ip})\]</span></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;what-is-a-hyperplane&quot;&gt;What Is a Hyperplane?&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Hyperplane&lt;/strong&gt;: In a p-dimensional space, a hyperplane is a flat affine subspace of dimension &lt;span class=&quot;math inline&quot;&gt;\(p − 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;e.g. in two dimensions, a hyperplane is a flat one-dimensional subspace—in other words, a line.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Mathematical definition of a hyperplane&lt;/strong&gt;: &lt;span class=&quot;math display&quot;&gt;\[
\beta_0+\beta_1X_1+\beta_2X_2,...+\beta_pX_p=0, \quad (9.1)
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Any &lt;span class=&quot;math inline&quot;&gt;\(X = (X_1,X_2,…X_p)^T\)&lt;/span&gt; for which (9.1) holds is a point on the hyperplane.&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="SVM" scheme="https://nancyyanyu.github.io/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>离开纽约前的一些想法</title>
    <link href="https://nancyyanyu.github.io/undefined/babf682e/"/>
    <id>https://nancyyanyu.github.io/undefined/babf682e/</id>
    <published>2019-06-11T18:17:57.000Z</published>
    <updated>2019-06-11T17:19:31.555Z</updated>
    
    <content type="html"><![CDATA[]]></content>
    
    <summary type="html">
    
      
      
        

      
    
    </summary>
    
      <category term="Journal" scheme="https://nancyyanyu.github.io/categories/Journal/"/>
    
    
  </entry>
  
  <entry>
    <title>ISLR Note - Clustering</title>
    <link href="https://nancyyanyu.github.io/undefined/9c99c8b6/"/>
    <id>https://nancyyanyu.github.io/undefined/9c99c8b6/</id>
    <published>2019-06-11T17:52:06.000Z</published>
    <updated>2019-06-17T22:23:43.457Z</updated>
    
    <content type="html"><![CDATA[<h1 id="clustering-methods">Clustering Methods</h1><p><strong>Clustering</strong> refers to a very broad set of techniques for finding subgroups, or clusters, in a data set.</p><ul><li><p>When we cluster the observations of a data set, we seek to partition them into distinct groups so that the observations within each group are quite similar to each other</p></li><li><p>This is an unsupervised problem because we are trying to discover structure</p></li></ul><a id="more"></a><h2 id="clustering-v.s.-pca">Clustering v.s. PCA</h2><p>Both clustering and PCA seek to simplify the data via a small number of summaries, but their mechanisms are different:</p><ul><li><p>PCA looks to find a low-dimensional representation of the observations that explain a good fraction of the variance;</p></li><li><p>Clustering looks to find homogeneous subgroups among the observations.</p></li></ul><p><strong>Application:market segmentation</strong></p><h1 id="k-means-clustering">K-Means Clustering</h1><p><strong>K-means clustering</strong> is a simple and elegant approach for partitioning a data set into K distinct, <strong><em>non-overlapping</em></strong> clusters.</p><p>The idea behind <strong>K-means clustering</strong> is that a <em>good</em> clustering is one for which the <strong><em>within-cluster</em></strong> <strong><em>variation</em></strong> is as small as possible.</p><p>The <strong>within-cluster variation</strong> for cluster <span class="math inline">\(C_k\)</span> is a measure <span class="math inline">\(W(C_k)\)</span> of the amount by which the observations within a cluster differ from each other.</p><p>Hence we want to solve the problem, <span class="math display">\[\min_{C_1,...,C_K}\left\{ \sum_{i=1}^KW(C_k) \right\}\]</span></p><ul><li>partition the observations into K clusters such that the total within-cluster variation, summed over all K clusters, is <em>as small as possible</em>.</li></ul><p><strong>Define the within-cluster variation</strong>: <strong><em>Euclidean distance</em></strong>: <span class="math display">\[W(C_k)=\frac{1}{|C_k|}\sum_{i,i^{&#39;}\in C_k}\sum_{j=1}^p(x_{ij}-x_{i^{&#39;}j})^2\]</span></p><ul><li>where <span class="math inline">\(|C_k|\)</span> denotes the number of observations in the kth cluster.</li><li>The within-cluster variation for the kth cluster is <em>the sum of all ofthe pairwise squared Euclidean distances between the observations in the kth cluste</em>r, divided by the total number of observations in the kth cluster.</li></ul><h2 id="k-means-clustering-optimization-problem">K-means Clustering Optimization Problem</h2><p><strong>Objective funtion</strong>: <span class="math display">\[\min_{C_1,...,C_K}\left\{ \sum_{i=1}^K\frac{1}{|C_k|}\sum_{i,i^{&#39;}\in C_k}\sum_{j=1}^p(x_{ij}-x_{i^{&#39;}j})^2\right\}\]</span> <img src="./C1.png"></p><p>Algorithm 10.1 is guaranteed to decrease the value of the objective at each step: <span class="math display">\[\frac{1}{|C_k|}\sum_{i,i^{&#39;}\in C_k}\sum_{j=1}^p(x_{ij}-x_{i^{&#39;}j})^2=2\sum_{i\in C_k}\sum_{j=1}^p(x_{ij}-\bar{x}_{kj})^2, \quad (10.12)\]</span></p><ul><li>where <span class="math inline">\(\bar{x}_{kj}=\frac{1}{C_k}\sum_{i \in C_k}x_{ij}\)</span> is the mean for feature <span class="math inline">\(j\)</span> in cluster <span class="math inline">\(C_k\)</span>.</li></ul><p><strong>Step 2(a)</strong> : the <em>cluster means</em> for each feature are the constants that minimize the sum-of-squared deviations</p><p><strong>Step 2(b)</strong> : reallocating the observations can only improve (10.12).</p><p><strong>Local optimum</strong> : This means that as the algorithmis run, the clustering obtained will continually improve until the result no longer changes; the objective will never increase.</p><ul><li>It is important to run the algorithm multiple times from different random initial configurations, because the results obtained will depend on the initial (random) cluster assignmentof each observation in Step 1 of Algorithm 10.1</li></ul><p><img src="./C2.png"></p><h1 id="hierarchical-clustering">Hierarchical Clustering</h1><p><strong>Hierarchical clustering</strong> is an alternative approach which does not require that we commit to a particular choice of <span class="math inline">\(K\)</span>.</p><ul><li>Added advantage over K-means clustering in that it results in an attractive tree-based representation of the observations, called a <strong><em>dendrogram</em></strong>.</li></ul><p><strong>Bottom-up</strong> or <strong>agglomerative</strong> clustering: the most common type of hierarchical clustering.</p><h2 id="interpreting-a-dendrogram">Interpreting a Dendrogram</h2><p><img src="./C4.png"></p><p><img src="./C3.png"></p><p>In the left-hand panel of Figure 10.9, each <em>leaf</em> of the dendrogram representsone of the 45 observations in Figure 10.8.</p><ul><li><p>As we move up the tree, some leaves begin to <strong>fuse</strong> into branches: observations that are similar to each other.</p></li><li>The earlier(lower in the tree) fusions occur, the more similar the groups of observationsare to each other.</li><li>For any two observations, we can look for the point in the tree where branches containing those two observations are first fused.<ul><li>The <em>height</em> of this fusion, as measured on the vertical axis, indicates how different the two observations are.</li></ul></li></ul><p>Thus, observations that fuse at the very bottom of the tree are quite similar to each other, whereas observationsthat fuse close to the top of the tree will tend to be quite different.</p><p><strong>Identifying clusters on the basisof a dendrogram</strong>:</p><p>Make a <em>horizontal cut</em> across the dendrogram, as shown in the center and right-hand panels of Figure 10.9. The distinct sets of observations beneath the cut can be interpreted as clusters.</p><blockquote><p><strong><em>The height of the cut to the dendrogram serves the same role as the K in K-means clustering: it controls the number ofclusters obtained.</em></strong></p></blockquote><p><strong><em>Hierarchical</em></strong> refers to the fact that clusters obtained by cutting the dendrogram at a given height are necessarily <em>nested</em> within the clusters obtained by cutting the <em>dendrogram</em> at any greater height.</p><p><strong>Disadvantage</strong>: hierarchical clustering can sometimes yield worse results than <em>K-means clustering</em> when the assumption of hierarchical structure unrealistic.</p><h2 id="the-hierarchical-clustering-algorithm">The Hierarchical Clustering Algorithm</h2><p><img src="C5.png"></p><p><strong>Explanation:</strong></p><p>The two clusters that are most similar to each other are then fused so that there now are n−1 clusters. Next the two clusters that are most similar to each other arefused again, so that there now are n − 2 clusters. The algorithm proceeds in this fashion until all of the observations belong to one single cluster, and the dendrogram is complete.</p><p><img src="C6.png"></p><blockquote><p>How did we determine that the cluster {5, 7} should be fused with the cluster {8}? - linkage</p></blockquote><p><strong>Linkage</strong> : defines the <em>dissimilarity</em> between two groups of observations.</p><ul><li>Complete, average, single, and centroid</li><li>The dissimilarities computed in Step 2(b)of the hierarchical clustering algorithm will depend on the type of linkage used, as well as on the choice of dissimilarity measure.</li></ul><p><img src="C7.png"></p><h2 id="choice-of-dissimilarity-measure">Choice of Dissimilarity Measure</h2><p><strong>Dissimilarity Measure</strong>:</p><ul><li><strong><em>Euclidean distance</em></strong></li><li><strong><em>Correlation-based distance</em></strong>: considers two observationsto be similar if their features are highly correlated</li></ul><p>In general, careful attention should be paid to the <em>type</em> of data being clustered and the scientific question at hand which determines what type of dissimilarity measureis used for hierarchical clustering.</p><p><strong>Note:</strong> variables should be <strong><em>scaled</em></strong> to have standarddeviation one before the dissimilarity between the observations iscomputed.</p><h1 id="practical-issues-in-clustering">Practical Issues in Clustering</h1><h2 id="small-decisions-with-big-consequences">Small Decisions with Big Consequences</h2><p>In order to perform clustering, some decisions must be made.</p><ul><li>Should the observations or features first be standardized to have mean zero and scaled to have standard deviation one.</li><li>In the case of hierarchical clustering,<ul><li>What dissimilarity measure should be used?</li><li>What type of linkage should be used?</li><li>Where should we cut the dendrogramin order to obtain clusters?</li></ul></li><li>In the case of K-means clustering, how many clusters should we lookfor in the data?\</li></ul><p>We try several different choices, and look for the one withthe most useful or interpretable solution.</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;clustering-methods&quot;&gt;Clustering Methods&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Clustering&lt;/strong&gt; refers to a very broad set of techniques for finding subgroups, or clusters, in a data set.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;When we cluster the observations of a data set, we seek to partition them into distinct groups so that the observations within each group are quite similar to each other&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This is an unsupervised problem because we are trying to discover structure&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Clustering" scheme="https://nancyyanyu.github.io/tags/Clustering/"/>
    
      <category term="Unsupervised" scheme="https://nancyyanyu.github.io/tags/Unsupervised/"/>
    
  </entry>
  
  <entry>
    <title>ISLR Note - PCA</title>
    <link href="https://nancyyanyu.github.io/undefined/9b1a6524/"/>
    <id>https://nancyyanyu.github.io/undefined/9b1a6524/</id>
    <published>2019-06-09T05:40:07.674Z</published>
    <updated>2019-06-17T19:59:51.314Z</updated>
    
    <content type="html"><![CDATA[<h1 id="principal-components-analysis">Principal Components Analysis</h1><p><strong>Principal component analysis (PCA)</strong> refers to the process by which principal components are computed, and the subsequent use of these components in understanding the data. - PCA also serves as a tool for data visualization (visualization of the observations or visualization of the variables).</p><h2 id="what-are-principal-components">What Are Principal Components?</h2><p><strong>PCA</strong> :finds a low-dimensional representation of a data set that contains as much as possible of the <strong>variation</strong></p><p>Each of the dimensions found by PCA is a linear combination of the <span class="math inline">\(p\)</span> features.</p><p><strong><em>The first principal component</em></strong> of a set of features <span class="math inline">\(X_1,X_2, . . . , X_p\)</span> is the normalized linear combination of the features</p><p><span class="math display">\[\begin{align}Z_1=\phi_{11}X_1+\phi_{21}X_2+,,,+\phi_{p1}X_p\end{align}\]</span></p><p>that has the <strong>largest variance</strong>.</p><a id="more"></a><p><strong>Normalized</strong>: <span class="math inline">\(\sum_{j=1}^p \phi_{j1}^2=1\)</span></p><p><strong>Loadings</strong>: <span class="math inline">\(\phi_{11}, . . . , \phi_{p1}\)</span> the loadings of the first principal component; - Together, the loadings make up the principal component loading vector, <span class="math inline">\(\phi_1=(\phi_{11},\phi_{21},...,\phi_{p1})^T\)</span></p><h3 id="compute-the-first-principal-component">Compute the first principal component</h3><ul><li><p>Assume that each of the variables in <span class="math inline">\(X\)</span> has been centered to have mean zero. We then look for the linear combination of the sample feature values of the form <span class="math display">\[\begin{align}z_{i1}=\phi_{11}x_{i1}+\phi_{21}x_{i2}+,,,+\phi_{p1}x_{ip} \quad \quad i=1,2,...,n\end{align}\]</span></p><p>that has largest sample variance, subject to the constraint that <span class="math inline">\(\sum_{j=1}^p \phi_{j1}^2=1\)</span></p></li><li><p>The first principal component loading vector solves the optimization problem <span class="math display">\[\begin{align}\max_{\phi_{11},...,\phi_{p1}}{\left\{ \frac{1}{n} \sum_{i=1}^n \left( \sum_{j=1}^p \phi_{j1}x_{ij}   \right)^2 \right\}} \, subject \, to \, \sum_{j=1}^p \phi_{j1}^2=1\end{align}\]</span></p></li><li><p>Since <span class="math inline">\(\sum_{i=1}^nx_{ij}/n=1\)</span>, the average of the <span class="math inline">\(z_{11}, . . . , z_{n1}\)</span> will be zero as well. Hence the objective that we are maximizing is just the <strong>sample variance</strong> of the <span class="math inline">\(n\)</span> values of zi1</p></li><li><p><strong>Scores</strong>: We refer to <span class="math inline">\(z_{11}, . . . , z_{n1}\)</span> as the scores of the first principal component.</p></li></ul><p><strong>Geometric interpretation</strong>: for the first principal component: The loading vector <span class="math inline">\(\phi_1\)</span> with elements <span class="math inline">\(\phi_{11},\phi_{21},...,\phi_{p1}\)</span> defines a direction in feature space along which the data <strong>vary the most</strong>. If we project the n data points <span class="math inline">\(x_1, . . . , x_n\)</span> onto this direction, the projected values are the principal component scores <span class="math inline">\(z_{11}, . . . , z_{n1}\)</span> themselves.</p><h3 id="compute-the-second-principal-component">Compute the second principal component</h3><p><strong>The second principal component <span class="math inline">\(Z_2\)</span></strong>: the linear combination of <span class="math inline">\(X_1,X_2, . . . , X_p\)</span> that has maximal variance out of all linear combinations that are <strong>uncorrelated with <span class="math inline">\(Z_1\)</span></strong>.</p><p>The second principal component scores <span class="math inline">\(z_{12}, . . . , z_{n2}\)</span> take the form</p><p><span class="math display">\[\begin{align}z_{i2}=\phi_{12}x_{i1}+\phi_{22}x_{i2}+,,,+\phi_{p2}x_{ip} \quad \quad i=1,2,...,n\end{align}\]</span> where <span class="math inline">\(\phi_2\)</span> is the second principal component <strong>loading</strong> vector, with elements <span class="math inline">\(\phi_{12},\phi_{22},...,\phi_{p2}\)</span>.</p><p>It turns out that constraining <span class="math inline">\(Z_2\)</span> to be uncorrelated with <span class="math inline">\(Z_1\)</span> is equivalent to constraining the direction <span class="math inline">\(\phi_2\)</span> to be <strong>orthogonal</strong> (perpendicular) to the direction <span class="math inline">\(\phi_1\)</span>.</p><p>To find <span class="math inline">\(\phi_2\)</span>, we solve a problem similar to (10.3) with <span class="math inline">\(\phi_2\)</span> replacing <span class="math inline">\(\phi_1\)</span>, and with the additional constraint that <span class="math inline">\(\phi_2\)</span> is orthogonal to <span class="math inline">\(\phi_1\)</span></p><p><img src="./1.png" width="600"></p><p><strong>Interpretation:</strong> - 1st loading vector places approximately equal weight on Assault, Murder, and Rape, with much less weight UrbanPop. Hence this component roughly corresponds to a measure of overall rates of serious crimes. - Overall, we see that the crime-related variables (Murder, Assault, and Rape) are located close to each other, and that the UrbanPop variable is far from the other three. - This indicates that the crime-related variables are correlated with each other—states with high murder rates tend to have high assault and rape rates—and that the UrbanPop variable is less correlated with the other three.</p><h2 id="another-interpretation-of-principal-components">Another Interpretation of Principal Components</h2><p><strong>An alternative interpretation for principal components</strong>: principal components provide low-dimensional linear surfaces that are closest to the observations</p><ul><li><p><strong>The first principal component loading vector has a very special property</strong>: it is the line in p-dimensional space that is closest to the n observations (using average squared Euclidean distance as a measure of closeness).</p></li><li><p>The appeal of this interpretation : we seek a single dimension of the data that lies as close as possible to all of the data points, since such a line will likely provide a good summary of the data.</p></li><li><p><strong>The first two principal components</strong> of a data set <strong>span the plane</strong> that is closest to the n observations, in terms of average squared Euclidean distance</p></li><li><p>Together <strong>the first M principal component</strong> score vectors and the first M principal component loading vectors provide the best M-dimensional approximation (in terms of Euclidean distance) to the ith observation <span class="math inline">\(x_{ij}\)</span> . <span class="math display">\[\begin{align}x_{ij} \approx \sum_{m=1}^Mz_{im}\phi_{jm}\end{align}\]</span> (assuming the original data matrix X is column-centered).</p><p>(assuming the original data matrix X is column-centered).</p></li><li><p>When <span class="math inline">\(M = min(n − 1, p)\)</span>, then the representation is exact: <span class="math inline">\(x_{ij} = \sum_{m=1}^Mz_{im}\phi_{jm}\)</span></p></li></ul><h2 id="more-on-pca">More on PCA</h2><h3 id="scaling-the-variables">Scaling the Variables</h3><p>Before PCA is performed, the variables should be <strong>centered to have mean zero</strong>. Furthermore, the results obtained when we perform PCA will also depend on whether the variables have been <strong>individually scaled</strong> (each multiplied by a different constant)</p><p><img src="./2.png" width="600"></p><h3 id="uniqueness-of-the-principal-components">Uniqueness of the Principal Components</h3><p><strong>Each principal component loading vector <span class="math inline">\(\phi_1=(\phi_{11},\phi_{21},...,\phi_{p1})^T\)</span> and the score vectors <span class="math inline">\(z_{11}, . . . , z_{n1}\)</span> is unique, up to a sign flip. </strong> - Two different software packages will yield the same principal component loading vectors and score vectors, although the signs of those loading vectors may differ. - <strong>The signs may differ</strong> because each principal component loading vector specifies a direction in p-dimensional space: flipping the sign has no effect as the direction does not change.</p><h3 id="the-proportion-of-variance-explained">The Proportion of Variance Explained</h3><p><strong>How much of the variance in the data is not contained in the first few principal components?</strong></p><p><strong>Proportion of variance explained (PVE)</strong> by each principal component: - The total variance present in a data set (assuming that the variables have been centered to have mean zero) is defined as</p><p><span class="math display">\[\begin{align}\sum_{j=1}^pVar(X_j)=\sum_{j=1}^p\frac{1}{n}\sum_{i=1}^nx_{ij}^2\end{align}\]</span></p><ul><li>The variance explained by the mth principal component is</li></ul><p><span class="math display">\[\begin{align}\frac{1}{n}\sum_{i=1}^nz_{im}^2=\frac{1}{n}\sum_{i=1}^n \left( \sum_{j=1}^p \phi_{jm}x_{ij} \right)^2\end{align}\]</span></p><ul><li>Therefore, the <strong>PVE of the mth principal component</strong> is given by</li></ul><p><span class="math display">\[\begin{align}\frac{\sum_{i=1}^n \left( \sum_{j=1}^p \phi_{jm}x_{ij} \right)^2}{\sum_{j=1}^p\sum_{i=1}^nx_{ij}^2}\end{align}\]</span></p><p>The PVE of each principal component is a positive quantity. In order to compute the <strong>cumulative PVE</strong> of the first <span class="math inline">\(M\)</span> principal components, we can simply sum (10.8) over each of the first <span class="math inline">\(M\)</span> PVEs. In total, there are <span class="math inline">\(min(n − 1, p)\)</span> principal components, and their PVEs sum to one.</p><p><img src="./3.png" width="600"></p><h3 id="deciding-how-many-principal-components-to-use">Deciding How Many Principal Components to Use</h3><p>We would like to use the smallest number of principal components required to get a good understanding of the data.</p><p><strong>How many principal components are needed?</strong> - We typically decide on the number of principal components required to visualize the data by examining a <strong>scree plot</strong> (Right FIGURE 10.4) - We choose the smallest number of principal components that are required in order to explain a sizable amount of the variation in the data. - We tend to look at the first few principal components in order to find interesting patterns in the data. If no interesting patterns are found in the first few principal components, then further principal components are unlikely to be of interest.</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;principal-components-analysis&quot;&gt;Principal Components Analysis&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Principal component analysis (PCA)&lt;/strong&gt; refers to the process by which principal components are computed, and the subsequent use of these components in understanding the data. - PCA also serves as a tool for data visualization (visualization of the observations or visualization of the variables).&lt;/p&gt;
&lt;h2 id=&quot;what-are-principal-components&quot;&gt;What Are Principal Components?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;PCA&lt;/strong&gt; :finds a low-dimensional representation of a data set that contains as much as possible of the &lt;strong&gt;variation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Each of the dimensions found by PCA is a linear combination of the &lt;span class=&quot;math inline&quot;&gt;\(p\)&lt;/span&gt; features.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;The first principal component&lt;/em&gt;&lt;/strong&gt; of a set of features &lt;span class=&quot;math inline&quot;&gt;\(X_1,X_2, . . . , X_p\)&lt;/span&gt; is the normalized linear combination of the features&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
Z_1=\phi_{11}X_1+\phi_{21}X_2+,,,+\phi_{p1}X_p
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;that has the &lt;strong&gt;largest variance&lt;/strong&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Unsupervised" scheme="https://nancyyanyu.github.io/tags/Unsupervised/"/>
    
      <category term="PCA" scheme="https://nancyyanyu.github.io/tags/PCA/"/>
    
      <category term="Dimension Reduction" scheme="https://nancyyanyu.github.io/tags/Dimension-Reduction/"/>
    
  </entry>
  
  <entry>
    <title>ISLR Note - Bagging,Random_Forest,Boosting</title>
    <link href="https://nancyyanyu.github.io/undefined/e9c631e9/"/>
    <id>https://nancyyanyu.github.io/undefined/e9c631e9/</id>
    <published>2019-06-09T05:36:15.314Z</published>
    <updated>2019-06-15T09:18:37.354Z</updated>
    
    <content type="html"><![CDATA[<h1 id="bagging">Bagging</h1><p><strong>Bootstrap aggregation</strong>, or <strong>bagging</strong>, is a general-purpose procedure for reducing the variance of a statistical learning method, frequently used in the context of decision trees.</p><p><strong>Averaging a set of observations reduces variance</strong>: Recall that given a set of n independent observations Z1, . . . , Zn, each with variance <span class="math inline">\(σ^2\)</span>, the variance of the mean <span class="math inline">\(\bar{Z}\)</span> of the observations is given by <span class="math inline">\(σ^2/n\)</span>. - A natural way to reduce the variance and hence increase the prediction accuracy of a statistical learning method is to <strong>take many training sets from the population</strong>, build a separate prediction model using each training set, and average the resulting predictions.</p><a id="more"></a><p><strong>Bootstrap</strong> taking repeated samples from the (single) training data set</p><p><strong>Bagging</strong> - Generate B different bootstrapped training data sets. - Train our method on the bth bootstrapped training set in order to get <span class="math inline">\(\hat{f}^{*b}(x)\)</span> - Finally average all the predictions, to obtain</p><p><span class="math display">\[\begin{align}\hat{f}_{bag}(x)=\frac{1}{B}\sum_{b=1}^B\hat{f}^{*b}(x)\end{align}\]</span></p><p><strong>Apply bagging to regression trees</strong> - Construct B regression trees using B bootstrapped training sets - Average the resulting predictions. These trees are grown deep, and are not pruned. Hence each individual tree has high variance, but low bias. Averaging these B trees reduces the variance.</p><p><strong>Bagging on Classification Tree</strong> - For a given test observation, we can record the class predicted by each of the B trees, and take a <strong>majority vote</strong>: the overall prediction is the most commonly occurring class among the B predictions.</p><p><strong>B</strong> - In practice we use a value of B sufficiently large that the error has settled down, like B=100.</p><h2 id="out-of-bag-error-estimation">Out-of-Bag Error Estimation</h2><p>Recall that the key to bagging is that trees are repeatedly fit to bootstrapped subsets of the observations. One can show that on average, each bagged tree makes use of around 2/3 of the observations. The remaining one-third of the observations not used to fit a given bagged tree are referred to as the <strong>out-of-bag (OOB)</strong> observations.</p><blockquote><p>We can predict the response for the ith observation using each of the trees inwhich that observation was OOB.</p></blockquote><ul><li>This will yield around B/3 predictions for the ith observation.</li><li>To obtain a single prediction for the ith observation, we can <strong>average</strong> these predicted responses (regression) or can take a <strong>majority vote</strong> (classification).</li><li>This leads to a single OOB prediction for the ith observation.</li></ul><p>The OOB approach for estimating the test error is particularly convenient when performing bagging on large data sets for which <strong>cross-validation</strong> would be computationally onerous.</p><h2 id="variable-importance-measures">Variable Importance Measures</h2><p><strong>Bagging improves prediction accuracy at the expense of interpretability</strong> - When we bag a large number of trees, it is no longer possible to represent the resulting statistical learning procedure using a single tree, and it is no longer clear which variables are most important to the procedure</p><p><strong>Variable Importance</strong> - One can obtain an overall summary of the importance of each predictor using the RSS (for bagging regression trees) or the Gini index (for bagging classification trees). - <strong>Bagging regression trees</strong>: Record the total amount that the RSS is decreased due to splits over a given predictor, averaged over all B trees. A large value indicates an important predictor. <span class="math display">\[\begin{align}RSS=\sum_{j=1}^J\sum_{i \in R_j} (y_i-\hat{y}_{R_j})^2\end{align}\]</span> - <strong>Bagging classification trees</strong>: Add up the total amount that the <strong>Gini index</strong> is decreased by splits over a given predictor, averaged over all B trees.</p><p><img src="./11.png" width="600"></p><h1 id="random-forest">Random Forest</h1><p><strong>Random forests</strong> provide an improvement over bagged trees by way of a small tweak that <strong>decorrelates</strong> the trees.</p><p>As in bagging, we build a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, <em>a random sample of m predictors is chosen as split candidates</em> from the full set of p predictors.</p><p><strong>The split is allowed to use only one of those m predictors.</strong> A fresh sample of m predictors is taken at each split, and typically we choose <span class="math inline">\(m ≈\sqrt{p}\)</span></p><p><strong>Rationale</strong>: - Suppose that there is one very strong predictor in the data set, along with a number of other moderately strong predictors. Then in the collection of bagged trees, most or all of the trees will use this strong predictor in the top split. Consequently, <em>all of the bagged trees will look quite similar to each other.</em> - Hence the predictions from the bagged trees will be highly correlated. Unfortunately, averaging many highly correlated quantities does not lead to as large of a reduction in variance as averaging many uncorrelated quantities.</p><p><strong>Decorrelating</strong> the trees: Random forests forces each split to consider only a subset of the predictors, making the average of the resulting trees less variable and hence more reliable.</p><h1 id="boosting">Boosting</h1><p><strong>Boosting</strong>: another approach for improving the predictions resulting from a decision tree. - Trees are grown <strong>sequentially</strong>: each tree is grown using information from previously grown trees. - Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set.</p><p><img src="./12.png" width="600"></p><p><strong>Idea behind this procedure</strong> - Unlike fitting a single large decision tree to the data, which amounts to fitting the data hard and potentially overfitting, the boosting approach instead <strong>learns slowly</strong>. - Given the current model, we fit a decision tree to the residuals from the model. That is, we fit a tree using the current residuals, rather than the outcome Y , as the response. - We then add this new decision tree into the fitted function in order to update the residuals. Each of these trees can be rather small, with just a few terminal nodes, determined by the parameter <strong>d</strong> in the algorithm. - By fitting small trees to the residuals, we slowly improve <span class="math inline">\(\hat{f}\)</span> in areas where it does not perform well. - The shrinkage parameter <strong>λ</strong> slows the process down even further, allowing more and different shaped trees to attack the residuals.</p><blockquote><p>Note that in boosting, unlike in bagging, the construction of each tree depends strongly on the trees that have already been grown.</p></blockquote><p><strong>Boosting has three tuning parameters:</strong> 1. The number of trees <span class="math inline">\(B\)</span>. 2. The shrinkage parameter <span class="math inline">\(λ\)</span>, a small positive number. This controls the rate at which boosting learns. 3. The number <span class="math inline">\(d\)</span> of splits in each tree, which controls the complexity of the boosted ensemble. Often d = 1 works well, in which case each tree is a <strong>stump</strong>, consisting of a single split. In this case, the boosted ensemble is fitting an <strong>additive model</strong>, since each term involves only a single variable. More generally <span class="math inline">\(d\)</span> is the <strong>interaction depth</strong>, and controls the interaction order of the boosted model, since <span class="math inline">\(d\)</span> splits can involve at most d variables.</p><p><strong>Boosting V.S. Random forests:</strong></p><ul><li>In boosting, because the growth of a particular tree takes into account the other trees that have already been grown, smaller trees are typically sufficient.</li><li>Using smaller trees can aid in interpretability as well; for instance, using <strong>stumps</strong> leads to an additive model.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;bagging&quot;&gt;Bagging&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Bootstrap aggregation&lt;/strong&gt;, or &lt;strong&gt;bagging&lt;/strong&gt;, is a general-purpose procedure for reducing the variance of a statistical learning method, frequently used in the context of decision trees.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Averaging a set of observations reduces variance&lt;/strong&gt;: Recall that given a set of n independent observations Z1, . . . , Zn, each with variance &lt;span class=&quot;math inline&quot;&gt;\(σ^2\)&lt;/span&gt;, the variance of the mean &lt;span class=&quot;math inline&quot;&gt;\(\bar{Z}\)&lt;/span&gt; of the observations is given by &lt;span class=&quot;math inline&quot;&gt;\(σ^2/n\)&lt;/span&gt;. - A natural way to reduce the variance and hence increase the prediction accuracy of a statistical learning method is to &lt;strong&gt;take many training sets from the population&lt;/strong&gt;, build a separate prediction model using each training set, and average the resulting predictions.&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Bagging" scheme="https://nancyyanyu.github.io/tags/Bagging/"/>
    
      <category term="Random Forest" scheme="https://nancyyanyu.github.io/tags/Random-Forest/"/>
    
      <category term="Boosting" scheme="https://nancyyanyu.github.io/tags/Boosting/"/>
    
      <category term="Trees" scheme="https://nancyyanyu.github.io/tags/Trees/"/>
    
  </entry>
  
  <entry>
    <title>ISLR Note - The Basics of Decision Trees</title>
    <link href="https://nancyyanyu.github.io/undefined/6b588a86/"/>
    <id>https://nancyyanyu.github.io/undefined/6b588a86/</id>
    <published>2019-06-09T05:36:08.114Z</published>
    <updated>2019-06-15T09:26:35.035Z</updated>
    
    <content type="html"><![CDATA[<h1 id="regression-trees">Regression Trees</h1><h2 id="predicting-baseball-players-salaries-using-regression-trees">Predicting Baseball Players’ Salaries Using Regression Trees</h2><p><strong>Terminal nodes</strong>: The regions R1, R2, and R3 are known as terminal nodes or leaves of the tree.</p><p><strong>Internal nodes</strong>: The points along the tree where the predictor space is split are referred to as internal nodes.</p><p><strong>Branches</strong>: The segments of the trees that connect the nodes as branches</p><a id="more"></a><p><img src="./2.png" width="500"> <img src="./1.png" width="500"></p><h2 id="prediction-via-stratification-of-the-feature-space">Prediction via Stratification of the Feature Space</h2><p><strong>Process of building a regression tree</strong></p><p><strong>Step 1</strong>: We divide the predictor space—that is, the set of possible values for X1,X2, . . .,Xp—into J distinct and non-overlapping regions, R1,R2, . . . , RJ .</p><p><strong>Step 2</strong>: For every observation that falls into the region Rj, we make the same prediction, which is simply the <em>mean of the response values</em> for the training observations in Rj .</p><h3 id="step-1">Step 1</h3><p><strong>How do we construct the regions R1, . . .,RJ?</strong> - We choose to divide the predictor space into high-dimensional rectangles, or <strong>boxes</strong>, for ease of interpretation of the resulting predictive model.</p><ul><li><p>The goal is to find boxes R1, . . . , RJ that <strong>minimize the RSS</strong>, given by <span class="math display">\[\begin{align}\sum_{j=1}^J\sum_{i \in R_j} (y_i-\hat{y}_{R_j})^2\end{align}\]</span></p><p>where <span class="math inline">\(\hat{y}_{R_j}\)</span> is the mean response for the training observations within the jth box.</p></li></ul><h4 id="recursive-binary-splitting">Recursive Binary Splitting</h4><p><strong>Recursive Binary Splitting</strong>: a <em>top-down, greedy</em> approach - <strong>Top-down</strong>: begins at the top of the tree (at which point all observations belong to a single region) and then successively splits the predictor space; each split is indicated via two new branches further down on the tree. - <strong>Greedy</strong>: at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step.</p><p><strong>Methods</strong>: 1. Select the predictor <span class="math inline">\(X_j\)</span> and the cutpoint <span class="math inline">\(s\)</span> such that splitting the predictor space into the regions <span class="math inline">\({X|X_j &lt; s}\)</span> and <span class="math inline">\({X|X_j ≥ s}\)</span> leads to the greatest possible reduction in RSS - In greater detail, for any <span class="math inline">\(j\)</span> and <span class="math inline">\(s\)</span>, we define the pair of half-planes <span class="math display">\[  \begin{align}  R_1(j, s) = {X|X_j &lt; s} ,\quad R_2(j, s) = {X|X_j ≥ s}  \end{align}  \]</span> and we seek the value of <span class="math inline">\(j\)</span> and <span class="math inline">\(s\)</span> that <strong>minimize</strong> the equation <span class="math display">\[  \begin{align}  \sum_{:x_i \in R_1(j,s)}(y_i-\hat{y}_{R_1})^2+\sum_{:x_i \in R_2(j,s)}(y_i-\hat{y}_{R_2})^2  \end{align}  \]</span> where <span class="math inline">\(\hat{y}_{R_1}\)</span>is the mean response for the training observations in <span class="math inline">\(R_1(j, s)\)</span>,</p><p>where <span class="math inline">\(\hat{y}_{R_1}\)</span>is the mean response for the training observations in <span class="math inline">\(R_1(j, s)\)</span>,</p><p>and we seek the value of <span class="math inline">\(j\)</span> and <span class="math inline">\(s\)</span> that <strong>minimize</strong> the equation <span class="math display">\[  \begin{align}  \sum_{:x_i \in R_1(j,s)}(y_i-\hat{y}_{R_1})^2+\sum_{:x_i \in R_2(j,s)}(y_i-\hat{y}_{R_2})^2  \end{align}  \]</span> where <span class="math inline">\(\hat{y}_{R_1}\)</span>is the mean response for the training observations in <span class="math inline">\(R_1(j, s)\)</span>,</p><p>where <span class="math inline">\(\hat{y}_{R_1}\)</span>is the mean response for the training observations in <span class="math inline">\(R_1(j, s)\)</span>,</p><ol start="2" type="1"><li>Repeat the process, looking for the best predictor and best cutpoint in order to split the data further so as to minimize the RSS within each of the resulting regions.</li></ol><ul><li><p>However, this time, instead of splitting the entire predictor space, we split one of the two previously identified regions.</p></li><li><p>We now have three regions. Again, we look to split one of these three regions further, so as to minimize the RSS.</p></li></ul><ol start="3" type="1"><li>The process continues until a stopping criterion is reached; for instance, we may continue until no region contains more than five observations.</li></ol><p><img src="./3.png" width="600"></p><h3 id="step-2">Step 2</h3><p>Predict the response for a given test observation using the mean of the training observations in the region to which that test observation belongs.</p><h2 id="tree-pruning">Tree Pruning</h2><p>A better strategy is to grow a very large tree <span class="math inline">\(T_0\)</span>, and then <strong>prune</strong> it back in order to obtain a <strong>subtree</strong></p><h3 id="cost-complexity-pruning">Cost complexity pruning</h3><p>a.k.a.: <strong>weakest link pruning</strong></p><p>Consider a sequence of trees indexed by a nonnegative tuning parameter α</p><p><img src="./4.png" width="600"></p><p>For each value of α there corresponds a subtree <span class="math inline">\(T ⊂ T_0\)</span> such that</p><p><span class="math display">\[\begin{align}\sum_{m=1}^T\sum_{i:x_i \in R_m}(y_i − \hat{y}_{R_m})^2 + \alpha|T|  \quad \quad (8.4)\end{align}\]</span> is as small as possible.</p><ul><li><span class="math inline">\(|T|\)</span>: the number of terminal nodes of the tree T ,</li><li><span class="math inline">\(R_m\)</span>: the rectangle (i.e. the subset of predictor space) corresponding to the m-th <strong>terminal node</strong>,</li><li><span class="math inline">\(\hat{y}_{R_m}\)</span>: the predicted response associated with <span class="math inline">\(R_m\)</span>—that is, the mean of the training observations in <span class="math inline">\(R_m\)</span>.</li></ul><p>The tuning parameter <span class="math inline">\(α\)</span> controls a <em>trade-off</em> between the subtree’s <strong>complexity</strong> and its <strong>fit to the training data</strong>. When α = 0, then the subtree T will simply equal T0, because then (8.4) just measures the training error. However, as α increases, there is a price to pay for having a tree with many terminal nodes, and so the quantity (8.4) will tend to be minimized for a smaller subtree.</p><p>Equation 8.4 is reminiscent of the lasso, in which a similar formulation was used in order to control the complexity of a linear model.</p><p><img src="./5.png" width="600"> <img src="./6.png" width="600"></p><h1 id="classification-trees">Classification Trees</h1><p>For a classification tree, - We predict that each observation belongs to the <strong>most commonly occurring class</strong> of training observations in the region to which it belongs. - RSS cannot be used as a criterion for making the binary splits <span class="math inline">\(\Rightarrow\)</span> <strong>classification error rate</strong>.</p><h2 id="classification-error-rate">Classification Error Rate</h2><ul><li>Since we plan to assign an observation in a given region to the most commonly occurring class of training observations in that region, the classification error rate is simply the <strong>fraction of the training observations in that region that do not belong to the most common class</strong>:</li></ul><p><span class="math display">\[\begin{align}E=1-\max_k(\hat{p}_{mk})\end{align}\]</span></p><ul><li><span class="math inline">\(\hat{p}_{mk}\)</span> : the proportion of training observations in the mth region that are from the kth class.</li><li>classification error is not sufficiently sensitive for tree-growing, and in practice two other measures are preferable: <strong>Gini index, cross-entropy.</strong></li></ul><h2 id="gini-index">Gini index</h2><p><span class="math display">\[\begin{align}G=\sum_{k=1}^K\hat{p}_{mk}(1-\hat{p}_{mk})\end{align}\]</span></p><ul><li>A measure of total variance across the K classes. It is not hard to see that the Gini index takes on a small value if all of the <span class="math inline">\(\hat{p}_{mk}\)</span>’s are close to zero or one.</li><li>For this reason the Gini index is referred to as a measure of node <strong>purity</strong>—a small value indicates that a node contains predominantly observations from a single class.</li></ul><h2 id="cross-entropy">Cross-Entropy</h2><p><span class="math display">\[\begin{align}D=-\sum_{k=1}^K\hat{p}_{mk}\log{\hat{p}_{mk}}\end{align}\]</span></p><ul><li>Since 0 ≤ <span class="math inline">\(\hat{p}_{mk}\)</span> ≤ 1, it follows that <span class="math inline">\(0 ≤ −\hat{p}_{mk}\log{\hat{p}_{mk}}\)</span>.</li><li>Cross-entropy will take on a value near zero if the <span class="math inline">\(\hat{p}_{mk}\)</span>’s are all near zero or near one. Therefore, like the Gini index, the cross-entropy will take on a small value if the mth node is <strong>pure</strong>.</li></ul><hr><p><strong>Cross-Entropy v.s. Gini index v.s. Classification Error Rate</strong> - When building a classification tree, either the Gini index or the crossentropy are typically used to evaluate the quality of a particular split, since these two approaches are more sensitive to node purity than is the classification error rate. Any of these three approaches might be used when pruning the tree, but the classification error rate is preferable if prediction accuracy of the final pruned tree is the goal.</p><p><img src="./9.png" width="800"></p><ul><li><strong>A surprising characteristic</strong>: some of the splits yield two terminal nodes that have the same predicted value.</li><li><strong>Why is the split performed at all?</strong> The split is performed because it leads to <strong>increased node purity.</strong></li><li><strong>Why is node purity important?</strong> Suppose that we have a test observation that belongs to the region given by that right-hand leaf. Then we can be pretty certain that its response value is Yes. In contrast, if a test observation belongs to the region given by the left-hand leaf, then its response value is probably Yes, but we are much less certain. Even though the split RestECG&lt;1 does not reduce the classification error, it improves the <strong>Gini index and the cross-entropy</strong>, which are more sensitive to node purity.</li></ul><h1 id="trees-versus-linear-models">Trees Versus Linear Models</h1><p>Linear regression assumes a model of the form <span class="math display">\[\begin{align}f(X)=\beta_0+\sum_{i=1}^p\beta_iX_i\end{align}\]</span> Regression trees assume a model of the form <span class="math display">\[\begin{align}f(X)=\sum_{m=1}^Mc_m \cdot I_{X \in R_m}\end{align}\]</span> where R1, . . .,RM represent a partition of feature space</p><p>where R1, . . .,RM represent a partition of feature space</p><p><strong>Linear regression works better</strong>: If the relationship between the features and the response is well approximated by a linear model; regression tree does not exploit this linear structure.</p><p><strong>Regression tree works better</strong>: If instead there is a highly non-linear and complex relationship between the features and the response.</p><h1 id="advantages-and-disadvantages-of-trees">Advantages and Disadvantages of Trees</h1><p><strong>Advantages of decision trees for regression and classification:</strong></p><p>▲ <strong>Interpretation</strong>: Trees are very <strong>easy to explain</strong> to people. In fact, they are even easier to explain than linear regression!</p><p>▲ Some people believe that decision trees more closely <strong>mirror human decision-making</strong> than do the regression and classification approaches.</p><p>▲ <strong>Visualization</strong>: Trees can be <strong>displayed graphically</strong>, and are easily interpreted even by a non-expert.</p><p>▲ Trees can easily handle qualitative predictors without the need to create dummy variables.</p><p><strong>Disadvantages of decision trees for regression and classification:</strong></p><p>▼ Unfortunately, trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches seen in this book.</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;regression-trees&quot;&gt;Regression Trees&lt;/h1&gt;
&lt;h2 id=&quot;predicting-baseball-players-salaries-using-regression-trees&quot;&gt;Predicting Baseball Players’ Salaries Using Regression Trees&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Terminal nodes&lt;/strong&gt;: The regions R1, R2, and R3 are known as terminal nodes or leaves of the tree.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Internal nodes&lt;/strong&gt;: The points along the tree where the predictor space is split are referred to as internal nodes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Branches&lt;/strong&gt;: The segments of the trees that connect the nodes as branches&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Trees" scheme="https://nancyyanyu.github.io/tags/Trees/"/>
    
  </entry>
  
  <entry>
    <title>ISLR Note - Considerations In High Dimensions.</title>
    <link href="https://nancyyanyu.github.io/undefined/7d03d840/"/>
    <id>https://nancyyanyu.github.io/undefined/7d03d840/</id>
    <published>2019-06-09T05:33:02.541Z</published>
    <updated>2019-06-12T03:02:00.452Z</updated>
    
    <content type="html"><![CDATA[<h1 id="high-dimensional-data">High-Dimensional Data</h1><p><strong>High-dimensional</strong>: Data sets containing more features than observations are often referred to as high-dimensional. - Classical approaches such as least squares linear highregression are not appropriate in this setting</p><h1 id="what-goes-wrong-in-high-dimensions">What Goes Wrong in High Dimensions?</h1><a id="more"></a><ol type="1"><li>When the number of features p is as large as, or &gt;n, least squares cannot be performed.</li></ol><p><strong>Reason</strong>: regardless of whether or not there truly is a relationship between the features and the response, least squares will yield a set of coefficient estimates that result in a perfect fit to the data, such that the residuals are zero. - This perfect fit will almost certainly lead to overfitting of the data - The problem is simple: when p &gt; n or p ≈ n, a simple least squares regression line is too <strong><em>flexible</em></strong> and hence overfits the data.</p><ol start="2" type="1"><li>Examines only the R2 or the training set MSE might erroneously conclude that the model with the greatest number of variables is best. <img src="./12.png" width="650"></li></ol><ul><li><p><strong>Cp, AIC, and BIC</strong> approaches are not appropriate in the high-dimensional setting, because estimating ˆσ2 is problematic.(For instance, the formula for ˆσ2 from Chapter 3 yields an estimate ˆσ2 = 0 in this setting.)</p></li><li><p><strong>Adjusted R2</strong> in the high-dimensional setting is problematic, since one can easily obtain a model with an adjusted R2 value of 1.</p></li></ul><h1 id="regression-in-high-dimensions">Regression in High Dimensions</h1><p><strong>Alternative approaches better-suited to the high-dimensional setting:</strong></p><ul><li>forward stepwise selection</li><li>ridge regression</li><li>the lasso</li><li>principal components regression</li></ul><p><strong>Reason:</strong> these approaches avoid overfitting by using a less flexible fitting approach than least squares.</p><p><strong>Three important points:</strong> (1) regularization or shrinkage plays a key role in high-dimensional problems,</p><ol start="2" type="1"><li><p>appropriate tuning parameter selection is crucial for good predictive performance, and</p></li><li><p>the test error tends to increase as the dimensionality of the problem (i.e. the number of features or predictors) increases, unless the additional features are truly associated with the response.<span class="math inline">\(\Rightarrow\)</span> <strong>curse of dimensionality</strong></p></li></ol><h2 id="curse-of-dimensionality">Curse of dimensionality</h2><p>Adding additional signal features that are truly associated with the response will improve the fitted model; However, adding noise features that are not truly associated with the response will lead to a deterioration in the fitted model.</p><p><strong>Reason</strong>: This is because noise features increase the dimensionality of the problem, exacerbating the risk of overfitting (since noise features may be assigned nonzero coefficients due to chance associations with the response on the training set) without any potential upside in terms of improved test set error.</p><h1 id="interpreting-results-in-high-dimensions">Interpreting Results in High Dimensions</h1><ol type="1"><li>Be cautious in reporting the results obtained when we perform the lasso, ridge regression, or other regression procedures in the high-dimensional setting.</li></ol><ul><li>In the high-dimensional setting, the <strong>multicollinearity</strong> problem is extreme: any variable in the model can be written as a linear combination of all of the other variables in the model. This means that we can never know exactly which variables (if any) truly are predictive of the outcome, and we can never identify the best coefficients for use in the regression.</li></ul><ol start="2" type="1"><li>Be cautious in reporting errors and measures of model fit in the high-dimensional setting</li></ol><ul><li>e.g.: when p &gt; n, it is easy to obtain a useless model that has zero residuals.</li><li><strong>One should never use sum of squared errors, p-values, R2 statistics, or other traditional measures of model fit on the training data as evidence of a good model fit in the high-dimensional setting</strong></li><li>It is important to instead report results on an independent test set, or cross-validation errors. For instance, the MSE or R2 on an independent test set is a valid measure of model fit, but the MSE on the training set certainly is not.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;high-dimensional-data&quot;&gt;High-Dimensional Data&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;High-dimensional&lt;/strong&gt;: Data sets containing more features than observations are often referred to as high-dimensional. - Classical approaches such as least squares linear highregression are not appropriate in this setting&lt;/p&gt;
&lt;h1 id=&quot;what-goes-wrong-in-high-dimensions&quot;&gt;What Goes Wrong in High Dimensions?&lt;/h1&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Regression" scheme="https://nancyyanyu.github.io/tags/Regression/"/>
    
      <category term="Curse of dimensionality" scheme="https://nancyyanyu.github.io/tags/Curse-of-dimensionality/"/>
    
  </entry>
  
  <entry>
    <title>ESL Note - Subset Selection.</title>
    <link href="https://nancyyanyu.github.io/undefined/5c16c99c/"/>
    <id>https://nancyyanyu.github.io/undefined/5c16c99c/</id>
    <published>2019-06-09T05:21:13.119Z</published>
    <updated>2019-06-12T03:02:14.879Z</updated>
    
    <content type="html"><![CDATA[<h1 id="subset-selection">3.3 Subset Selection</h1><h4 id="drawbacks-of-least-squares-estimates">Drawbacks of least squares estimates:</h4><ul><li><em>prediction accuracy</em>: the least squares estimates often have low bias but large variance. Prediction accuracy can sometimes be improved by shrinking or setting some coeﬃcients to zero.By doing so we sacriﬁce a little bit of bias to reduce the variance of the predicted values, and hence may improve the overall prediction accuracy.</li><li><em>interpretation</em>: With a large number of predictors, we often would like to determine a smaller subset that exhibit the strongest eﬀects. In order to get the “big picture,” we are willing to sacriﬁce some of the small details.</li></ul><h2 id="best-subset-selection">3.3.1 Best-Subset Selection</h2><p>Best subset regression ﬁnds for each k ∈{0, 1, 2,...,p} the subset of size k that gives smallest residual sum of squares.</p><p>We choose the smallest model that minimizes an estimate of the expected prediction error.</p><a id="more"></a><figure><img src="./best_subset.png" alt="title"><figcaption>title</figcaption></figure><blockquote><p>FIGURE 3.5.All possible subset models for the prostate cancer example. At each subset size is shown the residual sum-of-squares for each model of that size.</p></blockquote><h2 id="forward--and-backward-stepwise-selection">3.3.2 Forward- and Backward-Stepwise Selection</h2><blockquote><p>Rather than search through all possible subsets (which becomes infeasible for p much larger than 40), we can seek a good path through them.</p></blockquote><h3 id="forward-stepwise-selection">Forward-stepwise selection</h3><p><strong>Forward-stepwise selection</strong> starts with the intercept, and then sequentially adds into the model the predictor that most improves the ﬁt.</p><p>Forward-stepwise selection is a <em>greedy algorithm</em>, producing a nested sequence of models. In this sense it might seem sub-optimal compared to best-subset selection.</p><h4 id="advantages">Advantages:</h4><ul><li><strong>Computational</strong>: for large p we cannot compute the best subset sequence, but we can always compute the forward stepwise sequence</li><li><strong>Statistical</strong>: a price is paid in variance for selecting the best subset of each size; forward stepwise is a more constrained search, and will have lower variance, but perhaps more bias</li></ul><h3 id="backward-stepwise-selection">Backward-stepwise selection</h3><p><strong>Backward-stepwise selection</strong> starts with the full model, and sequentially deletes the predictor that has the least impact on the ﬁt. The candidate for dropping is the variable with the smallest Z-score</p><h3 id="comparison">Comparison</h3><p>Backward-stepwise selection can only be used when N&gt;p, while forward stepwise can always be used.</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;subset-selection&quot;&gt;3.3 Subset Selection&lt;/h1&gt;
&lt;h4 id=&quot;drawbacks-of-least-squares-estimates&quot;&gt;Drawbacks of least squares estimates:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;prediction accuracy&lt;/em&gt;: the least squares estimates often have low bias but large variance. Prediction accuracy can sometimes be improved by shrinking or setting some coeﬃcients to zero.By doing so we sacriﬁce a little bit of bias to reduce the variance of the predicted values, and hence may improve the overall prediction accuracy.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;interpretation&lt;/em&gt;: With a large number of predictors, we often would like to determine a smaller subset that exhibit the strongest eﬀects. In order to get the “big picture,” we are willing to sacriﬁce some of the small details.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;best-subset-selection&quot;&gt;3.3.1 Best-Subset Selection&lt;/h2&gt;
&lt;p&gt;Best subset regression ﬁnds for each k ∈{0, 1, 2,...,p} the subset of size k that gives smallest residual sum of squares.&lt;/p&gt;
&lt;p&gt;We choose the smallest model that minimizes an estimate of the expected prediction error.&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Model Selection" scheme="https://nancyyanyu.github.io/tags/Model-Selection/"/>
    
  </entry>
  
  <entry>
    <title>ESL Note - Shrinkage Methods</title>
    <link href="https://nancyyanyu.github.io/undefined/afb70442/"/>
    <id>https://nancyyanyu.github.io/undefined/afb70442/</id>
    <published>2019-06-09T05:21:02.282Z</published>
    <updated>2019-06-15T22:22:13.989Z</updated>
    
    <content type="html"><![CDATA[<h1 id="shrinkage-methods">3.4 Shrinkage Methods</h1><blockquote><p><em>Subset selection methods</em> are discrete process—variables are either retained or discarded—it often exhibits high variance,and so doesn’t reduce the prediction error of the full model. <em>Shrinkage methods</em> are more continuous, and don’t suﬀer as much from high variability.</p></blockquote><h2 id="ridge-regression">3.4.1 Ridge Regression</h2><p><strong>Ridge regression</strong> shrinks the regression coeﬃcients by imposing a penalty on their size.The ridge coeﬃcients minimize a penalized residual sum of squares: <span class="math display">\[\begin{align}\hat{\beta}^{ridge}=argmin_\beta {\sum_{i=1}^N(y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j)^2+\lambda\sum_{j=1}^p\beta_j^2}\end{align}\]</span> <a id="more"></a></p><ul><li>λ ≥ 0 is a complexity parameter that controls the amount of shrinkage</li></ul><p>Writing the criterion in matrix form:</p><p><span class="math display">\[\begin{align}RSS(\lambda)=(\mathbf{y}-\mathbf{X}\beta)^T(\mathbf{y}-\mathbf{X}\beta)+\lambda\beta^T\beta\end{align}\]</span> The ridge regression solutions: <span class="math display">\[\begin{align}\hat{\beta}^{ridge}=(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}\end{align}\]</span></p><ul><li><span class="math inline">\(\mathbf{I}\)</span> is the p×p identity matrix</li></ul><p>Note: - the ridge regression solution is again a linear function of <span class="math inline">\(\mathbf{y}\)</span>; - The solution adds a positive constant to the diagonal of <span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> before inversion, which makes the problem nonsingular.</p><h3 id="singular-value-decomposition-svd">Singular value decomposition (SVD)</h3><p>The <strong>singular value decomposition (SVD)</strong> of the centered input matrix X gives us some additional insight into the nature of ridge regression. The SVD of the N × p matrix X has the form:</p><p><span class="math display">\[\begin{align}X=UDV^T\end{align}\]</span></p><ul><li>U: N×p orthogonal matrices, with the columns of U spanning the column space of X</li><li>V: p×p orthogonal matrices, the columns of V spanning the row space of X</li><li>D: p×p diagonal matrix, with diagonal entries d1 ≥ d2 ≥···≥ dp ≥ 0 called the singular values of X. If one or more values dj =0,X is singular</li></ul><p>least squares ﬁtted vector:</p><p><span class="math display">\[\begin{align}\mathbf{X}\hat{\beta}^{ls}&amp;=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y} \\&amp;=UDV^T (VD^TU^TUDV^T)^{-1}VD^TU^Ty \\&amp;=UDV^T (VD^TDV^T)^{-1}VD^TU^Ty \\&amp;=UDV^T (V^T)^{-1}D^{-1}(D^T)^{-1}V^{-1}VD^TU^Ty \\&amp;=\mathbf{U}\mathbf{U}^T\mathbf{y}\end{align}\]</span> Note: <span class="math inline">\(\mathbf{U}^T\mathbf{y}\)</span> are the coordinates of y with respect to the orthonormal basis U.</p><p>The ridge solutions: <span class="math display">\[\begin{align}\mathbf{X}\hat{\beta}^{ridge}&amp;=\mathbf{X}(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y} \\&amp;=UD(D^2+\lambda\mathbf{I})^{-1}D^TU^Ty \\&amp;=\sum_{j=1}^p\mathbf{u}_j\frac{d^2_j}{d^2_j+\lambda}\mathbf{u}^T_j\mathbf{y}\end{align}\]</span></p><ul><li><span class="math inline">\(\mathbf{u}_j\)</span> are the columns of U</li></ul><p>Note: ridge regression computes the coordinates of y with respect to the orthonormal basis U. It then shrinks these coordinates by the factors <span class="math inline">\(\frac{d^2_j}{d^2_j+\lambda}\)</span></p><h4 id="what-does-a-small-value-of-d2_j-mean">What does a small value of <span class="math inline">\(d^2_j\)</span> mean?</h4><p>The SVD of the centered matrix X is another way of expressing the <strong>principal components</strong> of the variables in X. The sample covariance matrix is given by <span class="math inline">\(S=X^TX/N\)</span>, we have</p><p><strong>Eigen decomposition of <span class="math inline">\(X^TX\)</span>:</strong></p><p><span class="math display">\[\begin{align}\mathbf{X}^T\mathbf{X}=VD^TU^TUDV^T=VD^2V^T\end{align}\]</span> The eigenvectors <span class="math inline">\(v_j\)</span> (columns of V) are also called the <strong>principal components</strong> (or Karhunen–Loeve) directions of X. The ﬁrst principal component direction <span class="math inline">\(v_1\)</span> has the property that <span class="math inline">\(z_1=Xv_1\)</span> has the largest sample variance amongst all normalized linear combinations of the columns of X, which is:</p><p><span class="math display">\[\begin{align}Var(z_1)=Var(Xv_1)=\frac{d^2_1}{N}\end{align}\]</span> and in fact <span class="math inline">\(z_1=Xv_1=u_1d_1\)</span>. The derived variable <span class="math inline">\(z_1\)</span> is called the ﬁrst principal component of X, and hence <span class="math inline">\(u_1\)</span> is the normalized ﬁrst principal component.Subsequent principal components <span class="math inline">\(z_j\)</span> have maximum variance <span class="math inline">\(\frac{d^2_j}{N}\)</span>, subject to being orthogonal to the earlier ones.</p><p>Hence the small singular values <span class="math inline">\(d_j\)</span> correspond to directions in the column space of X having small variance, and ridge regression shrinks these directions the most.</p><h3 id="eﬀective-degrees-of-freedom">Eﬀective degrees of freedom</h3><p><span class="math display">\[\begin{align}df(\lambda)&amp;=tr[\mathbf{X}(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})^{-1}\mathbf{X}^T] \\&amp;=tr[\mathbf{H}\lambda] \\&amp;=\sum^p_{j=1}\frac{d^2_j}{d^2_j+\lambda}\end{align}\]</span></p><p>This monotone decreasing function of λ is the eﬀective degrees of freedom of the ridge regression ﬁt. Usually in a linear-regression ﬁt with p variables,the degrees-of-freedom of the ﬁt is p, the number of free parameters.</p><p>Note that &gt; df(λ)= p as λ = 0 (no regularization)</p><blockquote><p>df(λ) → 0 as λ →∞.</p></blockquote><h2 id="the-lasso">3.4.2 The Lasso</h2><p>The lasso is a shrinkage method like ridge, with subtle but important differences.The lasso estimate is deﬁned by:</p><p><span class="math display">\[\begin{align}\hat{\beta}^{lasso}&amp;=argmin_\beta\sum_{i=1}^N(y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j)^2 \\&amp; s.t. \sum_{j=1}^p|\beta_j|\leq t\end{align}\]</span> Lasso problem in <em>Lagrangian form</em>: <span class="math display">\[\begin{align}\hat{\beta}^{lasso}&amp;=argmin_\beta\{ \sum_{i=1}^N(y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j)^2+\lambda\sum_{j=1}^p|\beta_j| \}\end{align}\]</span></p><h4 id="difference-with-ridge">Difference with ridge:</h4><p>The L2 ridge penalty <span class="math inline">\(\sum_{j=1}^p\beta_j^2\)</span> is replaced by the L1 lasso penalty <span class="math inline">\(\sum_{j=1}^p|\beta_j|\)</span>. This latter constraint makes the solutions nonlinear in the <span class="math inline">\(y_i\)</span>, and there is no closed form expression as in ridge regression.</p><blockquote><p>t should be adaptively chosen to minimize an estimate of expected prediction error.</p></blockquote><ul><li>if <span class="math inline">\(t&gt;t_0=\sum_{j=1}^p|\hat{\beta_j^{ls}}|\)</span>, then the lasso estimates are the <span class="math inline">\(\hat{\beta_j^{ls}}\)</span></li><li>if <span class="math inline">\(t&gt;t_0/2\)</span>, the least squares coeﬃcients are shrunk by about 50% on average</li></ul><p>The standardized parameter: <span class="math inline">\(s=t/\sum_1^p|\hat{\beta_j}|\)</span></p><ul><li>s=1.0, the lasso coeﬃcients are the least squares estimates</li><li>s-&gt;0, as the lasso coeﬃcients -&gt;0</li></ul><h2 id="discussion-subset-selection-ridge-regression-and-the-lasso">3.4.3 Discussion: Subset Selection, Ridge Regression and the Lasso</h2><ul><li>Ridge regression: does a proportional shrinkage</li><li>Lasso: translates each coeﬃcient by a constant factor λ, truncating at zero --“soft thresholding,”</li><li>Best-subset selection: drops all variables with coeﬃcients smaller than the Mth largest --“hard-thresholding.” <img src="./images/lass_ridge.png" width="550"></li></ul><h3 id="bayes-view">Bayes View</h3><p>Consider the criterion</p><p><span class="math display">\[\begin{align}\tilde{\beta}&amp;=argmin_\beta\{ \sum_{i=1}^N(y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j)^2+\lambda\sum_{j=1}^p|\beta_j|^q \}\end{align}\]</span> for q ≥ 0. The contours of constant value of <span class="math inline">\(\sum_{j=1}^p|\beta_j|^q\)</span> are shown in Figure 3.12, for the case of two inputs. &lt;img src=&quot;./images/q.png&quot;,width=550&gt;</p><p><font color="red">The lasso, ridge regression and best subset selection are Bayes estimates with diﬀerent priors:</font>Thinking of <span class="math inline">\(\sum_{j=1}^p|\beta_j|^q\)</span> as the log-prior density for βj , these are also the equi-contours of the prior distribution of the parameters.</p><ul><li>q = 0 :variable subset selection, as the penalty simply counts the number of nonzero parameters;</li><li><p>q = 1 :the lasso, also Laplace distribution for each input, with density <span class="math inline">\(\frac{1}{2\tau}exp(-|\beta|/\tau)\)</span>, where <span class="math inline">\(\tau=1/\lambda\)</span></p></li><li><p>q = 2 :the ridge</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;shrinkage-methods&quot;&gt;3.4 Shrinkage Methods&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Subset selection methods&lt;/em&gt; are discrete process—variables are either retained or discarded—it often exhibits high variance,and so doesn’t reduce the prediction error of the full model. &lt;em&gt;Shrinkage methods&lt;/em&gt; are more continuous, and don’t suﬀer as much from high variability.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;ridge-regression&quot;&gt;3.4.1 Ridge Regression&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Ridge regression&lt;/strong&gt; shrinks the regression coeﬃcients by imposing a penalty on their size.The ridge coeﬃcients minimize a penalized residual sum of squares: &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
\hat{\beta}^{ridge}=argmin_\beta {\sum_{i=1}^N(y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j)^2+\lambda\sum_{j=1}^p\beta_j^2}
\end{align}
\]&lt;/span&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Ridge" scheme="https://nancyyanyu.github.io/tags/Ridge/"/>
    
      <category term="Lasso" scheme="https://nancyyanyu.github.io/tags/Lasso/"/>
    
  </entry>
  
  <entry>
    <title>ISLR Note - Dimension Reduction-PCA, PCR</title>
    <link href="https://nancyyanyu.github.io/undefined/cac93a23/"/>
    <id>https://nancyyanyu.github.io/undefined/cac93a23/</id>
    <published>2019-06-09T05:20:51.879Z</published>
    <updated>2019-06-17T20:04:09.445Z</updated>
    
    <content type="html"><![CDATA[<h1 id="intro-to-dimension-reduction-methods">Intro to Dimension Reduction Methods</h1><p>Subset selection and shrinkage methods all use the original predictors, X1,X2, . . . , Xp.</p><p>Dimension Reduction Methods <strong><em>transform</em></strong> the predictors and then fit a least squares model using the transformed variables.</p><h2 id="approach">Approach</h2><p>Let <span class="math inline">\(Z_1,Z_2, . . . ,Z_M\)</span> represent <span class="math inline">\(M &lt; p\)</span> linear combinations of our original <span class="math inline">\(p\)</span> predictors. That is,</p><p><span class="math display">\[\begin{align}Z_m=\sum_{j=1}^p\phi_{jm}X_j\end{align}\]</span> <a id="more"></a></p><p>for some constants <span class="math inline">\(φ_{1m}, φ_{2m} . . . , φ_{pm}, m = 1, . . .,M.\)</span> We can then fit the linear regression model <span class="math display">\[\begin{align}y_i=\theta_0+\sum_{m=1}^M\theta_m z_{im}+\epsilon_i  \quad  i=1,2,3,4,...,n\end{align}\]</span> <strong>Dimension reduction</strong>: reduces the problem of estimating the <span class="math inline">\(p+1\)</span> coefficients <span class="math inline">\(β_0, β_1, . . . , β_p\)</span> to the simpler problem of estimating the <span class="math inline">\(M + 1\)</span> coefficients <span class="math inline">\(θ_0, θ_1, . . . , θ_M\)</span>, where M &lt; p. In other words, the dimension of the problem has been reduced from <span class="math inline">\(p + 1\)</span> to <span class="math inline">\(M + 1\)</span>. <span class="math display">\[\begin{align}\sum_{m=1}^M\theta_m z_{im}&amp;=\sum_{m=1}^M\theta_m \sum_{j=1}^p\phi_{jm}x_{ij}=\sum_{m=1}^M\sum_{j=1}^p\theta_m \phi_{jm}x_{ij}=\sum_{j=1}^p \beta_jx_{ij}  \\\beta_j&amp;=\sum_{m=1}^M\theta_m \phi_{jm}\end{align}\]</span> <strong>All dimension reduction methods work in two steps:</strong></p><ol type="1"><li>the transformed predictors <span class="math inline">\(Z_1,Z_2, . . . ,Z_M\)</span>are obtained.</li><li>the model is fit using these M predictors. However, the choice of <span class="math inline">\(Z_1,Z_2, . . . ,Z_M\)</span>, or equivalently, the selection of the <span class="math inline">\(φ_{jm}\)</span>’s, can be achieved in different ways.</li></ol><h1 id="principal-components-regression">Principal Components Regression</h1><h2 id="an-overview-of-principal-components-analysis">An Overview of Principal Components Analysis</h2><p><strong>PCA</strong>: is a technique for reducing the dimension of a <span class="math inline">\(n × p\)</span> data matrix <span class="math inline">\(X\)</span>.</p><h3 id="st-principal-component">1st Principal Component</h3><h4 id="interpretation-1-greatest-variability">Interpretation 1: greatest variability</h4><p><strong>The first principal component</strong> direction of the data: is that along which the observations <strong>vary the most</strong>.</p><p><img src="./7.png" width="600"></p><p>The first principal component direction is the direction along which there is the greatest variability in the data. That is, if we projected the 100 observations onto this line (as shown in the left-hand panel of Figure 6.15), then the resulting projected observations would have the largest possible variance</p><p><img src="./8.png" width="600"></p><p>The first principal component is given by the formula</p><p><span class="math display">\[\begin{align}Z_1 = 0.839 × (pop − \bar{pop}) + 0.544 × (ad − \bar{ad})\end{align}\]</span> Here <span class="math inline">\(φ_{11} = 0.839\)</span> and <span class="math inline">\(φ_{21} = 0.544\)</span> are the <strong>principal component loadings</strong>, which define the direction referred to above.</p><blockquote><p>The idea is that out of every possible linear combination of pop and ad such that <span class="math inline">\(\phi_{11}^2+\phi_{21}^2=1\)</span>, this particular linear combination yields the highest variance: i.e. this is the linear combination for which <span class="math inline">\(Var(φ_{11} × (pop − \bar{pop}) + φ_{21} × (ad − \bar{ad}))\)</span> is maximized.</p></blockquote><p><strong>Principal Component Scores</strong></p><p>The values of <span class="math inline">\(z_{11}, . . . , z_{n1}\)</span> are known as the <strong>principal component scores</strong>, and can be seen in the right-hand panel of Figure 6.15. For example, <span class="math display">\[\begin{align}z_{i1} = 0.839 × (pop_i − \bar{pop}) + 0.544 × (ad_i − \bar{ad})\end{align}\]</span></p><h4 id="interpretation-2-closest-to-data">Interpretation 2: closest to data</h4><p>There is also another interpretation for PCA: the first principal component vector defines the line that is as close as possible to the data.</p><p>In Figure 6.14, the first principal component line minimizes the sum of the squared perpendicular distances between each point and the line.</p><p>In the right-hand panel of Figure 6.15, the left-hand panel has been rotated so that the first principal component direction coincides with the x-axis. It is possible to show that the <strong><em>first principal component score</em></strong> for the ith observation is the distance in the <span class="math inline">\(x\)</span>-direction of the ith cross from zero.</p><h4 id="interpretation-3-single-number-summarization">Interpretation 3: single number summarization</h4><p>We can think of the values of the principal component <span class="math inline">\(Z_1\)</span> as single number summaries of the joint pop and ad budgets for each location.</p><p>In this example, if <span class="math inline">\(z_{i1} = 0.839 × (pop_i − pop) + 0.544 × (ad_i − ad) &lt; 0\)</span>, then this indicates a city with below-average population size and belowaverage ad spending.</p><p><img src="./9.png" width="650"></p><p>Figure 6.16 displays <span class="math inline">\(z_{i1}\)</span> versus both pop and ad. The plots show a strong relationship between the first principal component and the two features. In other words, the first principal component appears to <em>capture most of the information</em> contained in the pop and ad predictors.</p><h3 id="nd-principal-component">2nd Principal Component</h3><p>The second principal component <span class="math inline">\(Z_2\)</span> is a linear combination of the variables that is uncorrelated with <span class="math inline">\(Z_1\)</span>, and has largest variance subject to this constraint.</p><p>It turns out that the zero correlation condition of <span class="math inline">\(Z_1\)</span> with <span class="math inline">\(Z_2\)</span> is equivalent to the condition that the direction must be perpendicular, or orthogonal, to the first principal component direction.</p><p>The second principal component is given by the formula:</p><p><span class="math display">\[\begin{align}Z_2 = 0.544 × (pop − \bar{pop}) − 0.839 × (ad − \bar{ad}).\end{align}\]</span> Figure 6.15. The fact that the second principal component scores are much closer to zero indicates that this component captures far less information.</p><h2 id="the-principal-components-regression-approach">The Principal Components Regression Approach</h2><p>The principal components regression (PCR) approach involves constructing the first M principal components, <span class="math inline">\(Z_1,Z_2, . . . ,Z_M\)</span>, and then using these components as the predictors in a linear regression model that is fit using least squares</p><p><strong>The key idea</strong></p><p>Often a small number of principal components suffice to explain most of the variability in the data, as well as the relationship with the response. In other words, we assume that <strong><em>the directions in which <span class="math inline">\(X_1, . . .,X_p\)</span> show the most variation are the directions that are associated with <span class="math inline">\(Y\)</span></em></strong></p><p><strong>Example</strong>:</p><p><img src="./10.png" width="650"></p><ul><li>Performing PCR with an appropriate choice of M can result in a substantial improvement over least squares</li><li>PCR does not perform as well as the two shrinkage methods<ul><li><strong>Reason</strong>: The data were generated in such a way that many principal components are required in order to adequately model the response. In contrast, PCR will tend to do well in cases when the first few principal components are sufficient to capture most of the variation in the predictors as well as the relationship with the response.</li></ul></li></ul><p><strong>Note</strong>: even though PCR provides a simple way to perform regression using <span class="math inline">\(M &lt; p\)</span> predictors, it is not a <em>feature selection</em> method!</p><ul><li>This is because each of the <span class="math inline">\(M\)</span> principal components used in the regression is a linear combination of all p of the original features.</li><li>PCR is more closely related to ridge regression than to the lasso. One can even think of ridge regression as a continuous version of PCR!</li></ul><p><strong>Cross-validation</strong>: In PCR, the number of principal components, <span class="math inline">\(M\)</span>, is typically chosen by cross-validation.</p><p><img src="./11.png" width="650"></p><p><strong>Standardisation</strong>: When performing PCR, we generally recommend standardizing each predictor, prior to generating the principal components. - In the absence of standardization, the <em>high-variance variables</em> will tend to play a larger role in the principal components obtained, and the scale on which the variables are measured will ultimately have an effect on the final PCR model.</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;intro-to-dimension-reduction-methods&quot;&gt;Intro to Dimension Reduction Methods&lt;/h1&gt;
&lt;p&gt;Subset selection and shrinkage methods all use the original predictors, X1,X2, . . . , Xp.&lt;/p&gt;
&lt;p&gt;Dimension Reduction Methods &lt;strong&gt;&lt;em&gt;transform&lt;/em&gt;&lt;/strong&gt; the predictors and then fit a least squares model using the transformed variables.&lt;/p&gt;
&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;
&lt;p&gt;Let &lt;span class=&quot;math inline&quot;&gt;\(Z_1,Z_2, . . . ,Z_M\)&lt;/span&gt; represent &lt;span class=&quot;math inline&quot;&gt;\(M &amp;lt; p\)&lt;/span&gt; linear combinations of our original &lt;span class=&quot;math inline&quot;&gt;\(p\)&lt;/span&gt; predictors. That is,&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
Z_m=\sum_{j=1}^p\phi_{jm}X_j
\end{align}
\]&lt;/span&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Model Selection" scheme="https://nancyyanyu.github.io/tags/Model-Selection/"/>
    
      <category term="PCA" scheme="https://nancyyanyu.github.io/tags/PCA/"/>
    
      <category term="Dimension Reduction" scheme="https://nancyyanyu.github.io/tags/Dimension-Reduction/"/>
    
  </entry>
  
</feed>
