<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Nancy&#39;s Notes</title>
  
  <subtitle>Code changes world!</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://nancyyanyu.github.io/"/>
  <updated>2019-10-19T23:01:12.678Z</updated>
  <id>https://nancyyanyu.github.io/</id>
  
  <author>
    <name>Nancy Yan</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Resampling Methods - Cross Validation, Bootstrap</title>
    <link href="https://nancyyanyu.github.io/posts/6d11b2f4/"/>
    <id>https://nancyyanyu.github.io/posts/6d11b2f4/</id>
    <published>2019-10-19T23:01:12.678Z</published>
    <updated>2019-10-19T23:01:12.678Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Resampling methods</strong>:involve repeatedly drawing samples from a training set and refitting a mode of interest on each sample in order to obtain additional information about the fitted model.</p><p><strong>model assessment</strong>： The process of evaluating a model’s performance</p><p><strong>model selection</strong>：The process of selecting the proper level of flexibility for a model</p><p><strong>cross-validation</strong>: can be used to estimate the test error associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility.</p><p><strong>bootstrap</strong>:provide a measure of accuracy of a parameter estimate or of a given selection statistical learning method.</p><a id="more"></a><h1 id="cross-validation">Cross Validation</h1><h2 id="the-validation-set-approach">The Validation Set Approach</h2><p><strong>The Validation Set Approach</strong>:</p><ol type="1"><li><p>Randomly dividing the available set of observations into two parts, a <strong>training set</strong> and a <strong>validation set</strong> or hold-out set.</p></li><li><p>The model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set.</p></li><li><p>The resulting validation set error rate—typically assessed using MSE in the case of a quantitative response—provides an estimate of the test error rate.</p></li></ol><p><strong>Disadvantage</strong>:</p><ol type="1"><li><p>The validation estimate of the test error rate can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set.</p></li><li><p>In the validation approach, only a subset of the observations—those that are included in the training set rather than in the validation set—are used to fit the model. Since statistical methods tend to perform worse when trained on <em>fewer observations</em>, this suggests that the validation set error rate may tend to overestimate the test error rate for the model fit on the entire data set.</p></li></ol><h2 id="k-fold-cross-validation">K-Fold Cross-Validation</h2><p><strong>Approach</strong>:</p><ol type="1"><li><p>Randomly k-fold CV dividing the set of observations into k groups, or folds, of approximately equal size.</p></li><li><p>The first fold is treated as a validation set, and the method is fit on the remaining k − 1 folds.</p></li><li><p>The mean squared error, MSE1, is then computed on the observations in the held-out fold. This procedure is repeated k times; each time, a different group of observations is treated as a validation set.</p></li><li><p>This process results in k estimates of the test error, MSE1,MSE2, . . . ,MSEk.</p></li><li><p>The k-fold CV estimate is computed by averaging these values,</p></li></ol><p><span class="math display">\[\begin{align}CV_{(k)}=\frac{1}{k}\sum_{i=1}^kMSE_i\end{align}\]</span></p><p><strong>Goal</strong>：</p><ol type="1"><li><p>Determine how well a given statistical learning procedure can be expected to perform on independent data</p></li><li><p>We are interested only in the location of the minimum point in the estimated test MSE curve. This is because we might be performing cross-validation on a number of statistical learning methods, or on a single method using different levels of flexibility, in order to identify the method that results in the lowest test error.</p></li></ol><h2 id="bias-variance-trade-off-for-k-fold-cross-validation">Bias-Variance Trade-Off for k-Fold Cross-Validation</h2><p><strong>Leave-One-Out Cross-Validation V.S. k-Fold Cross-Validation</strong>: - k-Fold more biased than LOOCV - LOOCV will give approximately unbiased estimates of the test error, since each training set contains n − 1 observations, which is almost as many as the number of observations in the full data set. - k-fold CV for, say, k = 5 or k = 10 will lead to an intermediate level of bias</p><ul><li>k-Fold less variance than LOOCV</li><li>When we perform LOOCV, we are in effect averaging the outputs of n fitted models, each of which is trained on an almost identical set of observations; therefore, these outputs are highly (positively) correlated with each other.</li><li>the mean of many highly correlated quantities has higher variance than does the mean of many quantities that are not as highly correlated</li></ul><h1 id="bootstrap">Bootstrap</h1><p><strong>Approach</strong>:</p><ol type="1"><li>A data set, which we call Z, that contains n observations. We randomly select n observations from the data set in order to produce a bootstrap data set, <span class="math inline">\(Z^{∗1}\)</span>.</li><li>The sampling is performed with <strong>replacement</strong>, which means that the replacement same observation can occur more than once in the bootstrap data set.</li></ol><ul><li>In this example, <span class="math inline">\(Z^{∗1}\)</span> contains the third observation twice, the first observation once, and no instances of the second observation.</li><li>Note that if an observation is contained in <span class="math inline">\(Z^{∗1}\)</span>, then both its X and Y values are included.</li></ul><ol start="3" type="1"><li><p>We can use <span class="math inline">\(Z^{∗1}\)</span> to produce a new bootstrap estimate for α, which we call <span class="math inline">\(\alpha^{∗1}\)</span>. This procedure is repeated B times for some large value of B, in order to produce B different bootstrap data sets, <span class="math inline">\(Z^{∗1}\)</span>,<span class="math inline">\(Z^{∗2}\)</span>, . . . , <span class="math inline">\(Z^{∗B}\)</span>, and B corresponding α estimates, <span class="math inline">\(\alpha^{∗1}\)</span>, <span class="math inline">\(\alpha^{∗2}\)</span>, . . . , <span class="math inline">\(\alpha^{∗B}\)</span>.</p></li><li><p>We can compute the standard error of these bootstrap estimates using the formula <span class="math display">\[\begin{align}SE_B(\hat{\alpha})=\sqrt{\frac{1}{B-1}\sum_{i=1}^B\left( \hat{\alpha}^{*i}-\frac{1}{B}\sum^{B}_{j=1}\hat{\alpha}^{*j} \right)}\end{align}\]</span> This serves as an estimate of the standard error of <span class="math inline">\(\hat{\alpha}\)</span> estimated from the original data set.</p><p>This serves as an estimate of the standard error of <span class="math inline">\(\hat{\alpha}\)</span> estimated from the original data set.</p></li></ol><p><img src="./1.png" width="600"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Resampling methods&lt;/strong&gt;:involve repeatedly drawing samples from a training set and refitting a mode of interest on each sample in order to obtain additional information about the fitted model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;model assessment&lt;/strong&gt;： The process of evaluating a model’s performance&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;model selection&lt;/strong&gt;：The process of selecting the proper level of flexibility for a model&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;cross-validation&lt;/strong&gt;: can be used to estimate the test error associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;bootstrap&lt;/strong&gt;:provide a measure of accuracy of a parameter estimate or of a given selection statistical learning method.&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Cross Validation" scheme="https://nancyyanyu.github.io/tags/Cross-Validation/"/>
    
      <category term="Model Selection" scheme="https://nancyyanyu.github.io/tags/Model-Selection/"/>
    
  </entry>
  
  <entry>
    <title>Comparing Logistic Regression, LDA, QDA, and KNN</title>
    <link href="https://nancyyanyu.github.io/posts/6084c2b2/"/>
    <id>https://nancyyanyu.github.io/posts/6084c2b2/</id>
    <published>2019-10-19T23:00:54.556Z</published>
    <updated>2019-10-19T23:00:54.556Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="logistic-regression-and-lda-methods-are-closely-connected.">Logistic regression and LDA methods are closely connected.</h2><p><strong>Setting</strong>: Consider the two-class setting with \(p = 1\) predictor, and let \(p_1(x)\) and \(p_2(x) = 1−p_1(x)\) be the probabilities that the observation \(X = x\) belongs to class 1 and class 2, respectively.</p><p>In LDA, from</p><p><span class="math display">\[\begin{align} p_k(x)=\frac{\pi_k \frac{1}{\sqrt{2\pi}\sigma}\exp{\left( -\frac{1}{2\sigma^2}(x-\mu_k)^2 \right)}}{\sum_{l=1}^K\pi\_l\frac{1}{\sqrt{2\pi}\sigma}\exp{\left( -\frac{1}{2\sigma^2}(x-\mu_l)^2 \right)}} \end{align}\]</span></p><p><span class="math display">\[\begin{align} \delta\_k(x)=x\frac{\\mu\_k}{\sigma^2}-\frac{\mu_k^2}{2\sigma^2}+\log(\pi_k) \end{align}\]</span> The <strong>log odds</strong> is given by</p><p><span class="math display">\[\begin{align}\log{\frac{p_1(x)}{1-p_1(x)}}=\log{\frac{p_1(x)}{p_2(x)}}=c_0+c_1x \end{align}\]</span> where c0 and c1 are functions of μ1, μ2, and σ2.</p><p>In Logistic Regression,</p><p><span class="math display">\[\begin{align} \log{\frac{p_1}{1-p_1}}=\beta\_0+\beta_1x \end{align}\]</span> <strong>SAME</strong></p><ul><li><strong>Both logistic regression and LDA produce linear decision boundaries.</strong></li></ul><p><strong>DIFFERENCES</strong></p><ul><li><p>The only difference between the two approaches lies in the fact that β0 and β1 are estimated using maximum likelihood, whereas c0 and c1 are computed using the estimated mean and variance from a normal distribution. This same connection between LDA and logistic regression also holds for multidimensional data with p &gt; 1.</p></li><li><p>LDA assumes that the observations are drawn from a Gaussian distribution with a common covariance matrix in each class, and so can provide some improvements over logistic regression when this assumption approximately holds. Conversely, logistic regression can outperform LDA if these Gaussian assumptions are not met.</p></li></ul><h2 id="knn-dominate-lda-and-logistic-in-non-linear-setting">KNN dominate LDA and Logistic in non-linear setting</h2><p>In order to make a prediction for an observation X = x, the K training observations that are closest to x are identified. Then X is assigned to the class to which the plurality of these observations belong. Hence KNN is a completely <strong>non-parametric</strong> approach: <em>no assumptions are made about the shape of the decision boundary</em>.</p><blockquote><p>Therefore, we can expect KNN to dominate LDA and logistic regression when the decision boundary is highly non-linear.</p></blockquote><p>On the other hand, KNN does not tell us which predictors are important</p><h2 id="qda-serves-as-a-compromise-between-knn-lda-and-logistic-regression">QDA serves as a compromise between KNN, LDA and logistic regression</h2><p>QDA serves as a compromise between the non-parametric KNN method and the linear LDA and logistic regression approaches. Since QDA assumes a quadratic decision boundary, it can accurately model a wider range of problems than can the linear methods. Though not as flexible as KNN, QDA can perform better in the presence of a <em>limited number of training observations</em> because it does make some assumptions about the form of the decision boundary.</p><p><img src="./images/17.png"></p><p><strong>Scenario 1</strong>: - 20 training observations in each of two classes. The observations within each class were uncorrelated random normal variables with a different mean in each class. - LDA performed well in this setting. KNN performed poorly because it paid a price in terms of variance that was not offset by a reduction in bias.</p><p><strong>Scenario 2</strong>: - Details are as in Scenario 1, except that within each class, the two predictors had a correlation of −0.5. - Little change in the relative performances of the methods as compared to the previous scenario.</p><p><strong>Scenario 3</strong>: - X1 and X2 are from the t-distribution, with 50 observations per class.</p><blockquote><p>The <strong>t-distribution</strong> has a similar shape to the normal distribution, but it has a tendency to yield more extreme points—that is, more points that are far from the mean.</p></blockquote><ul><li>The decision boundary was still linear, and so fit into the logistic regression framework. The set-up violated the assumptions of LDA, since the observations were not drawn from a normal distribution. QDA results deteriorated considerably as a consequence of non-normality.</li></ul><p><strong>Scenario 4</strong>: - The data were generated from a normal distribution, with a correlation of 0.5 between the predictors in the first class, and correlation of −0.5 between the predictors in the second class. - This setup corresponded to the QDA assumption, and resulted in quadratic decision boundaries.</p><p><strong>Scenario 5</strong>: - Within each class, the observations were generated from a normal distribution with uncorrelated predictors. However, the responses were sampled from the logistic function using \(X^2_1 , X^2_2, and \, X1 × X2\) as predictors. - Consequently, there is a quadratic decision boundary. QDA once again performed best, followed closely by KNN-CV. The linear methods had poor performance.</p><p><strong>Scenario 6</strong>: - Details are as in the previous scenario, but the responses were sampled from a more complicated non-linear function. - Even the quadratic decision boundaries of QDA could not adequately model the data. - Much more flexible KNN-CV method gave the best results. But KNN with K = 1 gave the worst results out of all methods.</p><blockquote><p>This highlights the fact that <strong>even when the data exhibits a complex nonlinear relationship, a non-parametric method such as KNN can still give poor results if the level of smoothness is not chosen correctly.</strong></p></blockquote><h2 id="conclusion">Conclusion</h2><ul><li><p>When the true decision boundaries are linear, then the LDA and logistic regression approaches will tend to perform well.</p></li><li><p>When the boundaries are moderately non-linear, QDA may give better results.</p></li><li><p>For much more complicated decision boundaries, a non-parametric approach such as KNN can be superior. But the level of smoothness for a non-parametric approach must be chosen carefully.</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;logistic-regression-and-lda-methods-are-closely-connected.&quot;&gt;Logistic regression and LDA methods are closely connec
      
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Linear Discriminant Analysis, ROC &amp; AUC, Confusion Matrix</title>
    <link href="https://nancyyanyu.github.io/posts/5b711fed/"/>
    <id>https://nancyyanyu.github.io/posts/5b711fed/</id>
    <published>2019-10-19T22:59:57.484Z</published>
    <updated>2019-10-19T22:59:57.484Z</updated>
    
    <content type="html"><![CDATA[<p><strong>LDA V.S. Logistic Regression</strong>:</p><ol type="1"><li><p>When the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem.</p></li><li><p>If n is small and the distribution of the predictors X is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.</p></li><li><p>Linear discriminant analysis is popular when we have more than two response classes.</p></li></ol><h1 id="using-bayes-theorem-for-classification">Using Bayes’ Theorem for Classification</h1><p>Suppose that we wish to classify an observation into one of K classes, where K ≥ 2.</p><a id="more"></a><p><strong>Prior</strong>:Let <span class="math inline">\(\pi_k=Pr(Y=k)\)</span> represent the overall or <strong><em>prior</em></strong> probability that a randomly chosen observation comes from the kth class. This is the probability that a given observation is associated with the kth category of the response variable Y .</p><p>Let <span class="math inline">\(f_k(X) ≡ Pr(X = x|Y = k)\)</span> denote the <strong><em>density function</em></strong> of X for an observation that comes from the kth class. In other words, fk(x) is relatively large if there is a high probability that an observation in the kth class has X ≈ x.</p><p><strong>Bayes’ theorem</strong> states that <span class="math display">\[\begin{align}Pr(Y=k|X=x)=\frac{\pi_k f_k(x)}{\sum_{l=1}^K\pi_lf_l(x)} \end{align}\]</span> <strong>Posterior</strong>:<span class="math inline">\(p_k(X) = Pr(Y = k|X)\)</span> an observation X = x belongs to the kth class, given the predictor value for that observation</p><p><strong>Estimating <span class="math inline">\(π_k\)</span>:</strong> simply compute the fraction of the training observations that belong to the kth class.</p><p><strong>Estimating <span class="math inline">\(f_k(X)\)</span>:</strong> more challenging</p><h1 id="linear-discriminant-analysis-for-p-1">Linear Discriminant Analysis for p = 1</h1><p>Assume p = 1—that is, we have only one predictor. We would like to obtain an estimate for <span class="math inline">\(f_k(x)\)</span> that we can estimate <span class="math inline">\(p_k(x)\)</span>. We will then classify an observation to the class for which <span class="math inline">\(p_k(x)\)</span> is greatest.</p><h2 id="assumptions">Assumptions</h2><p>In order to estimate <span class="math inline">\(f_k(x)\)</span>, we will first make some assumptions about its form:</p><ol type="1"><li>Assume that <span class="math inline">\(f_k(x)\)</span> is normal or Gaussian. <span class="math display">\[  \begin{align}  f_k(x)=\frac{1}{\sqrt{2\pi}\sigma_k}\exp{\left( -\frac{1}{2\sigma_k^2}(x-\mu_k)^2 \right)}  \end{align}  \]</span></li></ol><p>where <span class="math inline">\(μ_k\)</span> and <span class="math inline">\(σ_k^2\)</span> are the mean and variance parameters for the kth class.</p><ol start="2" type="1"><li><dl><dt>Assume that <span class="math inline">\(\sigma_1^2=...=\sigma_k^2\)</span></dt><dd>that is, there is a shared variance term across all K classes, which for simplicity we can denote by <span class="math inline">\(\sigma^2\)</span>.</dd></dl></li></ol><p>So <span class="math display">\[\begin{align}p_k(x)=\frac{\pi_k \frac{1}{\sqrt{2\pi}\sigma}\exp{\left( -\frac{1}{2\sigma^2}(x-\mu_k)^2 \right)}}{\sum_{l=1}^K\pi_l\frac{1}{\sqrt{2\pi}\sigma}\exp{\left( -\frac{1}{2\sigma^2}(x-\mu_l)^2 \right)}}\end{align}\]</span> The <strong>Bayes classifier</strong> involves assigning an observation X = x to the class for which <span class="math inline">\(p_k(x)\)</span> is largest. Taking the log of <span class="math inline">\(p_k(x)\)</span> and rearranging the terms, it is not hard to show that this is equivalent to assigning the observation to the class for which <span class="math display">\[\begin{align}\delta_k(x)=x\frac{\mu_k}{\sigma^2}-\frac{\mu_k^2}{2\sigma^2}+\log(\pi_k) \quad\quad (4.13)\end{align}\]</span> is largest.</p><p>For instance, if K = 2 and π1 = π2, then the Bayes classifier assigns an observation to class 1 if <span class="math inline">\(2x (μ_1 − μ_2) &gt; μ^2_1 − μ^2_2\)</span>, and to class 2 otherwise. In this case, the Bayes decision boundary corresponds to the point where <span class="math display">\[\begin{align}x=\frac{\mu_1^2-\mu_2^2}{2(\mu_1-\mu_2)}=\frac{\mu_1+\mu_2}{2}\end{align}\]</span></p><h2 id="parameters-estimation">Parameters Estimation</h2><p>In practice, even if we are quite certain of our assumption that X is drawn from a Gaussian distribution within each class, we still have to estimate the parameters <span class="math inline">\(μ_1, . . . , μ_K, π_1, . . . , π_K\)</span>, and <span class="math inline">\(σ^2\)</span>.</p><p><strong>Linear discriminant analysis (LDA)</strong> method approximates the Bayes classifier by plugging estimates for <span class="math inline">\(μ_1, . . . , μ_K, π_1, . . . , π_K\)</span>, and <span class="math inline">\(σ^2\)</span> into (4.13)</p><p><span class="math display">\[\begin{align}\hat{\mu}_k=\frac{1}{n_k}\sum_{i:y_i=k}x_i  \quad (4.15) \\\hat{\sigma}^2=\frac{1}{n-K}\sum_{k=1}^K\sum_{i:y_i=k}(x_i-\hat{\mu_k})^2 \quad (4.16)\\\hat{\pi_k}=\frac{n_k}{n}\end{align}\]</span></p><p>where n is the total number of training observations, and <span class="math inline">\(n_k\)</span> is the number of training observations in the kth class.</p><p><span class="math inline">\(\hat{\mu}_k\)</span>: average of all the training observations from the kth class;</p><p><span class="math inline">\(\hat{\sigma}^2\)</span>: a weighted average of the sample variances for each of the K classes.</p><p><span class="math inline">\(\hat{\pi_k}\)</span>: the proportion of the training observations that belong to the kth class</p><h2 id="lda-classifier">LDA classifier</h2><p>The LDA classifier assigns an observation X = x to the class for which</p><p><span class="math display">\[\begin{align}\hat{\delta}_k(x)=x\frac{\hat{\mu}_k}{\hat{\sigma}^2}-\frac{\hat{\mu}_k^2}{2\hat{\sigma}^2}+\log(\hat{\pi}_k)\end{align}\]</span> is largest.</p><p>The word <strong><em>linear</em></strong> in the classifier’s name stems from the fact that the <strong><em>discriminant functions</em></strong> <span class="math inline">\(\hat{\delta}_k(x)\)</span> are linear functions of x.</p><p><img src="./6.png" width="600"></p><p>The right-hand panel of Figure 4.4 displays a histogram of a random sample of 20 observations from each class.</p><p>To implement LDA,</p><ol type="1"><li>Estimating πk, μk, and σ2 using (4.15) and (4.16).</li><li>Compute the decision boundary, shown as a black solid line, that results from assigning an observation to the class for which <span class="math inline">\(\hat{\delta}_k(x)\)</span> is largest.</li></ol><p>In this case, since n1 = n2 = 20, we have <span class="math inline">\(\hat{\pi_1}\)</span> = <span class="math inline">\(\hat{\pi_2}\)</span>. As a result, the decision boundary corresponds to the midpoint between the sample means for the two classes,<span class="math inline">\(\frac{\mu_1+\mu_2}{2}\)</span></p><h1 id="linear-discriminant-analysis-for-p-1-1">Linear Discriminant Analysis for p &gt;1</h1><p>Assume that X = (X1,X2, . . .,Xp) is drawn from a <strong>multivariate Gaussian</strong> (or multivariate normal) distribution, with a class-specific mean vector and a common covariance matrix.</p><h2 id="multivariate-gaussian-distribution">Multivariate Gaussian Distribution</h2><p>Assumes that each individual predictor follows a one-dimensional normal distribution with some correlation between each pair of predictors.</p><p><img src="./7.png" width="600"></p><p>To indicate that a p-dimensional random variable X has a multivariate Gaussian distribution, we write X ∼ N(μ,Σ). Here E(X) = μ is the mean of X (a vector with p components), and Cov(X) = Σ is the p × p <strong>covariance matrix</strong> of X. Formally, the <strong>multivariate Gaussian density</strong> is defined as <span class="math display">\[\begin{align}f(x)=\frac{1}{\sqrt{(2\pi)^{p}|Σ|}}\exp{\left( \frac{1}{2}(x-\mu)^TΣ^{-1}(x-\mu) \right)}\end{align}\]</span> In the case of p &gt; 1 predictors, the <strong>LDA classifier</strong> assumes that the observations in the kth class are drawn from a multivariate Gaussian distribution <span class="math inline">\(N(μ_k,Σ)\)</span>, where <span class="math inline">\(μ_k\)</span> is a class-specific mean vector, and Σ is a covariance matrix that is common to all K classes.</p><p>Plugging the density function for the kth class, <span class="math inline">\(f_k(X = x)\)</span>, into <span class="math inline">\(Pr(Y = k|X = x)\)</span>, the Bayes classifier assigns an observation X = x to the class for which <span class="math display">\[\begin{align}\delta_k(x)=x^TΣ^{-1}\mu_k-\frac{1}{2}\mu_k^TΣ^{-1}\mu_k+\log{\pi_k}  \quad \quad (4.19)\end{align}\]</span> is largest.</p><p><img src="./8.png" width="600"></p><p>Once again, we need to estimate the unknown parameters <span class="math inline">\(μ_1, . . . , μ_K\)</span>, <span class="math inline">\(π_1, . . . , π_K\)</span>, and Σ; the formulas are similar to those used in the one dimensional case, given in (4.15). To assign a new observation X = x, <strong>LDA</strong> plugs these estimates into (4.19) and classifies to the class for which <span class="math inline">\(\hat{\delta}_k(x)\)</span> is largest.</p><blockquote><p>Overall, the LDA decision boundaries are pretty close to the Bayes decision boundaries, shown again as dashed lines.</p></blockquote><h2 id="caveats">Caveats</h2><ol type="1"><li><p>Training error rates will usually be lower than test error rates. The higher the ratio of parameters p to number of samples n, the more we expect this overfitting to play a role.</p></li><li><p>Second, since only 3.33% of the individuals in the training sample defaulted, a simple but useless classifier that always predicts that each individual will not default, regardless of his or her credit card balance and student status, will result in an error rate of 3.33%. In other words, the trivial <strong>null classifier</strong> will achieve an error rate that is only a bit higher than the LDA training set error rate.</p></li></ol><h3 id="two-types-of-error-confusion-matrix">Two Types of Error, Confusion Matrix</h3><p>Binary classifier can make two types of errors:</p><ol type="1"><li>it can incorrectly assign an individual who defaults to the no default category;</li><li>it can incorrectly assign an individual who does not default to the default category.</li></ol><p><strong>Confusion Matrix</strong></p><p>*注意这张图不是标准的confusion matrix，看下面那张 <img src="./10.png" width="600"></p><p><strong>Explanation</strong>： The matrix table reveals that LDA predicted that a total of 104 people would default. Of these people, 81 actually defaulted and 23 did not.</p><p><strong>Type I Error</strong>： Of the 333 individuals who defaulted, 252 (or 75.7%) were missed by LDA. So while the overall error rate is low, the error rate among individuals who defaulted is very high. From the perspective of a credit card company that is trying to identify high-risk individuals, an error rate of 252/333 = 75.7% among individuals who default may well be unacceptable.</p><p><strong>Type II Error</strong>：Only 23 out of 9, 667 of the individuals who did not default were incorrectly labeled. This looks like a pretty low error rate!</p><p><img src="./11.png" width="1000"></p><p><strong>Sensitivity</strong>:the percentage of true defaulters that are identified, a low 24.3% in this case.</p><p><strong>Specificity</strong>:the percentage of non-defaulters that are correctly identified, here (1 − 23/9, 667)× 100 = 99.8%.</p><h3 id="why-does-lda-do-such-a-poor-job-of-classifying-the-customers-who-default">Why does LDA do such a poor job of classifying the customers who default?</h3><blockquote><p>In other words, why does it have such a low sensitivity?</p></blockquote><p>LDA is trying to approximate the Bayes classifier, which has the lowest total error rate out of all classifiers (if the Gaussian model is correct). That is, the Bayes classifier will yield the smallest possible total number of misclassified observations, irrespective of which class the errors come from.</p><p>The Bayes classifier works by assigning an observation to the class for which the posterior probability pk(X) is greatest. In the two-class case, this amounts to assigning an observation to the default class if <span class="math inline">\(Pr(default = Yes|X = x) &gt; 0.5.\)</span></p><p>Thus, the Bayes classifier, and by extension LDA, uses a threshold of 50% for the posterior probability of default in order to assign an observation to the default class.</p><p><strong>Modify LDA</strong></p><p>If we are concerned about incorrectly predicting the default status for individuals who default, then we can consider lowering this threshold.</p><p><span class="math display">\[P(default = Yes|X = x) &gt; 0.2\]</span></p><p>Figure 4.7 illustrates the trade-off that results from modifying the threshold value for the posterior probability of default</p><p><img src="./12.png" width="700"></p><p>How can we decide which threshold value is best? Such a decision must be based on <strong>domain knowledge</strong>, such as detailed information about the costs associated with default.</p><h3 id="roc-auc">ROC &amp; AUC</h3><p><strong>ROC</strong>:The ROC curve is a popular graphic for simultaneously displaying the two types of errors for all possible thresholds.</p><p><strong>AUC</strong>: The overall performance of a classifier, summarized over all possible thresholds, is given by the area under the (ROC) curve (AUC).</p><ul><li><p>An ideal ROC curve will hug the top left corner, so the larger the AUC the better the classifier. We expect a classifier that performs no better than chance to have an AUC of 0.5</p></li><li><p>ROC curves are useful for comparing different classifiers, since they take into account all possible thresholds.</p></li></ul><p><img src="./13.png" width="700"></p><p><img src="./14.png" width="700"></p><p><img src="./15.png" width="700"></p><h1 id="quadratic-discriminant-analysis">Quadratic Discriminant Analysis</h1><p><strong>Quadratic discriminant analysis (QDA)</strong> classifier results from assuming that the observations from each class are drawn from a Gaussian distribution, and plugging estimates for the parameters into Bayes’ theorem in order to perform prediction.</p><p>However, unlike LDA, <strong>QDA assumes that each class has its own covariance matrix</strong>. That is, it assumes that an observation from the kth class is of the form <span class="math inline">\(X ∼ N(μ_k,Σ_k)\)</span>, where <span class="math inline">\(Σ_k\)</span> is a covariance matrix for the kth class. Under this assumption, the Bayes classifier assigns an observation <span class="math inline">\(X = x\)</span> to the class for which</p><p><span class="math display">\[\begin{align}\delta_k(x)&amp;=-\frac{1}{2}(x-\mu_k)^TΣ_k^{-1}(x-\mu_k)-\frac{1}{2}\log{|Σ_k|}+\log{\pi_k} \\&amp;=-\frac{1}{2}x^TΣ_k^{-1}x+x^TΣ_k^{-1}\mu_k-\frac{1}{2}\mu_k^TΣ_k^{-1}\mu_k-\frac{1}{2}\log{|Σ_k|}+\log{\pi_k}\end{align}\]</span></p><p>is largest.</p><p>So the QDA classifier involves plugging estimates for <span class="math inline">\(Σ_k, μ_k, π_k\)</span> into <span class="math inline">\(\delta_k(x)\)</span>, and then assigning an observation <span class="math inline">\(X = x\)</span> to the class for which this quantity is largest. Unlike LDA, the quantity <span class="math inline">\(x\)</span> appears as a quadratic function.</p><h2 id="why-does-it-matter-whether-or-not-we-assume-that-the-k-classes-share-a-common-covariance-matrix">Why does it matter whether or not we assume that the K classes share a common covariance matrix?</h2><p>The answer lies in the <strong>bias-variance trade-off</strong>: - When there are p predictors, then estimating a covariance matrix requires estimating p(p+1)/2 parameters. QDA estimates a separate covariance matrix for each class, for a total of Kp(p+1)/2 parameters. - Consequently, LDA is a much less flexible classifier than QDA, and so has substantially lower variance. - But there is a trade-off: if LDA’s assumption that the K classes share a common covariance matrix is badly off, then LDA can suffer from high bias.</p><p><strong>Conclusion</strong></p><ul><li>LDA tends to be a better bet than QDA if there are relatively few training observations and so reducing variance is crucial.</li><li>QDA is recommended if the training set is very large, so that the variance of the classifier is not a major concern, or if the assumption of a common covariance matrix for the K classes is clearly untenable</li></ul><p><img src="./16.png" width="700"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;LDA V.S. Logistic Regression&lt;/strong&gt;:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;When the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If n is small and the distribution of the predictors X is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Linear discriminant analysis is popular when we have more than two response classes.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&quot;using-bayes-theorem-for-classification&quot;&gt;Using Bayes’ Theorem for Classification&lt;/h1&gt;
&lt;p&gt;Suppose that we wish to classify an observation into one of K classes, where K ≥ 2.&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="LDA" scheme="https://nancyyanyu.github.io/tags/LDA/"/>
    
      <category term="Classification" scheme="https://nancyyanyu.github.io/tags/Classification/"/>
    
      <category term="Model Evaluation" scheme="https://nancyyanyu.github.io/tags/Model-Evaluation/"/>
    
  </entry>
  
  <entry>
    <title>Logistic Regression</title>
    <link href="https://nancyyanyu.github.io/posts/b358d10f/"/>
    <id>https://nancyyanyu.github.io/posts/b358d10f/</id>
    <published>2019-10-19T22:59:42.997Z</published>
    <updated>2019-10-19T22:59:42.998Z</updated>
    
    <content type="html"><![CDATA[<h1 id="why-not-linear-regression">Why Not Linear Regression?</h1><p>The logistic regression model arises from the desire to model the posterior probabilities of the K classes via linear functions in x, while at the same time ensuring that they sum to one and remain in [0, 1]. The model has the form: <span class="math display">\[\begin{align}\log\frac{\text{Pr}(G=1|X=x)}{\text{Pr}(G=K|X=x)} &amp;= \beta_{10} + \beta_1^Tx \\\log\frac{\text{Pr}(G=2|X=x)}{\text{Pr}(G=K|X=x)} &amp;= \beta_{20} + \beta_2^Tx \\&amp;\vdots \\\log\frac{\text{Pr}(G=K-1|X=x)}{\text{Pr}(G=K|X=x)} &amp;= \beta_{(K-1)0} + \beta_{K-1}^Tx \\\end{align}\]</span></p><p>Linear regression is not appropriate in the case of a qualitative response.**</p><p><strong>Reason:</strong> there is no natural way to convert a qualitative response variable with more than two levels into a quantitative response that is ready for linear regression.</p><p><strong>Setting</strong>: For the Default data, logistic regression models the probability of default. For example, the probability of default given balance can be written as <span class="math inline">\(Pr(default = Yes|balance).\)</span></p><p><img src="./1.png" width="600"></p><a id="more"></a><h1 id="the-logistic-model">The Logistic Model</h1><p>Logistic regression involves directly modeling Pr(Y = k|X = x) using the logistic function for the case of two response classes</p><p><strong>Logistic function:</strong></p><p><span class="math display">\[\begin{align}p(X)=\frac{e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}} \\\frac{p(X)}{1-p(X)}=e^{\beta_0+\beta_1X}\end{align}\]</span> <strong>Odds</strong></p><p>The quantity p(X)/[1−p(X)] is called the <strong>odds</strong>, and can take on any value odds between 0 and ∞. Values</p><p><strong>Log-odds (Logit)</strong></p><p><span class="math display">\[\begin{align}\log{\frac{p(X)}{1-p(X)}}=\beta_0+\beta_1X\end{align}\]</span> We see that the logistic model (4.2) has a logit that is linear in X.</p><ul><li><p>There is not a straight-line relationship between p(X) and X,</p></li><li><p>The rate of change in p(X) per unit change in X depends on the current value of X,</p></li></ul><h1 id="estimating-the-regression-coefficients">Estimating the Regression Coefficients</h1><p>The basic intuition behind using <strong>maximum likelihood</strong> to fit a logistic regression model is as follows: - We seek estimates for <span class="math inline">\(β_0\)</span> and <span class="math inline">\(β_1\)</span> such that the predicted probability <span class="math inline">\(\hat{p}(x_i)\)</span> of class &quot;default&quot; for each individual, using (4.2), corresponds as closely as possible to the individual’s observed &quot;default&quot; status. In other words, we try to find <span class="math inline">\(\hat{β}_0\)</span> and <span class="math inline">\(\hat{β}_1\)</span> such that plugging these estimates into the model for <span class="math inline">\(p(X)\)</span>, given in (4.2), yields a number close to one for all individuals who &quot;defaulted&quot;, and a number close to zero for all individuals who did not.</p><p><strong>Likelihood function</strong>:</p><p><span class="math display">\[\begin{align}l(\beta_0,\beta_1)=\prod_{i:y_i=1}p(x_i) \prod_{i^{&#39;}:y_{i^{&#39;}}=0}(1-p(x_{i^{&#39;}}))\end{align}\]</span> The estimates <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span> are chosen to maximize this <strong><em>likelihood function.</em></strong></p><p>In the linear regression setting, the least squares approach is in fact a special case of maximum likelihood.</p><h1 id="making-predictions">Making Predictions</h1><p>Once the coefficients have been estimated, it is a simple matter to compute the probability of default for any given credit card balance.</p><p><img src="./2.png" width="600"></p><p>For example, using the coefficient estimates given in Table 4.1, we predict that the default probability for an individual with a balance of $1, 000 is <span class="math display">\[\begin{align}\hat{p}(X)=\frac{e^{\hat{\beta_0}+\hat{\beta_1}X}}{1+e^{\hat{\beta_0}+\hat{\beta_1}X}}=\frac{e^{−10.6513+0.0055×1,000}}{1+e^{−10.6513+0.0055×1,000}}=0.00576\end{align}\]</span></p><h1 id="multiple-logistic-regression">Multiple Logistic Regression</h1><p>We now consider the problem of predicting a binary response using multiple predictors</p><p><strong>Log-odds (Logit)</strong></p><p><span class="math display">\[\begin{align}\log{\frac{p(X)}{1-p(X)}}=\beta_0+\sum_{i=1}^p\beta_iX\end{align}\]</span> where X = (X1, . . .,Xp) are p predictors</p><p><strong>Logistic function:</strong></p><p><span class="math display">\[\begin{align}p(X)=\frac{e^{\beta_0+\sum_{i=1}^p\beta_iX}}{1+e^{\beta_0+\sum_{i=1}^p\beta_iX}} \\\frac{p(X)}{1-p(X)}=e^{\beta_0+\sum_{i=1}^p\beta_iX}\end{align}\]</span></p><h2 id="confounding">Confounding</h2><p>In single variable setting: <img src="./3.png" width="600"></p><p>In multiple variables setting: <img src="./4.png" width="600"></p><blockquote><p>How is it possible for student status to be associated with an increase in probability of default in Table 4.2 and a decrease in probability of default in Table 4.3?</p></blockquote><p><img src="./5.png" width="600"></p><ul><li>The positive coefficient for student in the single variable logistic regression : the overall student default rate is higher than the non-student default rate</li><li>The negative coefficient for student in the multiple logistic regression: for a fixed value of balance and income, a student is less likely to default than a non-student.</li></ul><p><strong>Reason</strong>:The variables <em>student</em> and <em>balance</em> are correlated.</p><p><strong>Intuition</strong>: A student is riskier than a non-student if no information about the student’s credit card balance is available. However, that student is less risky than a non-student with the same credit card balance!</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;why-not-linear-regression&quot;&gt;Why Not Linear Regression?&lt;/h1&gt;
&lt;p&gt;The logistic regression model arises from the desire to model the posterior probabilities of the K classes via linear functions in x, while at the same time ensuring that they sum to one and remain in [0, 1]. The model has the form: &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
\log\frac{\text{Pr}(G=1|X=x)}{\text{Pr}(G=K|X=x)} &amp;amp;= \beta_{10} + \beta_1^Tx \\
\log\frac{\text{Pr}(G=2|X=x)}{\text{Pr}(G=K|X=x)} &amp;amp;= \beta_{20} + \beta_2^Tx \\
&amp;amp;\vdots \\
\log\frac{\text{Pr}(G=K-1|X=x)}{\text{Pr}(G=K|X=x)} &amp;amp;= \beta_{(K-1)0} + \beta_{K-1}^Tx \\
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Linear regression is not appropriate in the case of a qualitative response.**&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reason:&lt;/strong&gt; there is no natural way to convert a qualitative response variable with more than two levels into a quantitative response that is ready for linear regression.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Setting&lt;/strong&gt;: For the Default data, logistic regression models the probability of default. For example, the probability of default given balance can be written as &lt;span class=&quot;math inline&quot;&gt;\(Pr(default = Yes|balance).\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;./1.png&quot; width=&quot;600&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Classification" scheme="https://nancyyanyu.github.io/tags/Classification/"/>
    
      <category term="Logistic Regression" scheme="https://nancyyanyu.github.io/tags/Logistic-Regression/"/>
    
  </entry>
  
  <entry>
    <title>Linear Regression Part II: Potential Problems</title>
    <link href="https://nancyyanyu.github.io/posts/4df00c7b/"/>
    <id>https://nancyyanyu.github.io/posts/4df00c7b/</id>
    <published>2019-10-19T22:56:41.781Z</published>
    <updated>2019-10-19T22:56:41.782Z</updated>
    
    <content type="html"><![CDATA[<h1 id="qualitative-predictors">Qualitative Predictors</h1><h2 id="predictors-with-only-two-levels">Predictors with Only Two Levels</h2><p>Suppose that we wish to investigate differences in credit card balance between males and females, ignoring the other variables for the moment. If a qualitative predictor (also known as a <strong>factor</strong>) only has two <strong>levels</strong>, or possible values, then incorporating it into a regression model is very simple. We simply create an indicator or <strong>dummy variable</strong> that takes on two possible numerical values.</p><p><img src="./8.png" width="300"> and use this variable as a predictor in the regression equation. This results in the model</p><p><img src="./9.png" width="600"></p><a id="more"></a><p>Now β0 can be interpreted as the average credit card balance among males, β0 + β1 as the average credit card balance among females</p><h2 id="qualitative-predictors-with-more-than-two-levels">Qualitative Predictors with More than Two Levels</h2><p>When a qualitative predictor has more than two levels, we can create additional dummy variables. For example, for the ethnicity variable we create two dummy variables. The first could be</p><p><img src="./10.png" width="300"> and the second could be <img src="./11.png" width="300"></p><p>Then both of these variables can be used in the regression equation, in order to obtain the model</p><p><img src="./12.png" width="600"></p><p><strong>Baseline</strong></p><ul><li>There will always be <strong>one fewer</strong> dummy variable than the number of levels. The level with no dummy variable—African American in this example—is known as the baseline.</li></ul><p><img src="./13.png" width="600"></p><p>The p-values associated with the coefficient estimates for the two dummy variables are very large, suggesting no statistical evidence of a real difference in credit card balance between the ethnicities</p><blockquote><p>The coefficients and their p-values do depend on the choice of dummy variable coding</p></blockquote><p>Rather than rely on the individual coefficients, we can use an <strong>F-test</strong> to test H0 : β1 = β2 = 0; this does not depend on the coding.</p><p>This F-test has a p-value of 0.96, indicating that we cannot reject the null hypothesis that there is no relationship between balance and ethnicity.</p><h1 id="extensions-of-the-linear-model">Extensions of the Linear Model</h1><p>Two of the most important assumptions state that the relationship between the predictors and response are <strong>additive</strong> and <strong>linear</strong>. - <strong>Additive</strong>: the effect of changes in a predictor <span class="math inline">\(X_j\)</span> on the response <span class="math inline">\(Y\)</span> is independent of the values of the other predictors - <strong>Linear</strong>: the change in the response <span class="math inline">\(Y\)</span> due to a one-unit change in <span class="math inline">\(X_j\)</span> is constant, regardless of the value of <span class="math inline">\(X_j\)</span></p><h2 id="removing-the-additive-assumption">Removing the Additive Assumption</h2><p>Consider the standard linear regression model with two variables, <span class="math display">\[\begin{align}Y = β_0 + β_1X_1 + β_2X_2 + \epsilon\end{align}\]</span></p><p>One way of extending this model to allow for interaction effects is to include a third predictor, called an <strong>interaction term</strong>:</p><p><span class="math display">\[\begin{align}Y = β_0 + β_1X_1 + β_2X_2 +  β_3X_1X_2 + \epsilon \end{align}\]</span></p><p><strong>How does inclusion of this interaction term relax the additive assumption?</strong></p><p>The model above could be written as: <span class="math display">\[\begin{align}Y &amp;= β_0 + (β_1+β_3X_2)X_1 + β_2X_2 + \epsilon  \\&amp;= β_0 + \tilde{β}_1X_1 + β_2X_2 + \epsilon\end{align}\]</span></p><p>Since <span class="math inline">\(\tilde{β}_1\)</span> changes with <span class="math inline">\(X_2\)</span>, the effect of <span class="math inline">\(X_1\)</span> on <span class="math inline">\(Y\)</span> is no longer constant: adjusting <span class="math inline">\(X_2\)</span> will change the impact of <span class="math inline">\(X_1\)</span> on <span class="math inline">\(Y\)</span>.</p><p><img src="./14.png" width="600"></p><ul><li>Sometimes the case that an interaction term has a very small p-value, but the associated main effects (in this case, TV and radio) do not.</li><li>The <strong>hierarchical principle</strong> states that if we include an interaction in a model, we should also include the <strong>main effects</strong>, even if the p-values associated with their coefficients are not significant. (If the interaction between X1 and X2 seems important, we should include both X1 and X2 in the model even if their coefficient estimates have large p-values)</li></ul><p><strong>Concept of interactions applies on qualitative variables</strong> <img src="./15.png" width="600"></p><p>Adding an interaction variable, model now becomes: <img src="./17.png" width="600"> <img src="./16.png" width="600"></p><h2 id="non-linear-relationships">Non-linear Relationships</h2><p>Extending the linear model to accommodate non-linear relationships is known as <strong>polynomial regression</strong>, since we have included <strong>polynomial functions</strong> of the predictors in the regression model</p><h1 id="potential-problems">Potential Problems</h1><h2 id="non-linearity-of-the-data">Non-linearity of the Data</h2><p><strong>Assumption</strong>: The linear regression model assumes that there is a straight-line relationship between the predictors and the response.</p><p><strong>Residual plots</strong>: graphical tool for identifying non-linearity - Given a simple linear regression model, we can plot the residuals,<span class="math inline">\(e_i = y_i-\hat{y_i}\)</span> versus the predictor <span class="math inline">\(x_i\)</span>, or <span class="math inline">\(\hat{y_i}\)</span> when there are multiple predictors</p><p><img src="./18.png" width="600"></p><ul><li>Ideally, the residual plot will show no discernible pattern.</li><li>If the residual plot indicates non-linear associations in the data, then a simple approach is to use <strong>non-linear transformations</strong> of the predictors, such as <span class="math inline">\(\log{X},\sqrt{X}, X^2\)</span>, in the regression model.</li></ul><h2 id="correlation-of-error-terms">Correlation of Error Terms</h2><p><strong>Assumption</strong>: The error terms, <span class="math inline">\(\epsilon_1,\epsilon_2,...,\epsilon_n\)</span>, are uncorrelated. - If the errors are uncorrelated, then the fact that i is positive provides little or no information about the sign of i+1. - If the error terms are correlated, we may have an unwarranted sense of confidence in our model. - <strong>estimated standard errors</strong> will underestimate the true standard errors. - <strong>confidence and prediction intervals</strong> will be narrower than they should be. For example, a 95% confidence interval may in reality have a much lower probability than 0.95 of containing the true value of the parameter. - <strong>p-values</strong> will be lower than they should be - Lead to erroneously conclude that a parameter is statistically significant.</p><p><strong>Why might correlations among the error terms occur?</strong> - Such correlations frequently occur in the context of <strong>time series</strong> data - In many cases, observations that are obtained at adjacent time points will have <strong>positively correlated errors</strong>. - Plot the residuals from our model as a function of time to identify this correlation. - <strong>Tracking</strong>: If the error terms are positively correlated, then we may see <strong>tracking</strong> in the residuals—that is, adjacent residuals may have similar values. <img src="./19.png" width="600"></p><h2 id="non-constant-variance-of-error-terms">Non-constant Variance of Error Terms</h2><p><strong>Assumption</strong>: the error terms have a constant variance, <span class="math inline">\(Var(\epsilon_i) = σ^2\)</span>. - The standard errors, confidence intervals, and hypothesis tests associated with the linear model rely upon this assumption.</p><p>The variances of the error terms are non-constant. - For instance, the variances of the error terms may increase with the value of the response. - One can identify non-constant variances in the errors, or <strong>heteroscedasticity</strong>异方差性,from the presence of a funnel shape in residual plot. - <strong>Solution</strong>: transform the response Y using a concave function such as <span class="math inline">\(\log{Y}\)</span> or <span class="math inline">\(\sqrt{Y}\)</span> . Such a transformation results in a greater amount of shrinkage of the larger responses, leading to a reduction in <strong>heteroscedasticity</strong>.</p><p><img src="./20.png" width="600"></p><h2 id="outliers">Outliers</h2><p><strong>Outlier</strong>: is a point for which <span class="math inline">\(y_i\)</span> is far from the value predicted by the model. Outliers can arise for a variety of reasons, such as incorrect recording of an observation during data collection.</p><p><strong>Problems of Outlier</strong>: - Effect on the least squares fit, - Effect on interpretation of the fit - For instance, in this example, the RSE is 1.09 when the outlier is included in the regression, but it is only 0.77 when the outlier is removed.</p><p><strong>Residual Plots</strong> can be used to identify outliers</p><p><img src="./21.png" width="600"></p><ul><li>Difficult to decide how large a residual needs to be</li></ul><p><strong>Studentized residuals</strong>: computed by dividing each residual <span class="math inline">\(e_i\)</span> by its estimated standard error. - Observations whose studentized residuals are greater than 3 in abso- residual lute value are possible outliers.</p><h2 id="high-leverage-points">High Leverage Points</h2><p><strong>High Leverage</strong>:Observations with high leverage have an unusual value for xi - removing the high leverage observation has a much more substantial impact on the least squares line than removing the outlier. <img src="./23.png" width="600"></p><p><strong>Leverage Statistic</strong>: quantify an observation’s leverage</p><p>For a simple linear regression</p><p><span class="math display">\[\begin{align}h_i=\frac{1}{n}+\frac{(x_i-\bar{x})^2}{\sum_{i^{&#39;}=1}^n (x_{i^{&#39;}}-\bar{x})^2}\end{align}\]</span></p><ul><li><span class="math inline">\(h_i\)</span> increases with the distance of <span class="math inline">\(x_i\)</span> from <span class="math inline">\(\bar{x}\)</span>.</li><li><span class="math inline">\(h_i\)</span> is always between 1/n and 1, and the <strong>average leverage</strong> for all the observations is always equal to <span class="math inline">\((p+1)/n\)</span>.</li><li><strong>High leverage</strong>: a leverage statistic that greatly exceeds <span class="math inline">\((p+1)/n\)</span>, high leverage.</li></ul><p><img src="./22.png" width="600"></p><p>The right-hand panel of Figure 3.13 provides a plot of the studentized residuals versus <span class="math inline">\(h_i\)</span> for the data in the left-hand panel of Figure 3.13. Observation 41 stands out as having a very high leverage statistic as well as a high studentized residual. In other words, it is an outlier as well as a high leverage observation.</p><h2 id="collinearity">Collinearity</h2><p>Collinearity: situation in which two or more predictor variables are closely related to one another.</p><p><strong>Problems of Collinearity</strong></p><ul><li>Difficult to separate out the individual effects of collinear variables on the response</li><li>Uncertainty in the coefficient estimates.</li><li>Causes the standard error for <span class="math inline">\(\hat{β_j}\)</span> to grow</li><li>Recall that the t-statistic for each predictor is calculated by dividing <span class="math inline">\(\hat{β_j}\)</span> by its standard error. Consequently, collinearity results in a decline in the t-statistic. As a result, in the presence of collinearity, we may fail to reject H0 : βj = 0. This means that the <strong>power</strong> of the hypothesis test—the probability of correctly detecting a non-zero coefficient—is reduced by collinearity.</li></ul><p><img src="./24.png" width="600"></p><p><strong>Detection of Collinearity</strong></p><ul><li><strong>Correlation matrix</strong> of the predictors.</li><li>An element of this matrix that is large in absolute value indicates a pair of highly correlated variables.</li><li><strong>Situation Multicollinearity</strong>: it is possible for collinearity to exist between three or more variables even if no pair of variables has a particularly high correlation</li><li><strong>Variance Inflation Factor (VIF)</strong></li><li>The ratio of the variance of <span class="math inline">\(\hat{β_j}\)</span> when fitting the full model divided by the variance of <span class="math inline">\(\hat{β_j}\)</span> if fit on its own. The smallest possible value for VIF is 1, which indicates the complete absence of collinearity.</li><li>A VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity.</li></ul><p>The VIF for each variable,</p><p>where <span class="math inline">\(R^2_{X_j|X_{-j}}\)</span> is the <span class="math inline">\(R^2\)</span> from a regression of <span class="math inline">\(X_j\)</span> onto all of the other predictors. If <span class="math inline">\(R^2_{X_j|X_{-j}}\)</span> is close to one, then collinearity is present, and so the VIF will be large.</p><p><strong>Solution of Collinearity</strong></p><ul><li>Drop one of the problematic variables from the regression.</li><li>Combine the collinear variables together into a single predicto</li><li>E.g.: take the average of standardized versions of limit and rating in order to create a new variable that measures credit worthiness</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;qualitative-predictors&quot;&gt;Qualitative Predictors&lt;/h1&gt;
&lt;h2 id=&quot;predictors-with-only-two-levels&quot;&gt;Predictors with Only Two Levels&lt;/h2&gt;
&lt;p&gt;Suppose that we wish to investigate differences in credit card balance between males and females, ignoring the other variables for the moment. If a qualitative predictor (also known as a &lt;strong&gt;factor&lt;/strong&gt;) only has two &lt;strong&gt;levels&lt;/strong&gt;, or possible values, then incorporating it into a regression model is very simple. We simply create an indicator or &lt;strong&gt;dummy variable&lt;/strong&gt; that takes on two possible numerical values.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;./8.png&quot; width=&quot;300&quot;&gt; and use this variable as a predictor in the regression equation. This results in the model&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;./9.png&quot; width=&quot;600&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Linear Regression" scheme="https://nancyyanyu.github.io/tags/Linear-Regression/"/>
    
      <category term="Regression" scheme="https://nancyyanyu.github.io/tags/Regression/"/>
    
  </entry>
  
  <entry>
    <title>Linear Regression Part I: Linear Regression Models</title>
    <link href="https://nancyyanyu.github.io/posts/9b5af8e1/"/>
    <id>https://nancyyanyu.github.io/posts/9b5af8e1/</id>
    <published>2019-10-19T22:56:00.145Z</published>
    <updated>2019-10-19T22:56:00.145Z</updated>
    
    <content type="html"><![CDATA[<h1 id="simple-linear-regression-models">Simple Linear Regression Models</h1><h2 id="linear-regression-model">Linear Regression Model</h2><ul><li><p>Form of the linear regression model: <em><span class="math inline">\(f(X)=\beta_{0}+\sum_{j=1}^{p}X_{j}\beta_{j}\)</span></em>.</p></li><li><p>Training data: (<span class="math inline">\(x_1\)</span>,<span class="math inline">\(y_1\)</span>) ... (<span class="math inline">\(x_N\)</span>,<span class="math inline">\(y_N\)</span>). Each <span class="math inline">\(x_{i} =(x_{i1},x_{i2},...,x_{ip})^{T}\)</span> is a vector of feature measurements for the <span class="math inline">\(i\)</span>-th case.</p></li><li><p>Goal: estimate the parameters <span class="math inline">\(β\)</span></p></li><li><p>Estimation method: <strong>Least Squares</strong>, we pick the coeﬃcients <span class="math inline">\(β =(β_0,β_1,...,β_p)^{T}\)</span> to minimize the <strong>residual sum of squares</strong></p></li></ul><p><strong>Assumptions:</strong></p><ul><li>Observations <span class="math inline">\(y_i\)</span> are uncorrelated and have constant variance <span class="math inline">\(\sigma^2\)</span>;</li><li><span class="math inline">\(x_i\)</span> are ﬁxed (non random)</li><li>The regression function E(Y |X) is linear, or the linear model is a reasonable approximation.</li></ul><a id="more"></a><h2 id="residual-sum-of-squares">Residual Sum of Squares</h2><p><strong>Residual</strong>: <span class="math inline">\(e_i = y_i−\hat{y_i}\)</span> represents the ith residual—this is the difference between residual the ith observed response value and the ith response value that is predicted by our linear model. <span class="math display">\[\begin{align}RSS(\beta)&amp;=e_1^2+e_2^2+e_3^2+...e_n^2 \\&amp;=\sum_{i=1}^{N}(y_{i}-f(x_{i}))^2 \\&amp;=\sum_{i=1}^{N}(y_{i}-\beta_{0}-\sum_{j=1}^{p}X_{ij}\beta_{j})^2\end{align}\]</span></p><h4 id="solution">Solution</h4><p>Denote by <span class="math inline">\(X\)</span> the $N × (p + 1) $matrix with each row an input vector (with a 1 in the ﬁrst position), and similarly let <span class="math inline">\(y\)</span> be the <span class="math inline">\(N\)</span>-vector of outputs in the training set.</p><p><span class="math display">\[\begin{align} \min RSS(\beta)= (y-\mathbf{X}\beta)^T(y-\mathbf{X}\beta) \end{align}\]</span> A quadratic function in the <span class="math inline">\(p + 1\)</span> parameters</p><p>Taking derivatives:</p><p><span class="math display">\[\begin{align} \frac{\partial RSS}{\partial \beta}=-2\mathbf{X}^T(y-\mathbf{X}\beta) \end{align}\]</span></p><p><span class="math display">\[\begin{align} \frac{\partial^2 RSS}{\partial \beta \partial \beta^T}=2\mathbf{X}^T\mathbf{X}  \end{align}\]</span></p><p>Assuming (for the moment) that <span class="math inline">\(\mathbf{X}\)</span> has full column rank, and hence <span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> is positive deﬁnite, we set the ﬁrst derivative to zero: <span class="math inline">\(\mathbf{X}^T(y-\mathbf{X}\beta)=0\)</span></p><p><span class="math display">\[\begin{align}\Rightarrow \hat{\beta_1}&amp;=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^Ty \\&amp;=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2} \\\hat{\beta_0}&amp;=\bar{y}-\hat{\beta_1}\bar{x} \end{align}\]</span> where <span class="math inline">\(\bar{y}=\sum_{i=1}^ny_i/n\)</span>, <span class="math inline">\(\bar{x}=\sum_{i=1}^nx_i/n\)</span> are the <strong>sample means</strong>.</p><p>Fitted values at the training inputs: <span class="math inline">\(\hat{y}=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^Ty\)</span></p><p>Hat matrix: <span class="math inline">\(H=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\)</span></p><h2 id="assessing-the-accuracy-of-the-coefficient-estimates">Assessing the Accuracy of the Coefficient Estimates</h2><p>Assume that the true relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> takes the form <span class="math inline">\(Y = f(X) + \epsilon\)</span> for some unknown function <span class="math inline">\(f\)</span>, where <span class="math inline">\(\epsilon\)</span> is a mean-zero random error term.</p><p><strong>Least squares line</strong>: <span class="math display">\[\begin{align}\hat{y_i} = \hat{β_0} + \hat{β_1}x_i\end{align}\]</span> <strong>Population regression line</strong>: <span class="math display">\[\begin{align}Y=\beta_0+\beta_1X+\epsilon\end{align}\]</span> The <strong>error term</strong> is a catch-all for what we miss with this simple model: the true relationship is probably not linear, there may be other variables that cause variation in <span class="math inline">\(Y\)</span> , and there may be measurement error. We typically assume that the error term is independent of <span class="math inline">\(X\)</span>.</p><p><img src="./1_v2.png" width="600"></p><h3 id="population-v.s.-sample">Population V.S. Sample</h3><p>The true relationship is generally not known for real data, but the least squares line can always be computed using the coefficient estimates.</p><p><strong>Why there are two different lines describe the relationship between the predictor and the response?</strong></p><ul><li>The concept of these two lines is a natural extension of the standard statistical approach of using information from a sample to estimate characteristics of a large population.</li><li>The <strong>sample mean</strong> <span class="math inline">\(\bar{x}=\sum_{i=1}^nx_i/n\)</span> and the <strong>population mean</strong> <span class="math inline">\(\mu\)</span> are different, but in general the sample mean <span class="math inline">\(\bar{x}\)</span> will provide a good estimate of the population mean <span class="math inline">\(\hat{\mu}\)</span>.</li></ul><p><strong>Unbiased</strong></p><ul><li>If we use the sample mean <span class="math inline">\(\hat{\mu}\)</span> to estimate μ, this estimate is <strong>unbiased</strong>, in the sense that on average, we expect <span class="math inline">\(\hat{\mu}\)</span> to equal <span class="math inline">\(μ\)</span>.</li><li>An unbiased estimator does not systematically over- or under-estimate the true parameter.</li></ul><h3 id="standard-error">Standard Error</h3><p><strong>How accurate is the sample mean <span class="math inline">\(\hat{\mu}\)</span> as an estimate of μ?</strong></p><ul><li><strong>Standard error of <span class="math inline">\(\hat{\mu}\)</span>(SE(<span class="math inline">\(\hat{\mu}\)</span>)</strong>): average amount that this estimate <span class="math inline">\(\hat{\mu}\)</span> differs from the actual value of μ. <span class="math display">\[\begin{align}Var(\hat{\mu})=SE(\hat{\mu})^2=\frac{\sigma^2}{n}\end{align}\]</span> where <span class="math inline">\(σ\)</span> is the standard deviation of each of the realizations <span class="math inline">\(y_i\)</span> of <span class="math inline">\(Y\)</span> provided that the n observations are <strong>uncorrelated</strong>.</li></ul><p><strong>Standard Deviation V.S. Standard Error</strong></p><ul><li>The <strong>standard deviation (SD)</strong> measures the amount of variability, or dispersion, for a subject set of data from the mean</li><li>The <strong>standard error of the mean (SEM)</strong> measures how far the sample mean of the data is likely to be from the true population mean.</li></ul><p><img src="./2_v2.png" width="300"></p><p><strong>How close <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span> are to the true values <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>?</strong></p><ul><li><p><span class="math display">\[\begin{align}SE(\hat{\beta_0})^2&amp;=\sigma^2\left[\frac{1}{n}+\frac{\bar{x}^2}{\sum_{i=1}^n(x_i-\bar{x})^2}  \right]  \\SE(\hat{\beta_1})^2&amp;=\frac{\sigma^2}{\sum_{i=1}^n(x_i-\bar{x})^2} \end{align}\]</span></p><p>where <span class="math inline">\(\sigma^2 = Var(\epsilon)\)</span></p></li><li><p>For these formulas to be strictly valid, we need to assume that the errors <span class="math inline">\(\epsilon_i\)</span> for each observation are uncorrelated with common variance <span class="math inline">\(σ^2\)</span>.</p></li></ul><p><strong>Estimate <span class="math inline">\(\sigma^2\)</span></strong></p><ul><li><p><strong>residual standard error(RSE)</strong>: <span class="math inline">\(\sigma^2\)</span> is not known, but can be estimated from the data. This estimate is known as the <strong>residual standard error</strong></p></li><li><p><span class="math display">\[\begin{align}RSE=\sqrt{RSS/(n-2)}\end{align}\]</span></p></li></ul><h2 id="sampling-properties-of-beta">Sampling Properties of <span class="math inline">\(\beta\)</span></h2><p>The <u>variance–covariance</u> matrix of the least squares parameter estimates: <span class="math display">\[\begin{align} Var(\hat{\beta})=(\mathbf{X}^T\mathbf{X})^{-1}\sigma^2 \end{align}\]</span> <strong>Unbiased estimate of <span class="math inline">\(\sigma^2\)</span>:</strong> <span class="math display">\[\begin{align} \hat{\sigma}^2=\frac{1}{N-p-1}\sum^{N}_{i=1}(y_i-\hat{y_i})^2 \end{align}\]</span> Assume the deviations of <span class="math inline">\(\mathbf{Y}\)</span> around its expectation are <u>additive and Gaussian</u>: <span class="math display">\[\begin{align} Y=E(Y|X_1,...,X_p)+\epsilon=\beta_0+\sum_{j=1}^{p}X_j\beta_j+\epsilon \end{align}\]</span> where <span class="math inline">\(\epsilon \sim N(0,\sigma^2)\)</span></p><p>Thus, <span class="math inline">\(\beta\)</span> follows <u>multivariate normal distribution</u> with mean vector and variance–covariance matrix: <span class="math display">\[\begin{align}\hat{\beta} \sim N(\beta,(\mathbf{X}^T\mathbf{X})^{-1}\sigma^2 ) \end{align}\]</span> Also, a chi-squared distribution with <span class="math inline">\(N −p−1\)</span> degrees of freedom: <span class="math display">\[\begin{align} (N-p-1)\hat{\sigma}^2 \sim \sigma^2 \chi_{N-p-1}^{2} \end{align}\]</span> (<span class="math inline">\(\hat{\beta}\)</span> and <span class="math inline">\(\hat{\sigma^2}\)</span> are indep.)</p><p>We use these distributional properties to form tests of hypothesis and conﬁdence intervals for the parameters <span class="math inline">\(\beta_j\)</span>:</p><p><strong>Confidence Intervals</strong></p><ul><li><p><strong>A 95% confidence confidence interval</strong>: is defined as a range of values such that with 95% interval probability, the range will contain the true unknown value of the parameter.</p></li><li><p>For linear regression, the 95% confidence interval for <span class="math inline">\(β_1\)</span> approximately takes the form <span class="math display">\[\begin{align}&amp;\hat{\beta_1} \pm 2 \cdot SE(\hat{\beta_1})     \\&amp;SE(\hat{\beta_1}) =\frac{\sigma^2}{\sum_{i=1}^n(x_i-\bar{x})^2} \end{align}\]</span> (which relies on the assumption that the errors are Gaussian. Also, the factor of 2 in front of the <span class="math inline">\(SE(\hat{\beta_1})\)</span> term will vary slightly depending on the number of observations n in the linear regression. To be precise, rather than the number 2, it should contain the 97.5% quantile of a t-distribution with n−2 degrees of freedom.)</p><p>(which relies on the assumption that the errors are Gaussian. Also, the factor of 2 in front of the <span class="math inline">\(SE(\hat{\beta_1})\)</span> term will vary slightly depending on the number of observations n in the linear regression. To be precise, rather than the number 2, it should contain the 97.5% quantile of a t-distribution with n−2 degrees of freedom.)</p></li></ul><p><strong>In ESL book:</strong></p><p><span class="math inline">\(1-2\alpha\)</span> conﬁdence interval for <span class="math inline">\(\beta_j\)</span>: <span class="math display">\[\begin{align} (\hat{\beta_j}-z^{(1-\alpha)}\upsilon^{0.5}_j \hat{\sigma},\hat{\beta_j}+z^{(1-\alpha)}\upsilon^{0.5}_j \hat{\sigma}) \end{align}\]</span> where <span class="math inline">\(z^{(1-\alpha)}\)</span> is the 1 − α percentile of the normal distribution. <span class="math inline">\(\alpha\)</span> could be 0.025, 0.5, etc.</p><p>where <span class="math inline">\(z^{(1-\alpha)}\)</span> is the 1 − α percentile of the normal distribution. <span class="math inline">\(\alpha\)</span> could be 0.025, 0.5, etc.</p><p>In a similar fashion we can obtain an approximate confidence set for the entire parameter vector <span class="math inline">\(\beta\)</span>, namely <span class="math display">\[\begin{equation}C_\beta = \left\{ \beta \big| (\hat\beta-\beta)^T\mathbf{X}^T\mathbf{X}(\hat\beta-\beta) \le \hat\sigma^2{\chi^2_{p+1}}^{(1-\alpha)}\right\},\end{equation}\]</span> This condifence set for <span class="math inline">\(\beta\)</span> generates a corresponding confidence set for the true function <span class="math inline">\(f(x) = x^T\beta\)</span>: <span class="math inline">\(\left\{ x^T\beta \big| \beta \in C_\beta \right\}\)</span></p><h2 id="inference">Inference</h2><h3 id="hypothesis-tests">Hypothesis Tests</h3><p>The most common hypothesis test involves testing the <strong>null test hypothesis</strong> of</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">H_0: There is no relationship between X and Y or β1=0</span><br></pre></td></tr></table></figure><p>versus the <strong>alternative hypothesis</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">H_a : There is some relationship between X and Y or β1≠0</span><br></pre></td></tr></table></figure><p>To test the null hypothesis, we need to determine whether <span class="math inline">\(\hat{\beta_1}\)</span>, our estimate for <span class="math inline">\(\beta_1\)</span>, is sufficiently far from zero that we can be confident that <span class="math inline">\(\beta_1\)</span> is non-zero <span class="math inline">\(\Rightarrow\)</span> it depends on <span class="math inline">\(SE( \hat{\beta_1}\)</span>)</p><ul><li>If <span class="math inline">\(SE( \hat{\beta_1}\)</span>) is small, then even relatively small values of <span class="math inline">\(\hat{\beta_1}\)</span> may provide strong evidence that <span class="math inline">\(\beta_1 \neq 0\)</span>, and hence that there is a relationship between X and Y</li></ul><h4 id="t-statistic"><strong>T-statistic</strong>:</h4><p>To test the hypothesis that a particular coeﬃcient <span class="math inline">\(\beta_j= 0\)</span>, we form the standardized coeﬃcient or Z-score <span class="math display">\[\begin{align}t=\frac{\hat{\beta_1}-0}{SE(\hat{\beta_1})}  \\ or \quad z_j=\frac{\hat{\beta_j}-0}{\hat{\sigma}\sqrt{\upsilon_j}}\end{align}\]</span> where <span class="math inline">\(\upsilon_j\)</span> is the j-th diagonal element of <span class="math inline">\((\mathbf{X}^T\mathbf{X})^{-1}\)</span></p><p>which measures the number of standard deviations that <span class="math inline">\(\hat{\beta_1}\)</span> is away from 0.If there really is no relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> , then we expect it will have a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n−2\)</span> degrees of freedom.</p><p>Under the null hypothesis that <span class="math inline">\(\beta_j= 0\)</span>, <span class="math inline">\(z_j\)</span> is distributed as <span class="math inline">\(t_{N-p-1}\)</span>, and hence a large (absolute) value of <span class="math inline">\(z_j\)</span> will lead to rejection of this null hypothesis. If <span class="math inline">\(\hat{\sigma}\)</span> is replaced by a known value <span class="math inline">\(σ\)</span>, then <span class="math inline">\(z_j\)</span> would have a standard normal distribution.</p><h4 id="p-value"><strong>p-value</strong></h4><ul><li>The probability of observing any value <span class="math inline">\(≥ t\)</span> or <span class="math inline">\(≤ -t\)</span>, assuming <span class="math inline">\(β_1 = 0\)</span>.</li></ul><p><img src="./3_v2.png" width="300"> (Here |t|=2.17, p-value=0.015.The area in red is 0.015 + 0.015 = 0.030, 3%. If we had chosen a significance level of 5%, this would mean that we had achieved statistical significance. We would reject the null hypothesis in favor of the alternative hypothesis.)</p><ul><li><strong>Interpretation</strong>:a small p-value indicates<ul><li>It is unlikely to observe such a substantial association between the predictor and the response due to LUCK, in the absence of any real association between the predictor and the response.</li><li>There is an association between the predictor and the response.</li><li>We reject the null hypothesis—that is, we declare a relationship to exist between X and Y</li></ul></li></ul><h4 id="f-statistic"><strong>F-statistic</strong>:</h4><p>To test if a categorical variable with <span class="math inline">\(k\)</span> levels can be excluded from a model, we need to test whether the coeﬃcients of the dummy variables used to represent the levels can all be set to zero. Here we use the <span class="math inline">\(F\)</span> statistic：</p><p><span class="math display">\[\begin{align} F=\frac{(RSS_0-RSS_1)/(p_1-p_0)}{RSS_1/(N-p_1-1)} \end{align}\]</span></p><ul><li><span class="math inline">\(RSS_1\)</span> is the residual sum-of-squares for the least squares ﬁt of the bigger model with <span class="math inline">\(p_1+1\)</span> parameters;</li><li><span class="math inline">\(RSS_0\)</span> the same for the nested smaller model with <span class="math inline">\(p_0 +1\)</span> parameters, having <span class="math inline">\(p_1 −p_0\)</span> parameters constrained to be zero.</li></ul><p>The <span class="math inline">\(F\)</span> statistic measures the change in residual sum-of-squares per additional parameter in the bigger model, and it is normalized by an estimate of <span class="math inline">\(\sigma^2\)</span>.</p><p>Under the Gaussian assumptions, and the null hypothesis that the smaller model is correct, the <span class="math inline">\(F\)</span> statistic will have a <span class="math inline">\(F_{p_1-p_0,N-p_1-1}\)</span> distribution.</p><h2 id="the-gaussmarkov-theorem">The Gauss–Markov Theorem</h2><p>We focus on estimation of any linear combination of the parameters <span class="math inline">\(\theta=\alpha^T\beta\)</span>, for example, predictions <span class="math inline">\(f(x_0)=x^T_0\beta\)</span> are of this form.The least squares estimate of <span class="math inline">\(\alpha^T\beta\)</span> is: <span class="math display">\[\begin{equation}\hat{\theta}=\alpha^T\hat{\beta}=\alpha^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\end{equation}\]</span> Considering <span class="math inline">\(\mathbf{X}\)</span> to be ﬁxed, this is a linear function <span class="math inline">\(\mathbf{c}^T_0\mathbf{y}\)</span> of the response vector <span class="math inline">\(\mathbf{y}\)</span>.If we assume that the linear model is correct, <span class="math inline">\(\alpha^T\hat{\beta}\)</span> is unbiased.</p><p>The Gauss–Markov theorem states that if we have any other linear estimator <span class="math inline">\(\tilde{\theta}=\mathbf{c}^T\mathbf{y}\)</span> that is unbiased for <span class="math inline">\(\alpha^T\beta\)</span>, that is, <span class="math inline">\(E(\mathbf{c}^T\mathbf{y})= \alpha^T\beta\)</span>, then: <span class="math display">\[\begin{equation}Var(\alpha^T\hat{\beta})\leq Var(\mathbf{c}^T\mathbf{y})\end{equation}\]</span></p><h2 id="assessing-the-accuracy-of-the-model">Assessing the Accuracy of the Model</h2><p>The quality of a linear regression fit is typically assessed using two related quantities: <strong>the residual standard error (RSE)</strong> and the <strong>R2</strong> statistic.</p><h3 id="residual-standard-error">Residual Standard Error</h3><p>The <span class="math inline">\(RSE\)</span> is <u>an estimate of the standard deviation of <span class="math inline">\(\epsilon\)</span>: t</u>he average amount that the response will deviate from the true regression line <span class="math display">\[\begin{align}RSS&amp;=\sum_{i=1}^n(y_i-\hat{y})^2  \\RSE&amp;=\sqrt{\frac{1}{n-2}RSS}=\sqrt{\frac{1}{n-2}\sum_{i=1}^n(y_i-\hat{y})^2}\end{align}\]</span> <img src="./4_v2.png" width="600"></p><p>In the case of the advertising data, we see from the linear regression output in Table 3.2 that the RSE is 3.26. In other words, actual sales in each market deviate from the true regression line by approximately 3,260 units, on average.</p><p>The mean value of sales over all markets is approximately 14,000 units, and so the percentage error is 3,260/14,000 = 23%.</p><p><u>The RSE is considered a measure of the <strong>lack of fit</strong> of the model <span class="math inline">\(Y = β_0 + β_1X + \epsilon\)</span> to the data</u>.</p><h3 id="r2-statistic">R2 Statistic</h3><p>The <span class="math inline">\(RSE\)</span> is measured in the units of <span class="math inline">\(Y\)</span> , it is not always clear what constitutes a good <span class="math inline">\(RSE\)</span>.</p><p>The <span class="math inline">\(R^2\)</span> statistic takes the form of a <strong>proportion</strong>—<u>the proportion of variance explained</u>—and so it always takes on a value between 0 and 1, and isi ndependent of the scale of <span class="math inline">\(Y\)</span> . <span class="math display">\[\begin{align}R^2 = (TSS − RSS)/TSS= 1− RSS/TSS =  1-\frac{\sum(y_i-\hat{y})^2}{\sum(y_i-\bar{y})^2}\end{align}\]</span> <strong>TSS(total sum of squares)</strong>: <span class="math inline">\(\sum(y_i-\bar{y})^2\)</span> - the amount of variability inherent in the response before the regression is performed</p><p><strong>RSS</strong>: <span class="math inline">\(\sum_{i=1}^n(y_i-\hat{y})^2\)</span> - the amount of variabilityt hat is left unexplained after performing the regression</p><p><strong>(TSS−RSS)</strong>: measures the amount of variability in the response that is explained (or removed) by performing the regression, and <strong><span class="math inline">\(R^2\)</span> measures the proportion of variability in <span class="math inline">\(Y\)</span> that can be explained using <span class="math inline">\(X\)</span>.</strong></p><p><strong>Interpretation</strong>：</p><ul><li><u>close to 1</u> : a large proportion of the variability in the response has been explained by the regression.</li><li><u>close to 0</u> : the regression did not explain much of the variability in the response<ul><li>The linear model is wrong</li><li>The inherent error <span class="math inline">\(σ^2\)</span> is high, or both.</li></ul></li></ul><h2 id="squared-correlation-v.s.-r2-statistic">Squared Correlation V.S. R2 Statistic</h2><p><strong>Correlation</strong>: <span class="math display">\[\begin{align}Cor(X,Y)=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^n(x_i-\bar{x})^2}\sqrt{\sum_{i=1}^n(y_i-\bar{y})^2}}\end{align}\]</span> is also a measure of the linear relationship between X and Y.</p><blockquote><p>In the simple linear regression setting, <span class="math inline">\(R^2 = [Cor]^2\)</span>. In other words, the squared correlation and the R2 statistic are identical</p></blockquote><h2 id="note-of-p-value-v.s.-r-square">Note of P-Value v.s. R-Square</h2><p>Referring answer fromn <a href="https://www.researchgate.net/profile/Faye_Anderson3" target="_blank" rel="noopener">Faye Anderson</a>:</p><p>There is no established association/relationship between p-value and R-square. This all depends on the data (i.e.; contextual).</p><p>R-square value tells you how much variation is explained by your model. So 0.1 R-square means that your model explains 10% of variation within the data. The greater R-square the better the model. Whereas p-value tells you about the F statistic hypothesis testing of the &quot;fit of the intercept-only model and your model are equal&quot;. So if the p-value is less than the significance level (usually 0.05) then your model fits the data well.</p><p><u>Thus you have four scenarios:</u></p><p><strong>1) low R-square and low p-value (p-value &lt;= 0.05) :</strong> means that your model doesn't explain much of variation of the data but it is significant (better than not having a model)</p><p><strong>2) low R-square and high p-value (p-value &gt; 0.05) :</strong> means that your model doesn't explain much of variation of the data and it is not significant (worst scenario)</p><p><strong>3) high R-square and low p-value :</strong> means your model explains a lot of variation within the data and is significant (best scenario)</p><p><strong>4) high R-square and high p-value :</strong> means that your model explains a lot of variation within the data but is not significant (model is worthless)</p><h2 id="variance-bias">Variance &amp; Bias</h2><p>Consider the mean squared error of an estimator <span class="math inline">\(\tilde{\theta}\)</span> in estimating <span class="math inline">\(\theta\)</span>:</p><p><span class="math display">\[\begin{align}MSE(\tilde{\theta})&amp;= E(\tilde{\theta}-\theta)^2 \\&amp;= E(\tilde{\theta^2}+\theta^2-2\theta\tilde{\theta}) \\&amp;= E(\tilde{\theta^2})-E^2(\tilde{\theta})+E^2(\tilde{\theta})+E(\theta^2-2\theta\tilde{\theta})\\&amp;= Var(\tilde{\theta})+[E(\tilde{\theta})-\theta]^2\end{align}\]</span> The ﬁrst term is the <strong>variance</strong>, while the second term is the <strong>squared bias</strong>.</p><p>The <strong>Gauss-Markov theorem</strong> implies that the <em>least squares estimator</em> has the smallest mean squared error of all linear estimators with no bias. However, there may well exist a biased estimator with smaller mean squared error. <font color="red">Such an estimator would trade a little bias for a larger reduction in variance.</font></p><p>From a more pragmatic point of view, most models are distortions of the truth, and hence are biased; picking the right model amounts to creating the right balance between bias and variance.</p><p>Mean squared error is intimately related to prediction accuracy. Consider the prediction of the new response at input <span class="math inline">\(x_0\)</span>, <span class="math display">\[\begin{equation}y_0=f(x_0)+\epsilon_0\end{equation}\]</span></p><h2 id="prediction-error-mse">Prediction error &amp; MSE</h2><p>The expected prediction error of an estimate <span class="math inline">\(\tilde{f}(x_0)=x_0^T\tilde{\beta}\)</span>:</p><p><span class="math display">\[\begin{align}E(y_0-\tilde{f}(x_0))^2 &amp;=E(f(x_0)+\epsilon_0-x_0^T\tilde{\beta})^2 \\&amp;=E(\epsilon_0^2)+E(f(x_0)-x_0^T\tilde{\beta})^2-2E(\epsilon_0(f(x_0)-x_0^T\tilde{\beta})) \\&amp;=\sigma^2+E(f(x_0)-x_0^T\tilde{\beta})^2 \\&amp;=\sigma^2+MSE(\tilde{f}(x_0))\end{align}\]</span> Therefore, expected prediction error and mean squared error diﬀer only by the constant <span class="math inline">\(\sigma^2\)</span>, representing the variance of the new observation <span class="math inline">\(y_0\)</span>.</p><h1 id="multiple-linear-regression">Multiple Linear Regression</h1><p>Multiple linear regression model takes the form: <span class="math display">\[\begin{align}Y=\beta_0+\beta_1X_1+,,,+\beta_pX_p+\epsilon\end{align}\]</span></p><h2 id="estimating-the-regression-coefficients">Estimating the Regression Coefficients</h2><p>We choose <span class="math inline">\(β_0, β_1, . . . , β_p\)</span> to minimize the sum of squared residuals <span class="math display">\[\begin{align}RSS&amp;=\sum_{i=1}^n(y_i-\hat{y}_i)^2 \\&amp;=\sum_{i=1}^n(y_i-\hat{\beta_0}-\hat{\beta_1}x_{i1}-,,,-\hat{\beta_p}x_{ip})^2\end{align}\]</span> <img src="./5_v3.png" width="600"></p><p><strong>Does it make sense for the multiple regression to suggest no relationship between <em>sales</em> and <em>newspaper</em> while the simple linear regression implies the opposite?</strong></p><ul><li>Notice that the correlation between radio and newspaper is 0.35.</li><li>In markets where we spend more on radio our sales will tend to be higher, and as our correlation matrix shows, we also tend to spend more on newspaper advertising in those same markets.</li><li>Hence, in a simple linear regression which only examines sales versus newspaper, we will observe that higher values of newspaper tend to be associated with higher values of sales, even though newspaper advertising does not actually affect sales.</li></ul><h2 id="some-important-questions">Some Important Questions</h2><h3 id="is-there-a-relationship-between-the-response-and-predictors">1. Is There a Relationship Between the Response and Predictors?</h3><h4 id="hypothesis-test"><strong>Hypothesis Test</strong></h4><p>We use a hypothesis test to answer this question.</p><p>We test the <strong>null hypothesis</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">H_0 : β1 = β2 = · · · = βp = 0</span><br></pre></td></tr></table></figure><p>versus the <strong>alternative</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">H_a : at least one βj is non-zero</span><br></pre></td></tr></table></figure><p>This hypothesis test is performed by computing the <strong>F-statistic</strong>, <span class="math display">\[\begin{align}F=\frac{(TSS-RSS)/p}{RSS/(n-p-1)}\end{align}\]</span> where <span class="math inline">\(TSS =\sum(y_i − \bar{y})^2\)</span> and <span class="math inline">\(RSS =\sum(y_i−\hat{y}_i)^2\)</span>.</p><p>If the linear model assumptions are correct, one can show that <span class="math display">\[\begin{align}E[RSS/(n-p-1)]=\sigma^2\end{align}\]</span> and that, provided <span class="math inline">\(H_0\)</span> is true, <span class="math display">\[\begin{align}E[(TSS-RSS)/p]=\sigma^2\end{align}\]</span></p><ul><li>When there is no relationship between the response and predictors, one would expect the F-statistic to take on a value close to 1.</li><li>On the other hand, if <span class="math inline">\(H_a\)</span> is true, then <span class="math inline">\(E[(TSS-RSS)/p]&gt;\sigma^2\)</span>, so we expect <span class="math inline">\(F\)</span> to be greater than 1.</li></ul><p><img src="./6_v3.png" width="600"></p><h3 id="how-large-does-the-f-statistic-need-to-be-before-we-can-reject-h0-and-conclude-that-there-is-a-relationship"><strong>2. How large does the F-statistic need to be before we can reject H0 and conclude that there is a relationship?</strong></h3><ul><li>When n is large, an F-statistic that is just a little larger than 1 might still provide evidence against H_0.</li><li>In contrast, a larger F-statistic is needed to reject H_0 if n is small.</li><li>For the advertising data, the <strong>p-value</strong> associated with the F-statistic in Table 3.6 is essentially zero, so we have extremely strong evidence that at least one of the media is associated with increased sales.</li></ul><p><strong>To test that a particular subset of q of the coefficients are zero</strong></p><p>This corresponds to a null hypothesis</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">H_0 : β(p-q+1) = β(p-q+2) = · · · = βp = 0</span><br></pre></td></tr></table></figure><p>In this case we fit a second model that uses all the variables <strong>except those last q</strong>. Suppose that the residual sum of squares for that model is <span class="math inline">\(RSS_0\)</span>. Then the appropriate F-statistic is <span class="math display">\[\begin{align}F=\frac{(RSS_0-RSS)/q}{RSS/(n-p-1)}\end{align}\]</span></p><h4 id="f-statistics-v.s.-t-statistics"><strong>F-statistics v.s. t-statistics</strong></h4><ul><li><strong>Equivalency</strong>: In Table 3.4, for each individual predictor a t-statistic and a p-value were reported. These provide information about whether each individual predictor is related to the response, after adjusting for the other predictors. It turns out that each of these are exactly equivalent to the F-test that omits that single variable from the model, leaving all the others in—i.e. q=1 in the model. So it reports the <strong>partial effect</strong> of adding that variable to the model.</li></ul><blockquote><p>The square of each <em>t-statistic</em> is the corresponding <em>F-statistic</em>.</p></blockquote><ul><li><strong>p is large</strong>: If any one of the p-values for the individual variables is very small, then <strong><em>at least one of the predictors is related to the response</em></strong>. However, this logic is flawed, especially when the number of predictors p is large.<ul><li>If we use the individual t-statistics and associated p-values to decide whether there is any association between the variables and the response, high chance we will incorrectly conclude there is a relationship.</li><li>However, the F-statistic does not suffer from this problem because it adjusts for the number of predictors.</li></ul></li><li><strong>p &gt; n</strong>: more coefficients βj to estimate than observations from which to estimate them.<ul><li>cannot even fit the multiple linear regression model using least squares,</li></ul></li></ul><h3 id="do-all-the-predictors-help-to-explain-y-or-is-only-a-subset-of-the-predictors-useful">3. Do all the predictors help to explain Y , or is only a subset of the predictors useful?</h3><h4 id="variable-selection"><strong>Variable Selection</strong></h4><ul><li>Various statistics can be used to judge the quality of a model:<ul><li><strong>Mallow’s Cp, Akaike informa-Mallow’s Cp tion criterion (AIC)</strong></li><li><strong>Bayesian information criterion (BIC)</strong></li><li><strong>adjusted R2</strong></li></ul></li><li>There are three classical approaches to select models:<ul><li><strong>Forward selection</strong></li><li><strong>Backward selection</strong></li><li><strong>Mixed selection</strong></li></ul></li></ul><h3 id="how-well-does-the-model-fit-the-data">4. How well does the model fit the data?</h3><p>Two of the most common numerical measures of model fit are the <strong>RSE</strong> and <strong><span class="math inline">\(R^2\)</span></strong></p><h4 id="r2-statistics">R2 Statistics</h4><p>An <span class="math inline">\(R^2\)</span> value close to 1 indicates that the model explains a large portion of the variance in the response variable. <span class="math display">\[\begin{align}R^2 = (TSS − RSS)/TSS= 1− RSS/TSS\end{align}\]</span> Recall that in simple regression, <span class="math inline">\(R^2\)</span> is the square of the correlation of the response and the variable. In multiple linear regression, it turns out that it equals <span class="math inline">\(Cor(Y, \hat{Y} )^2\)</span>, the square of the correlation between the response and the fitted linear model; in fact one property of the fitted linear model is that it maximizes this correlation among all possible linear models.</p><p><strong><span class="math inline">\(R^2\)</span> will always increase when more variables are added to the model, even if those variables are only weakly associated with the response.</strong></p><ul><li>This is due to the fact that adding another variable tothe least squares equations must allow us to fit the training data (though not necessarily the testing data) more accurately.</li><li>The fact that adding <em>newspaper</em> advertising to the model containing only TV and radio advertising leads to just a tiny increase in R2 provides additional evidence that newspaper can be dropped from the model.</li></ul><h4 id="rse">RSE</h4><p><strong>RSE</strong> is defined as <span class="math display">\[\begin{align}RSE=\sqrt{\frac{RSS}{n-p-1}}\end{align}\]</span> Models with more variables can have higher RSE if the decrease in RSS is small relative to the increase in p.</p><h4 id="graphical-summaries">Graphical summaries</h4><p><img src="./7_v3.png" width="600"> It suggests a <strong>synergy</strong> or <strong>interaction</strong> effect between the advertising media, whereby combining the media together results in a bigger boost to sales than using any single medium</p><h3 id="given-a-set-of-predictor-values-what-response-value-should-we-predict-and-how-accurate-is-our-prediction">5. Given a set of predictor values, what response value should we predict, and how accurate is our prediction?</h3><h4 id="uncertainty-associated-with-prediction"><strong>Uncertainty associated with prediction</strong></h4><ol type="1"><li>The coefficient estimates <span class="math inline">\(\hat{\beta_0},\hat{\beta_1},...,\hat{\beta_p}\)</span> are estimates for <span class="math inline">\(β_0, β_1, . . . , β_p\)</span>. That is, the <strong>least squares plane</strong> <span class="math display">\[\begin{align}\hat{Y}=\hat{\beta_0}+\hat{\beta_1}X_1+,...+\hat{\beta_p}X_p\end{align}\]</span> is only an estimate for the <strong>true population regression plane</strong> <span class="math display">\[\begin{align}f(X)=\beta_0+\beta_1X_1+,...+\beta_pX_p\end{align}\]</span></li></ol><ul><li>The inaccuracy in the coefficient estimates is related to the <strong>reducible error</strong>.</li><li>We can compute a <strong>confidence interval</strong> in order to determine how close <span class="math inline">\(\hat{Y}\)</span> will be to f(X).</li></ul><ol start="2" type="1"><li>In practice assuming a linear model for <span class="math inline">\(f(X)\)</span> is almost always an approximation of reality, so there is an additional source of potentially <strong>reducible error</strong> which we call <strong>model bias</strong>.</li><li>Even if we knew <span class="math inline">\(f(X)\)</span>—true values for <span class="math inline">\(β_0, β_1, . . . , β_p\)</span>—the response value cannot be predicted perfectly because of the random error <span class="math inline">\(\epsilon\)</span> --<strong>irreducible error</strong>.</li></ol><ul><li>How much will Y vary from <span class="math inline">\(\hat{Y}\)</span>? -- <strong>prediction intervals</strong></li></ul><h4 id="prediction-intervals"><strong>Prediction intervals</strong></h4><p><strong>Prediction intervals</strong> are always wider than <strong>confidence intervals</strong></p><ul><li>Because they incorporate both <em>the error in the estimate for f(X) (the reducible error) and the uncertainty as to how much an individual point will differ from the population regression plane (the irreducible error).</em></li></ul><p>E.g.</p><ul><li><strong>confidence interval</strong> : quantify the uncertainty surrounding the average sales over a large number of cities.</li><li><strong>prediction interval</strong> : quantify the uncertainty surrounding sales for a particular city.</li></ul><h2 id="multiple-regression-from-simple-univariate-regression">Multiple Regression from Simple Univariate Regression</h2><p>Gram-Schmidt正交化</p><h3 id="simple-univariate-regression">Simple Univariate Regression</h3><p>Suppose ﬁrst that we have a univariate model with no intercept: <span class="math display">\[\begin{align}\mathbf{Y}=\mathbf{X}\beta+\epsilon\end{align}\]</span> The least squares estimate and residuals are: <span class="math display">\[\begin{align}\hat{\beta}&amp;=\frac{\sum_1^Nx_iy_i}{\sum_1^Nx_i^2} \\r_i&amp;=y_i-x_i\hat{\beta}\end{align}\]</span> The least squares estimate and residuals are: <span class="math display">\[\begin{align}\hat{\beta}&amp;=\frac{\sum_1^Nx_iy_i}{\sum_1^Nx_i^2} \\r_i&amp;=y_i-x_i\hat{\beta}\end{align}\]</span> Let <span class="math inline">\(\mathbf{y}=(y_1,...,y_N)^T\)</span>,<span class="math inline">\(\mathbf{x}=(x_1,...,x_N)^T\)</span></p><p>Define the <strong>inner product</strong> between <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>: <span class="math display">\[\begin{align}&lt;\mathbf{x},\mathbf{y}&gt;=\sum_1^Nx_iy_i=\mathbf{x}^T\mathbf{y}\end{align}\]</span> Then, <span class="math display">\[\begin{align}\hat{\beta}&amp;=\frac{&lt;\mathbf{x},\mathbf{y}&gt;}{&lt;\mathbf{x},\mathbf{x}&gt;} \\\mathbf{r}&amp;=\mathbf{y}-\mathbf{x}\hat{\beta}\end{align}\]</span> Suppose the inputs <span class="math inline">\(x_1, x_2,..., x_p\)</span> (the columns of the data matrix <span class="math inline">\(\mathbf{X}\)</span>) are orthogonal; that is <span class="math inline">\(&lt;\mathbf{x_k},\mathbf{x_j}&gt;=0\)</span>. Then the multiple least squares estimates <span class="math inline">\(\hat{\beta_j}\)</span> are equal to <span class="math inline">\(\frac{&lt;\mathbf{x_j},\mathbf{y}&gt;}{&lt;\mathbf{x_j},\mathbf{x_j}&gt;}\)</span>—the univariate estimates. In other words, <font color="red">when the inputs are orthogonal, they have no eﬀect on each other’s parameter estimates in the model.</font></p><h3 id="orthogonalization">Orthogonalization</h3><p><span class="math display">\[\begin{align}\hat{\beta}_1&amp;=\frac{&lt;\mathbf{x}-\bar{x}\mathbf{1},\mathbf{y}&gt;}{&lt;\mathbf{x}-\bar{x}\mathbf{1},\mathbf{x}-\bar{x}\mathbf{1}&gt;} \\\end{align}\]</span></p><ul><li><span class="math inline">\(\bar{x}=\sum_ix_i/N\)</span>;</li><li><span class="math inline">\(\mathbf{1}\)</span>, the vector of N ones;</li></ul><p><strong>Steps:</strong> 1. regress <span class="math inline">\(\mathbf{x}\)</span> on <span class="math inline">\(\mathbf{1}\)</span> to produce the residual <span class="math inline">\(\mathbf{z}=\mathbf{x}-\bar{x}\mathbf{1}\)</span>; 2. regress <span class="math inline">\(\mathbf{y}\)</span> on the residual <span class="math inline">\(\mathbf{z}\)</span> to give the coeﬃcient <span class="math inline">\(\hat{\beta}_1\)</span></p><p>Regress <span class="math inline">\(\mathbf{a}\)</span> on <span class="math inline">\(\mathbf{b}\)</span> (<span class="math inline">\(\mathbf{b}\)</span> is adjusted for <span class="math inline">\(\mathbf{a}\)</span>),(or <span class="math inline">\(\mathbf{b}\)</span> is <strong>“orthogonalized”</strong> with respect to <span class="math inline">\(\mathbf{a}\)</span>); a simple univariate regression of <span class="math inline">\(\mathbf{b}\)</span> on a with no intercept, producing coeﬃcient <span class="math inline">\(\hat{\lambda}=\frac{&lt;\mathbf{a},\mathbf{b}&gt;}{&lt;\mathbf{a},\mathbf{a}&gt;}\)</span> and residual vector $ - $.</p><p>The orthogonalization does not change the subspace spanned by <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, it simply produces an <strong>orthogonal basis</strong> for representing it.</p><h3 id="gramschmidt-procedure-for-multiple-regression">Gram–Schmidt procedure for multiple regression</h3><p><strong>ALGORITHM 3.1 Regression by Successive Orthogonalization</strong></p><ol type="1"><li>Initialize <span class="math inline">\(\mathbf{z_0}=\mathbf{x_0}=\mathbf{1}\)</span>.</li><li>For j=1,2,...,1,,...,p,<br> Regress <span class="math inline">\(\mathbf{x_j}\)</span> on <span class="math inline">\(\mathbf{z_0},\mathbf{z_1},...,\mathbf{z_{j-1}}\)</span> to produce coeﬃcients <span class="math inline">\(\hat{\lambda}_{l,j}=\frac{&lt;\mathbf{z_l},\mathbf{x_j}&gt;}{&lt;\mathbf{z_l},\mathbf{z_l}&gt;}\)</span>, l=0,1,...,j-1, and residual vector <span class="math inline">\(\mathbf{z_j}=\mathbf{x_j}-\sum_{k=0}^{j-1}\hat{\lambda_{kj}}\mathbf{z_k}\)</span></li><li>Regress <span class="math inline">\(\mathbf{y}\)</span> on the residual <span class="math inline">\(\mathbf{z_p}\)</span> to give the estimate <span class="math inline">\(\hat{\beta_p}=\frac{&lt;\mathbf{z_p},\mathbf{y}&gt;}{&lt;\mathbf{z_p},\mathbf{z_p}&gt;}\)</span>.</li></ol><p><strong>Note:</strong></p><ul><li>Each of the <span class="math inline">\(\mathbf{x}_j\)</span> is a linear combination of the <span class="math inline">\(\mathbf{z}_k\)</span>, <span class="math inline">\(k ≤ j\)</span>.</li><li>Since the <span class="math inline">\(\mathbf{z}_j\)</span> are all orthogonal, they form a basis for the column space of <span class="math inline">\(\mathbf{X}\)</span>, and hence the least squares projection onto this subspace is <span class="math inline">\(\mathbf{\hat{y}}\)</span>.</li><li>By rearranging the <span class="math inline">\(x_j\)</span> , any one of them could be in the last position, and a similar results holds.</li><li>The multiple regression coeﬃcient <span class="math inline">\(\mathbf{x}_j\)</span> represents the additional contribution of <span class="math inline">\(\mathbf{x}_j\)</span> on <span class="math inline">\(\mathbf{y}\)</span>, after <span class="math inline">\(\mathbf{x}_j\)</span> has been adjusted for <span class="math inline">\(x_0, x_1,..., x_{j−1},x_{j+1},..., x_p\)</span>.</li></ul><h3 id="precision-of-coefficient-estimation">Precision of Coefficient Estimation</h3><p>If <span class="math inline">\(\mathbf{x}_p\)</span> is highly correlated with some of the other <span class="math inline">\(\mathbf{x}_k\)</span>’s, the residual vector <span class="math inline">\(\mathbf{z}_p\)</span> will be close to zero, and the coeﬃcient <span class="math inline">\(\mathbf{x}_j\)</span> will be very unstable.</p><p>From <span class="math inline">\(\hat{\beta_p}=\frac{&lt;\mathbf{z_p},\mathbf{y}&gt;}{&lt;\mathbf{z_p},\mathbf{z_p}&gt;}\)</span>, we also obtain an alternate formula for the variance estimates:</p><p><span class="math display">\[\begin{align}Var(\hat{\beta}_p)=\frac{\sigma^2}{&lt;\mathbf{z_p},\mathbf{z_p}&gt;}=\frac{\sigma^2}{||\mathbf{z_p}||^2} \end{align}\]</span></p><p>The precision with which we can estimate <span class="math inline">\(\hat{\beta_p}\)</span> depends on the length of the residual vector <span class="math inline">\(\mathbf{z_p}\)</span>; this represents how much of <span class="math inline">\(\mathbf{x_p}\)</span> is unexplained by the other <span class="math inline">\(\mathbf{x_k}\)</span>’s</p><h3 id="qr-decomposition">QR decomposition</h3><p>We can represent step 2 of Algorithm 3.1 in matrix form:</p><p><span class="math display">\[\begin{align}\mathbf{X}=\mathbf{Z}\mathbf{Γ}\end{align}\]</span></p><ul><li><span class="math inline">\(\mathbf{Z}\)</span> has as columns the <span class="math inline">\(\mathbf{z_j}\)</span> (in order)</li><li><span class="math inline">\(\mathbf{Γ}\)</span> is the upper triangular matrix with entries <span class="math inline">\(\hat{\lambda}_{kj}\)</span></li></ul><p>Introducing the diagonal matrix D with jth diagonal entry <span class="math inline">\(D_{jj} = ||\mathbf{z_j}||\)</span>, we get</p><p><strong>QR decomposition of X</strong>:</p><p><span class="math display">\[\begin{align}\mathbf{X}=\mathbf{Z}\mathbf{D}^{-1}\mathbf{D}\mathbf{Γ}=\mathbf{Q}\mathbf{R}\end{align}\]</span></p><ul><li><span class="math inline">\(\mathbf{Q}\)</span> is an <span class="math inline">\(N ×(p+1)\)</span> orthogonal matrix, <span class="math inline">\(Q^TQ=I\)</span>;</li><li><span class="math inline">\(\mathbf{R}\)</span> is a $(p +1) × (p + 1) $Vupper triangular matrix.</li></ul><p><strong>Least squares solution:</strong> <span class="math display">\[\begin{align}\hat{\beta}&amp;=R^{-1}Q^T\mathbf{y} \\\mathbf{\hat{y}}&amp;=QQ^T\mathbf{y}\end{align}\]</span></p><h2 id="multiple-outputs">3.2.4 Multiple Outputs</h2><p>Suppose we have multiple outputs Y1,Y2,...,YK that we wish to predict from our inputs X0,X1,X2,...,Xp. We assume a linear model for each output:</p><p><span class="math display">\[\begin{align}Y_k&amp;=\beta_{0k}+\sum_{j=1}^pX_j\beta_{jk}+\epsilon_k \\&amp;=f_k(X)+\epsilon_k\end{align}\]</span></p><p>With N training cases we can write the model in matrix notation:</p><ul><li><p><span class="math display">\[\begin{align}Y=XB+E\end{align}\]</span></p></li><li><p>Y: N×K response matrix</p></li><li><p>X: N×(p+1) input matrix</p></li><li><p>B: (p+1)× K matrix of parameters</p></li><li><p>E: N×K matrix of errors</p></li></ul><p>A straightforward generalization of the univariate loss function:</p><p><span class="math display">\[\begin{align}RSS(B)&amp;=\sum_{k=1}^K\sum_{i=1}^N(y_{ik}-f_k(x_i))^2 \\&amp;=tr[(Y-XB)^T(Y-XB)]\end{align}\]</span> The least squares estimates have exactly the same form as before: <span class="math display">\[\begin{align}\hat{B}=(X^TX)^{-1}X^Ty\end{align}\]</span> If the errors <span class="math inline">\(\epsilon =(\epsilon_1,...,\epsilon_K)\)</span> in are correlated, suppose <span class="math inline">\(Cov(\epsilon)= Σ\)</span>, then the multivariate weighted criterion: <span class="math display">\[\begin{align}RSS(B;Σ)&amp;=\sum_{i=1}^N(y_{ik}-f_k(x_i))^TΣ^{-1}(y_{ik}-f_k(x_i)) \end{align}\]</span></p><h1 id="comparison-of-linear-regression-with-k-nearest-neighbors">Comparison of Linear Regression with K-Nearest Neighbors</h1><h2 id="parametric-v.s.-non-parametric">Parametric v.s. Non-parametric</h2><p>Linear regression is an example of a parametric approach because it assumes a linear functional form for f(X).</p><p><strong>Parametric methods</strong></p><ul><li><strong>Advantages</strong>:</li><li>Easy to fit, because one need estimate only a small number of coefficients.</li><li>Simple interpretations, and tests of statistical significance can be easily performed</li><li><strong>Disadvantage</strong>:</li><li>Strong assumptions about the form of f(X). If the specified functional form is far from the truth, and prediction accuracy is our goal, then the parametric method will perform poorly.</li></ul><p><strong>Non-parametric methods</strong></p><ul><li>Do not explicitly assume a parametric form for f(X), and thereby provide an alternative and more flexible approach for performing regression.</li><li>K-nearest neighbors regression (KNN regression)</li></ul><h2 id="knn-regression">KNN Regression</h2><p>Given a value for <span class="math inline">\(K\)</span> and a prediction point <span class="math inline">\(x_0\)</span>, KNN regression first identifies the <span class="math inline">\(K\)</span> training observations that are closest to <span class="math inline">\(x_0\)</span>, represented by <span class="math inline">\(N_0\)</span>. It then estimates <span class="math inline">\(f(x_0)\)</span> using the average of all the training responses in <span class="math inline">\(N_0\)</span>. <span class="math display">\[\begin{align}\hat{f}(x_0)=\frac{1}{K}\sum_{x_i\in N_0}y_i\end{align}\]</span></p><ul><li>The optimal value for K will depend on the <strong>bias-variance trade-off</strong>.</li><li>A small value for K provides the most flexible fit, which will have low bias but high variance. This variance is due to the fact that the prediction in a given region is entirely dependent on just one observation.</li><li>A larger values of K provide a smoother and less variable fit; the prediction in a region is an average of several points, and so changing one observation has a smaller effect. However, the smoothing may cause bias by masking some of the structure in f(X)</li></ul><p><strong>The parametric approach will outperform the nonparametric approach if the parametric form that has been selected is close to the true form of f.</strong></p><ul><li>A non-parametric approach incurs a cost in variance that is not offset by a reduction in bias</li><li>KNN performs slightly worse than linear regression when the relationship is linear, but much better than linear regression for non-linear situations. <img src="./26.png" width="600"></li></ul><p><strong>The increase in dimension has only caused a small deterioration in the linear regression test set MSE, but it has caused more than a ten-fold increase in the MSE for KNN.</strong></p><ul><li>This decrease in performance as the dimension increases is a common problem for KNN, and results from the fact that in higher dimensions there is effectively a reduction in sample size.<span class="math inline">\(\Rightarrow\)</span> <strong>curse of dimensionality</strong></li><li>As a general rule, parametric methods will tend to outperform non-parametric approaches when there is a small number of observations per predictor. <img src="./25.png" width="600"></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;simple-linear-regression-models&quot;&gt;Simple Linear Regression Models&lt;/h1&gt;
&lt;h2 id=&quot;linear-regression-model&quot;&gt;Linear Regression Model&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Form of the linear regression model: &lt;em&gt;&lt;span class=&quot;math inline&quot;&gt;\(f(X)=\beta_{0}+\sum_{j=1}^{p}X_{j}\beta_{j}\)&lt;/span&gt;&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Training data: (&lt;span class=&quot;math inline&quot;&gt;\(x_1\)&lt;/span&gt;,&lt;span class=&quot;math inline&quot;&gt;\(y_1\)&lt;/span&gt;) ... (&lt;span class=&quot;math inline&quot;&gt;\(x_N\)&lt;/span&gt;,&lt;span class=&quot;math inline&quot;&gt;\(y_N\)&lt;/span&gt;). Each &lt;span class=&quot;math inline&quot;&gt;\(x_{i} =(x_{i1},x_{i2},...,x_{ip})^{T}\)&lt;/span&gt; is a vector of feature measurements for the &lt;span class=&quot;math inline&quot;&gt;\(i\)&lt;/span&gt;-th case.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Goal: estimate the parameters &lt;span class=&quot;math inline&quot;&gt;\(β\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Estimation method: &lt;strong&gt;Least Squares&lt;/strong&gt;, we pick the coeﬃcients &lt;span class=&quot;math inline&quot;&gt;\(β =(β_0,β_1,...,β_p)^{T}\)&lt;/span&gt; to minimize the &lt;strong&gt;residual sum of squares&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Assumptions:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Observations &lt;span class=&quot;math inline&quot;&gt;\(y_i\)&lt;/span&gt; are uncorrelated and have constant variance &lt;span class=&quot;math inline&quot;&gt;\(\sigma^2\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(x_i\)&lt;/span&gt; are ﬁxed (non random)&lt;/li&gt;
&lt;li&gt;The regression function E(Y |X) is linear, or the linear model is a reasonable approximation.&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Linear Regression" scheme="https://nancyyanyu.github.io/tags/Linear-Regression/"/>
    
      <category term="Regression" scheme="https://nancyyanyu.github.io/tags/Regression/"/>
    
  </entry>
  
</feed>
