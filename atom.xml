<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Nancy&#39;s Notes</title>
  
  <subtitle>Code changes world!</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://nancyyanyu.github.io/"/>
  <updated>2019-10-19T23:21:01.286Z</updated>
  <id>https://nancyyanyu.github.io/</id>
  
  <author>
    <name>Nancy Yan</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Study Note: Linear Regression Example Prostate Cancer</title>
    <link href="https://nancyyanyu.github.io/posts/2ce36d51/"/>
    <id>https://nancyyanyu.github.io/posts/2ce36d51/</id>
    <published>2019-10-19T23:04:00.161Z</published>
    <updated>2019-10-19T23:21:01.286Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">import</span> scipy.stats</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data=pd.read_csv(<span class="string">'./data/prostate.data'</span>,delimiter=<span class="string">'\t'</span>,index_col=<span class="number">0</span>)</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure><a id="more"></a><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }        .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe"><thead><tr style="text-align: right;"><th></th><th>lcavol</th><th>lweight</th><th>age</th><th>lbph</th><th>svi</th><th>lcp</th><th>gleason</th><th>pgg45</th><th>lpsa</th><th>train</th></tr></thead><tbody><tr><th>1</th><td>-0.579818</td><td>2.769459</td><td>50</td><td>-1.386294</td><td>0</td><td>-1.386294</td><td>6</td><td>0</td><td>-0.430783</td><td>T</td></tr><tr><th>2</th><td>-0.994252</td><td>3.319626</td><td>58</td><td>-1.386294</td><td>0</td><td>-1.386294</td><td>6</td><td>0</td><td>-0.162519</td><td>T</td></tr><tr><th>3</th><td>-0.510826</td><td>2.691243</td><td>74</td><td>-1.386294</td><td>0</td><td>-1.386294</td><td>7</td><td>20</td><td>-0.162519</td><td>T</td></tr><tr><th>4</th><td>-1.203973</td><td>3.282789</td><td>58</td><td>-1.386294</td><td>0</td><td>-1.386294</td><td>6</td><td>0</td><td>-0.162519</td><td>T</td></tr><tr><th>5</th><td>0.751416</td><td>3.432373</td><td>62</td><td>-1.386294</td><td>0</td><td>-1.386294</td><td>6</td><td>0</td><td>0.371564</td><td>T</td></tr></tbody></table></div><p>The correlation matrix of the predictors:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.corr()</span><br></pre></td></tr></table></figure><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }        .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe"><thead><tr style="text-align: right;"><th></th><th>lcavol</th><th>lweight</th><th>age</th><th>lbph</th><th>svi</th><th>lcp</th><th>gleason</th><th>pgg45</th><th>lpsa</th></tr></thead><tbody><tr><th>lcavol</th><td>1.000000</td><td>0.280521</td><td>0.225000</td><td>0.027350</td><td>0.538845</td><td>0.675310</td><td>0.432417</td><td>0.433652</td><td>0.734460</td></tr><tr><th>lweight</th><td>0.280521</td><td>1.000000</td><td>0.347969</td><td>0.442264</td><td>0.155385</td><td>0.164537</td><td>0.056882</td><td>0.107354</td><td>0.433319</td></tr><tr><th>age</th><td>0.225000</td><td>0.347969</td><td>1.000000</td><td>0.350186</td><td>0.117658</td><td>0.127668</td><td>0.268892</td><td>0.276112</td><td>0.169593</td></tr><tr><th>lbph</th><td>0.027350</td><td>0.442264</td><td>0.350186</td><td>1.000000</td><td>-0.085843</td><td>-0.006999</td><td>0.077820</td><td>0.078460</td><td>0.179809</td></tr><tr><th>svi</th><td>0.538845</td><td>0.155385</td><td>0.117658</td><td>-0.085843</td><td>1.000000</td><td>0.673111</td><td>0.320412</td><td>0.457648</td><td>0.566218</td></tr><tr><th>lcp</th><td>0.675310</td><td>0.164537</td><td>0.127668</td><td>-0.006999</td><td>0.673111</td><td>1.000000</td><td>0.514830</td><td>0.631528</td><td>0.548813</td></tr><tr><th>gleason</th><td>0.432417</td><td>0.056882</td><td>0.268892</td><td>0.077820</td><td>0.320412</td><td>0.514830</td><td>1.000000</td><td>0.751905</td><td>0.368987</td></tr><tr><th>pgg45</th><td>0.433652</td><td>0.107354</td><td>0.276112</td><td>0.078460</td><td>0.457648</td><td>0.631528</td><td>0.751905</td><td>1.000000</td><td>0.422316</td></tr><tr><th>lpsa</th><td>0.734460</td><td>0.433319</td><td>0.169593</td><td>0.179809</td><td>0.566218</td><td>0.548813</td><td>0.368987</td><td>0.422316</td><td>1.000000</td></tr></tbody></table></div><p><strong>Scatterplot matrix:</strong>(showing every pairwise plot between the variables)We see that <em>svi</em> is a binary variable, and <em>gleason</em> is an ordered categorical variable.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pd.plotting.scatter_matrix(data,figsize=(<span class="number">12</span>,<span class="number">12</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure><img src="ESL-Example-Prostate%20Cancer_5_0.png" alt="png"><figcaption>png</figcaption></figure><h2 id="simple-univariate-regression">Simple univariate regression</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">y=data[<span class="string">'lpsa'</span>]</span><br><span class="line">mask_train=data[<span class="string">'train'</span>]</span><br><span class="line">X=data.drop([<span class="string">'lpsa'</span>,<span class="string">'train'</span>],axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_train_data</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="comment">#normalize X</span></span><br><span class="line">    X=X.apply(scipy.stats.zscore)</span><br><span class="line">    X_train=X[mask_train==<span class="string">'T'</span>]</span><br><span class="line">    y_train=y[mask_train==<span class="string">'T'</span>]</span><br><span class="line">    X_test=X[mask_train!=<span class="string">'T'</span>]</span><br><span class="line">    y_test=y[mask_train!=<span class="string">'T'</span>]</span><br><span class="line">    X_test=np.hstack((np.ones((len(X_test),<span class="number">1</span>)),X_test))</span><br><span class="line">    X_train=np.hstack((np.ones((len(X_train),<span class="number">1</span>)),X_train))</span><br><span class="line">    <span class="keyword">return</span> X_train, y_train,X_test,y_test</span><br><span class="line"></span><br><span class="line">X_train, y_train,X_test,y_test=get_train_data(X)</span><br><span class="line">X_train.shape</span><br></pre></td></tr></table></figure><p>(67, 9)</p><p><span class="math display">\[\begin{align}\hat{\beta}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^Ty \end{align}\]</span> Fitted values at the training inputs: <span class="math inline">\(\hat{y}=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^Ty\)</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ols</span><span class="params">(X_,y_)</span>:</span></span><br><span class="line">    w=np.linalg.inv(X_.T.dot(X_)).dot(X_.T).dot(y_)</span><br><span class="line">    y_hat=X_.dot(w)</span><br><span class="line">    <span class="keyword">return</span> w,y_hat</span><br><span class="line">w,y_hat=ols(X_train,y_train)</span><br><span class="line">y_hat.shape</span><br></pre></td></tr></table></figure><p>(67,)</p><p>Unbiased estimate of <span class="math inline">\(\sigma^2\)</span>: <span class="math display">\[\begin{align} \hat{\sigma^2}=\frac{1}{N-p-1}\sum^{N}_{i=1}(y_i-\hat{y_i})^2 \end{align}\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sigma2_hat=sum((y_train-y_hat)**<span class="number">2</span>)/(len(y_train)<span class="number">-1</span>-len(X.columns))</span><br><span class="line">sigma2_hat</span><br></pre></td></tr></table></figure><p>0.5073514562053173</p><p>The variance–covariance matrix of the least squares parameter estimates: <span class="math display">\[\begin{align} Var(\hat{\beta})=(\mathbf{X}^T\mathbf{X})^{-1}\sigma^2) \end{align}\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">std_w=np.sqrt(np.diag(np.linalg.inv(X_train.T.dot(X_train)))*sigma2_hat)</span><br><span class="line">std_w</span><br></pre></td></tr></table></figure><p>array([ 0.08931498, 0.12597461, 0.095134 , 0.10081871, 0.10169077, 0.1229615 , 0.15373073, 0.14449659, 0.1528197 ])</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">table=pd.DataFrame(columns=[<span class="string">'Term'</span>,<span class="string">'Coeﬃcient'</span>,<span class="string">'Std. Error'</span>,<span class="string">'Z Score'</span>])</span><br><span class="line">table[<span class="string">'Term'</span>]=[<span class="string">'Intercept'</span>]+list(X.columns)</span><br><span class="line">table[<span class="string">'Coeﬃcient'</span>]=w</span><br><span class="line">table[<span class="string">'Std. Error'</span>]=std_w</span><br><span class="line">table[<span class="string">'Z Score'</span>]=w/std_w</span><br><span class="line">table</span><br></pre></td></tr></table></figure><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }        .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe"><thead><tr style="text-align: right;"><th></th><th>Term</th><th>Coeﬃcient</th><th>Std. Error</th><th>Z Score</th></tr></thead><tbody><tr><th>0</th><td>Intercept</td><td>2.464933</td><td>0.089315</td><td>27.598203</td></tr><tr><th>1</th><td>lcavol</td><td>0.676016</td><td>0.125975</td><td>5.366290</td></tr><tr><th>2</th><td>lweight</td><td>0.261694</td><td>0.095134</td><td>2.750789</td></tr><tr><th>3</th><td>age</td><td>-0.140734</td><td>0.100819</td><td>-1.395909</td></tr><tr><th>4</th><td>lbph</td><td>0.209061</td><td>0.101691</td><td>2.055846</td></tr><tr><th>5</th><td>svi</td><td>0.303623</td><td>0.122962</td><td>2.469255</td></tr><tr><th>6</th><td>lcp</td><td>-0.287002</td><td>0.153731</td><td>-1.866913</td></tr><tr><th>7</th><td>gleason</td><td>-0.021195</td><td>0.144497</td><td>-0.146681</td></tr><tr><th>8</th><td>pgg45</td><td>0.265576</td><td>0.152820</td><td>1.737840</td></tr></tbody></table></div><p>We can also test for the exclusion of a number of terms at once, using the F-statistic: <span class="math display">\[\begin{align} F=\frac{(RSS_2-RSS_1)/(p_1-p_2)}{RSS_1/(N-p_1)} \end{align}\]</span> <span class="math display">\[\begin{align}RSS(\beta)=\sum_{i=1}^{N}(y_{i}-f(x_{i}))^2=\sum_{i=1}^{N}(y_{i}-\beta_{0}-\sum_{j=1}^{p}X_{ij}\beta_{j})^2\end{align}\]</span></p><p>we consider dropping all the non-signiﬁcant terms, namely <em>age, lcp, gleason, and pgg45</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">X2=X.drop([<span class="string">'age'</span>, <span class="string">'lcp'</span>, <span class="string">'gleason'</span>, <span class="string">'pgg45'</span>],axis=<span class="number">1</span>)</span><br><span class="line">X_train2, y_train2,X_test2,y_test2=get_train_data(X2)</span><br><span class="line">w2,y_hat2=ols(X_train2,y_train2)</span><br><span class="line">RSS1=sum((y_train-y_hat)**<span class="number">2</span>)</span><br><span class="line">RSS2=sum((y_train2-y_hat2)**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">p2=X_train2.shape[<span class="number">1</span>]</span><br><span class="line">N,p1=X_train.shape</span><br><span class="line">F=round(((RSS2-RSS1)/(p1-p2))/((RSS1)/(N-p1)),<span class="number">2</span>)</span><br><span class="line">p_value=round(<span class="number">1</span>-scipy.stats.f.cdf(F,p1-p2,N-p1),<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"F:&#123;&#125;"</span>.format(F))</span><br><span class="line">print(<span class="string">"Pr(F(&#123;a&#125;,&#123;b&#125;)&gt;&#123;c&#125;)=&#123;d&#125;"</span>.format(a=p1-p2,b=N-p1,c=F,d=p_value))</span><br></pre></td></tr></table></figure><p>F:1.67 Pr(F(4,58)&gt;1.67)=0.17</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RSS1,RSS2</span><br></pre></td></tr></table></figure><p>(29.4263844599084, 32.81499474881556)</p><p>The mean prediction error on the test data is 0.521. In contrast, prediction using the mean training value of lpsa has a test error of 1.057, which is called the “base error rate.” Hence the linear model reduces the base error rate by about 50%. We will return to this example later to compare various selection and shrinkage methods.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y_test_fit=X_test.dot(w)</span><br><span class="line">base_error=round(sum((y_test-y_train.mean())**<span class="number">2</span>)/len(y_test),<span class="number">3</span>)</span><br><span class="line">prediction_error=round(sum((y_test-y_test_fit)**<span class="number">2</span>)/len(y_test),<span class="number">3</span>)</span><br><span class="line">print(<span class="string">'base_error:'</span>,base_error)</span><br><span class="line">print(<span class="string">'prediction_error:'</span>,prediction_error)</span><br></pre></td></tr></table></figure><p>base_error: 1.057 prediction_error: 0.521</p><h2 id="best-subset-selection">Best-Subset Selection</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> itertools</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nCr</span><span class="params">(n,r)</span>:</span></span><br><span class="line">    <span class="string">"""calculate how many combination of (n,r)"""</span></span><br><span class="line">    f = math.factorial</span><br><span class="line">    <span class="keyword">return</span> int(f(n) / f(r) / f(n-r))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">best_subset_ols</span><span class="params">(i)</span>:</span></span><br><span class="line">    <span class="string">"""calculate all ols estimate of i size subset"""</span></span><br><span class="line">    combine=itertools.combinations(range(<span class="number">1</span>,p+<span class="number">1</span>),i)</span><br><span class="line">    y_subset_train=y_train</span><br><span class="line">    result=[]</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> combine:</span><br><span class="line">        X_subset_train=X_train[:,[<span class="number">0</span>]+list(k)]</span><br><span class="line">        w,y_hat=ols(X_subset_train,y_subset_train)</span><br><span class="line">        result.append((w,sum((y_subset_train-y_hat)**<span class="number">2</span>)))</span><br><span class="line">    <span class="keyword">return</span> np.array(result)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">p=len(X.columns)</span><br><span class="line">best=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, p+<span class="number">1</span>):</span><br><span class="line">    ncr=nCr(p,i)</span><br><span class="line">    result=best_subset_ols(i)</span><br><span class="line">    best.append(min(result[:,<span class="number">1</span>]))</span><br><span class="line">    plt.plot(np.ones(ncr)*i,result[:,<span class="number">1</span>],<span class="string">'o'</span>,c=<span class="string">'grey'</span>)</span><br><span class="line">plt.plot(range(<span class="number">0</span>, p+<span class="number">1</span>),best,<span class="string">'o--'</span>,c=<span class="string">'red'</span>)  </span><br><span class="line">plt.xlabel(<span class="string">'Subset Size k'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'RSS'</span>)</span><br><span class="line">plt.savefig(<span class="string">'./images/best_subset.png'</span>)</span><br></pre></td></tr></table></figure><figure><img src="ESL-Example-Prostate%20Cancer_22_0.png" alt="png"><figcaption>png</figcaption></figure><h2 id="the-ridge-regression">The ridge regression</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">U,D,V=np.linalg.svd(X_train, full_matrices=<span class="literal">False</span>)</span><br><span class="line">p=len(D)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">df</span><span class="params">(D,lam)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> sum(D**<span class="number">2</span>/(D**<span class="number">2</span>+lam))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ddf</span><span class="params">(D,lam)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> -sum(D**<span class="number">2</span>/(D**<span class="number">2</span>+lam)**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">newton</span><span class="params">(p,D)</span>:</span></span><br><span class="line">    <span class="string">"""Calculate lambdas through eﬀective degrees of freedom """</span></span><br><span class="line">    edfs=np.linspace(<span class="number">0.5</span>, p<span class="number">-0.5</span>, (p<span class="number">-1</span>)*<span class="number">10</span>+<span class="number">1</span>)</span><br><span class="line">    threshold=<span class="number">1e-3</span></span><br><span class="line">    lambdas=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> edfs:</span><br><span class="line">        lam0=(p-i)/i</span><br><span class="line">        lam1=<span class="number">1e6</span></span><br><span class="line">        diff=lam1-lam0</span><br><span class="line">        <span class="keyword">while</span> diff&gt;threshold:</span><br><span class="line">            lam1=lam0-(df(D,lam0)-i)/ddf(D,lam0)</span><br><span class="line">            diff=lam1-lam0</span><br><span class="line">            lam0=lam1</span><br><span class="line">            </span><br><span class="line">        lambdas.append(lam1)</span><br><span class="line">    lambdas.append(<span class="number">0</span>)</span><br><span class="line">    edfs=np.concatenate(([<span class="number">0</span>],edfs,[p]))</span><br><span class="line">    <span class="keyword">return</span> edfs,np.array(lambdas)</span><br><span class="line"></span><br><span class="line">edfs,lambdas=newton(p,D)</span><br><span class="line">beta_ridge=[np.zeros(p)]</span><br><span class="line"><span class="keyword">for</span> lam <span class="keyword">in</span> lambdas:</span><br><span class="line">    beta=V.T.dot(np.diag(D/(D**<span class="number">2</span>+lam))).dot(U.T).dot(y_train)</span><br><span class="line">    beta_ridge.append(beta)</span><br><span class="line">beta_ridge=np.array(beta_ridge)</span><br><span class="line">plt.plot(edfs,beta_ridge, <span class="string">'o-'</span>, markersize=<span class="number">2</span>)</span><br><span class="line">plt.xlabel(<span class="string">'df(lambda)'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'beta_ridge'</span>)</span><br><span class="line">plt.legend(X.columns)</span><br><span class="line">plt.title(<span class="string">'Proﬁles of ridge coeﬃcients'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure><img src="ESL-Example-Prostate%20Cancer_25_0.png" alt="png"><figcaption>png</figcaption></figure><hr><p><strong>Ref:</strong></p><p>James, Gareth, et al. <em>An introduction to statistical learning</em>. Vol. 112. New York: springer, 2013.</p><p>Hastie, Trevor, et al. &quot;The elements of statistical learning: data mining, inference and prediction.&quot; <em>The Mathematical Intelligencer</em> 27.2 (2005): 83-85</p>]]></content>
    
    <summary type="html">
    
      &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; scipy&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; scipy.stats&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; numpy &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; np&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; pandas &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; pd&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; plt&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;%matplotlib inline&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;data=pd.read_csv(&lt;span class=&quot;string&quot;&gt;&#39;./data/prostate.data&#39;&lt;/span&gt;,delimiter=&lt;span class=&quot;string&quot;&gt;&#39;\t&#39;&lt;/span&gt;,index_col=&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;data.head()&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Study Note: Assessing Model Accuracy</title>
    <link href="https://nancyyanyu.github.io/posts/86b37baa/"/>
    <id>https://nancyyanyu.github.io/posts/86b37baa/</id>
    <published>2019-10-19T23:03:53.783Z</published>
    <updated>2019-10-19T23:20:39.552Z</updated>
    
    <content type="html"><![CDATA[<p><strong><em>no free lunch in statistics</em></strong>: no one method dominates all others over all possible data sets.</p><ul><li>Explanation: On a particular data set, one specific method may work best, but some other method may work better on a similar but different data set. Hence it is an important task to decide for any given set of data which method produces the best results.</li></ul><h1 id="measuring-the-quality-of-fit">Measuring the Quality of Fit</h1><h2 id="mean-squared-error-mse">mean squared error (MSE)</h2><p><span class="math display">\[\begin{align}MSE=\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{f}(x_i))^2\end{align}\]</span></p><p><strong><em>overfitting</em></strong>: When a given method yields a small training MSE but a large test MSE.</p><ul><li>Explanation: a less flexible model would have yielded a smaller test MSE. This happens because our statistical learning procedure is working too hard to find patterns in the training data, and may be picking up some patterns that are just caused by random chance rather than by true properties of the unknown function f.</li></ul><a id="more"></a><p><img src="./1.png" width="500"></p><h1 id="the-bias-variance-trade-off">The Bias-Variance Trade-Off</h1><h2 id="decomposition">Decomposition</h2><p>The expected test MSE, for a given value <span class="math inline">\(x_0\)</span> can always be decomposed into the sum of three fundamental quantities: <strong>the variance of <span class="math inline">\(\hat{f}(x_0)\)</span>, the squared bias of <span class="math inline">\(\hat{f}(x_0)\)</span>, and the variance of the error variance terms <span class="math inline">\(\epsilon\)</span>.</strong> <span class="math display">\[\begin{align}E(y_0-\hat{f}(x_0))^2=Var(\hat{f}(x_0))+[Bias(\hat{f}(x_0))]^2+Var(\epsilon)\end{align}\]</span> The overall expected test MSE can be computed by averaging <span class="math inline">\(E(y_0-\hat{f}(x_0))^2\)</span> over all possible values of x0 in the test set.</p><h3 id="variance">Variance</h3><p><strong><em>Variance</em></strong>: refers to the amount by which <span class="math inline">\(\hat{f}\)</span> would change if we estimated it using a different training data set. <strong><em>more flexible statistical methods have higher variance</em></strong> - Explanation: different training data sets will result in a different <span class="math inline">\(\hat{f}\)</span>. But ideally the estimate for f should not vary too much between training sets. However, if a method has high variance then small changes in the training data can result in large changes in <span class="math inline">\(\hat{f}\)</span></p><h3 id="bias">Bias</h3><p><strong><em>bias</em></strong>: refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model. - Explanation: As we increase the flexibility of a class of methods, the bias tends to initially decrease faster than the variance increases. Consequently, the expected test MSE declines. However, at some point increasing flexibility has little impact on the bias but starts to significantly increase the variance. When this happens the test MSE increases.</p><p><img src="./2.png" width="600"> <img src="./4.png" width="400"></p><hr><h3 id="math-explanation">Math Explanation</h3><p><strong>Math Explanation</strong>: If we assume that <span class="math inline">\(Y=f(X)+\epsilon\)</span> where <span class="math inline">\(E(\epsilon)=0\)</span>, and <span class="math inline">\(Var(\epsilon)=\sigma^2_\epsilon\)</span>, we can derive an expression for the expected prediction error of a regression fit <span class="math inline">\(\hat{f}(X)\)</span> at an input point X = x0, using squared-error loss: <span class="math display">\[\begin{align}Err(x_0)&amp;=E[(Y-\hat{f}(x_0))^2|X=x_0] \\&amp;=E[(f(x_0)+\epsilon-\hat{f}(x_0))^2] \\&amp;=E[\epsilon^2+(f(x_0)-\hat{f}(x_0))^2+2\epsilon(f(x_0)-\hat{f}(x_0))] \\&amp;=\sigma^2_\epsilon+E[f(x_0)^2+\hat{f}(x_0)^2-2f(x_0)\hat{f}(x_0)] \\&amp;=\sigma^2_\epsilon+E[\hat{f}(x_0)^2]+f(x_0)^2-2f(x_0)E[\hat{f}(x_0)]  \\&amp;=\sigma^2_\epsilon+(E[\hat{f}(x_0)])^2+f(x_0)^2-2f(x_0)E[\hat{f}(x_0)] +E[\hat{f}(x_0)^2]-(E[\hat{f}(x_0))^2 \\&amp;=\sigma^2_\epsilon+(E\hat{f}(x_0)-f(x_0))^2+Var(\hat{f}(x_0))\\&amp;=\sigma^2_\epsilon+Bias^2(\hat{f}(x_0))+Var(\hat{f}(x_0))\\&amp;= Irreducible Error+ Bias^2 + Variance\end{align}\]</span></p><ol type="1"><li>The first term is the variance of the target around its true mean f(x0), and cannot be avoided no matter how well we estimate f(x0), unless <span class="math inline">\(\sigma^2_\epsilon=0\)</span></li><li>The second term is the squared bias, the amount by which the average of our estimate differs from the true mean</li><li>The last term is the variance; the expected squared deviation of <span class="math inline">\(\hat{f}(x_0)\)</span> around its mean.</li></ol><blockquote><p>Typically the more complex we make the model <span class="math inline">\(\hat{f}\)</span>, the lower the (squared) bias but the higher the variance.</p></blockquote><h1 id="the-classification-setting">The Classification Setting</h1><p><strong><em>training error rate</em></strong>： <span class="math inline">\(\frac{1}{n}\sum_{i=1}^nI(y_i\neq\hat{y}_i)\)</span></p><p>Here <span class="math inline">\(\hat{y}_i\)</span> is the predicted class label for the ith observation using <span class="math inline">\(\hat{f}\)</span></p><p><strong><em>test error rate</em></strong>： <span class="math inline">\(Ave (I(y_0 \neq \hat{y}_0))\)</span></p><p>where <span class="math inline">\(\hat{y}_0\)</span> is the predicted class label that results from applying the classifier to the test observation with predictor x0. A good classifier is one for which the test error is smallest.</p><h2 id="the-bayes-classifier">The Bayes Classifier</h2><p><strong>Bayes classifier</strong>: <span class="math inline">\(Pr(Y=j|X=x_0)\)</span> - <strong>Explanation</strong>: The test error rate given in <span class="math inline">\(Ave (I(y_0 \neq \hat{y}_0))\)</span> is minimized, on average, by a very simple classifier that assigns each observation to the most likely class, given its predictor values. - <strong>Example</strong>: In a two-class problem where there are only two possible response values, say class 1 or class 2, the Bayes classifier corresponds to predicting class one if <span class="math inline">\(Pr(Y=1|X=x_0)\)</span>&gt; 0.5, and class two otherwise. - <strong>Disadvantage</strong>: For real data, we do not know the conditional distribution of Y given X, and so computing the Bayes classifier is impossible. Therefore, the Bayes classifier serves as an unattainable gold standard against which to compare other methods</p><p><strong>Bayes error rate</strong>: <span class="math inline">\(1-E\left(\max_jPr(Y=j|X)\right)\)</span></p><p>Since the Bayes classifier will always choose the class Bayes error <span class="math inline">\(Pr(Y=j|X=x_0)\)</span> is largest, the error rate at X = x0 will be <span class="math inline">\(1-\max_jPr(Y=j|X)\)</span></p><h2 id="k-nearest-neighbors">K-Nearest Neighbors</h2><p><strong>K-nearest neighbors (KNN) classifier</strong>: 1. Given a positive integer <span class="math inline">\(K\)</span> and a test observation <span class="math inline">\(x_0\)</span>, the KNN classifier first identifies the neighbors <span class="math inline">\(K\)</span> points in the training data that are closest to <span class="math inline">\(x_0\)</span>, represented by <span class="math inline">\(N_0\)</span>.</p><ol start="2" type="1"><li><p>It then estimates the conditional probability for class <span class="math inline">\(j\)</span> as the fraction of points in <span class="math inline">\(N_0\)</span> whose response values equal <span class="math inline">\(j\)</span>: <span class="math display">\[  \begin{align}  Pr(Y=j|X=x_0)=\frac{1}{K}\sum_{i\in N_0} I(y_i=j)  \end{align}  \]</span></p></li><li><p>Finally, KNN applies Bayes rule and classifies the test observation <span class="math inline">\(x_0\)</span> to the class with the largest probability.</p></li></ol><p><img src="./3.png" width="600"></p><p>As K grows, the method becomes less flexible and produces a decision boundary that is close to linear. This corresponds to a low-variance but high-bias classifier.</p><p>As we use more flexible classification methods, the training error rate will decline but the test error rate may not.</p><hr><p><strong>Ref:</strong></p><p>James, Gareth, et al. <em>An introduction to statistical learning</em>. Vol. 112. New York: springer, 2013.</p><p>Hastie, Trevor, et al. &quot;The elements of statistical learning: data mining, inference and prediction.&quot; <em>The Mathematical Intelligencer</em> 27.2 (2005): 83-85</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;&lt;em&gt;no free lunch in statistics&lt;/em&gt;&lt;/strong&gt;: no one method dominates all others over all possible data sets.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explanation: On a particular data set, one specific method may work best, but some other method may work better on a similar but different data set. Hence it is an important task to decide for any given set of data which method produces the best results.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;measuring-the-quality-of-fit&quot;&gt;Measuring the Quality of Fit&lt;/h1&gt;
&lt;h2 id=&quot;mean-squared-error-mse&quot;&gt;mean squared error (MSE)&lt;/h2&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
MSE=\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{f}(x_i))^2
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;overfitting&lt;/em&gt;&lt;/strong&gt;: When a given method yields a small training MSE but a large test MSE.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explanation: a less flexible model would have yielded a smaller test MSE. This happens because our statistical learning procedure is working too hard to find patterns in the training data, and may be picking up some patterns that are just caused by random chance rather than by true properties of the unknown function f.&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Model Assessment" scheme="https://nancyyanyu.github.io/tags/Model-Assessment/"/>
    
  </entry>
  
  <entry>
    <title>Study Note: Bias, Variance and Model Complexity</title>
    <link href="https://nancyyanyu.github.io/posts/2a71b2a0/"/>
    <id>https://nancyyanyu.github.io/posts/2a71b2a0/</id>
    <published>2019-10-19T23:03:44.786Z</published>
    <updated>2019-10-19T23:20:36.068Z</updated>
    
    <content type="html"><![CDATA[<h1 id="bias-variance-and-model-complexity">Bias, Variance and Model Complexity</h1><p><strong>Test error</strong> (generalization error): the prediction error over an independent test sample <span class="math display">\[𝐸𝑟𝑟𝜏=𝐸[𝐿(𝑌,\hat{f} (𝑋))|𝜏]\]</span> Here the training set <span class="math inline">\(\tau\)</span> is fixed, and test error refers to the error for this specific training set.</p><a id="more"></a><p><strong>Expected test error: </strong> <span class="math display">\[Err=E[L(Y,\hat{f}(X)]=E[Err_\tau]\]</span></p><p>This expectation averages over everything that is random, including the randomness in the training set that produced <span class="math inline">\(\hat{f}\)</span></p><p><strong>Training error</strong>: the average loss over the training sample <span class="math display">\[\bar{err}=\frac{1}{N}\sum_{i=1}^NL(y_i,\hat{f}(x_i))\]</span></p><p><img src="./bv.PNG" width="370"></p><p><strong>Model selection:</strong> estimating the performance of different models in order to choose the best one.</p><p><strong>Model assessment:</strong> having chosen a final model, estimating its prediction error (generalization error) on new data.</p><p>Randomly divide the dataset into three parts: - a <strong>training set</strong>: fit the models - a <strong>validation set</strong>: estimate prediction error for model selection - a <strong>test set</strong>: assessment of the generalization error of the nal chosen model</p><p>A typical split might be 50% for training, and 25% each for validation and testing:</p><h1 id="the-bias-variance-decomposition">The Bias Variance Decomposition</h1><h2 id="general-model">General Model</h2><p>If we assume that <span class="math inline">\(Y=f(X)+\epsilon\)</span> where <span class="math inline">\(E(\epsilon)=0\)</span>, and <span class="math inline">\(Var(\epsilon)=\sigma^2_\epsilon\)</span>, we can derive an expression for the expected prediction error of a regression fit <span class="math inline">\(\hat{f}(X)\)</span> at an input point X = x0, using squared-error loss:</p><p><span class="math display">\[\begin{align}Err(x_0)&amp;=E[(Y-\hat{f}(x_0))^2|X=x_0] \\&amp;=E[(f(x_0)+\epsilon-\hat{f}(x_0))^2] \\&amp;=E[\epsilon^2+(f(x_0)-\hat{f}(x_0))^2+2\epsilon(f(x_0)-\hat{f}(x_0))] \\&amp;=\sigma^2_\epsilon+E[f(x_0)^2+\hat{f}(x_0)^2-2f(x_0)\hat{f}(x_0)] \\&amp;=\sigma^2_\epsilon+E[\hat{f}(x_0)^2]+f(x_0)^2-2f(x_0)E[\hat{f}(x_0)]  \\&amp;=\sigma^2_\epsilon+(E[\hat{f}(x_0)])^2+f(x_0)^2-2f(x_0)E[\hat{f}(x_0)] +E[\hat{f}(x_0)^2]-(E[\hat{f}(x_0))^2 \\&amp;=\sigma^2_\epsilon+(E\hat{f}(x_0)-f(x_0))^2+Var(\hat{f}(x_0))\\&amp;=\sigma^2_\epsilon+Bias^2(\hat{f}(x_0))+Var(\hat{f}(x_0))\\&amp;= Irreducible Error+ Bias^2 + Variance\end{align}\]</span></p><ol type="1"><li>The first term is the variance of the target around its true mean f(x0), and cannot be avoided no matter how well we estimate f(x0), unless <span class="math inline">\(\sigma^2_\epsilon=0\)</span></li><li>The second term is the squared bias, the amount by which the average of our estimate differs from the true mean</li><li>The last term is the variance; the expected squared deviation of <span class="math inline">\(\hat{f}(x_0)\)</span> around its mean.</li></ol><blockquote><p>Typically the more complex we make the model <span class="math inline">\(\hat{f}\)</span>, the lower the (squared) bias but the higher the variance.</p></blockquote><h2 id="knn-regression">KNN regression</h2><p>For the k-nearest-neighbor regression t, these expressions have the sim- ple form <span class="math display">\[\begin{align}Err(x_0)&amp;=E[(Y-\hat{f}_k(x_0))^2|X=x_0] \\\end{align}\]</span> <img src="./bv2.PNG" width="470"></p><hr><p><strong>Ref:</strong></p><p>James, Gareth, et al. <em>An introduction to statistical learning</em>. Vol. 112. New York: springer, 2013.</p><p>Hastie, Trevor, et al. &quot;The elements of statistical learning: data mining, inference and prediction.&quot; <em>The Mathematical Intelligencer</em> 27.2 (2005): 83-85</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;bias-variance-and-model-complexity&quot;&gt;Bias, Variance and Model Complexity&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Test error&lt;/strong&gt; (generalization error): the prediction error over an independent test sample &lt;span class=&quot;math display&quot;&gt;\[
𝐸𝑟𝑟𝜏=𝐸[𝐿(𝑌,\hat{f} (𝑋))|𝜏]
\]&lt;/span&gt; Here the training set &lt;span class=&quot;math inline&quot;&gt;\(\tau\)&lt;/span&gt; is fixed, and test error refers to the error for this specific training set.&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Model Assessment" scheme="https://nancyyanyu.github.io/tags/Model-Assessment/"/>
    
      <category term="Model Selection" scheme="https://nancyyanyu.github.io/tags/Model-Selection/"/>
    
  </entry>
  
  <entry>
    <title>Study Note: Dimension Reduction - PCA, PCR</title>
    <link href="https://nancyyanyu.github.io/posts/cac93a23/"/>
    <id>https://nancyyanyu.github.io/posts/cac93a23/</id>
    <published>2019-10-19T23:03:29.340Z</published>
    <updated>2019-10-19T23:20:56.964Z</updated>
    
    <content type="html"><![CDATA[<h1 id="dimension-reduction-methods">Dimension Reduction Methods</h1><p>Subset selection and shrinkage methods all use the original predictors, X1,X2, . . . , Xp.</p><p>Dimension Reduction Methods <strong><em>transform</em></strong> the predictors and then fit a least squares model using the transformed variables.</p><h2 id="approach">Approach</h2><p>Let <span class="math inline">\(Z_1,Z_2, . . . ,Z_M\)</span> represent <span class="math inline">\(M &lt; p\)</span> linear combinations of our original <span class="math inline">\(p\)</span> predictors. That is,</p><p><span class="math display">\[\begin{align}Z_m=\sum_{j=1}^p\phi_{jm}X_j\end{align}\]</span> <a id="more"></a></p><p>for some constants <span class="math inline">\(φ_{1m}, φ_{2m} . . . , φ_{pm}, m = 1, . . .,M.\)</span> We can then fit the linear regression model <span class="math display">\[\begin{align}y_i=\theta_0+\sum_{m=1}^M\theta_m z_{im}+\epsilon_i  \quad  i=1,2,3,4,...,n\end{align}\]</span> <strong>Dimension reduction</strong>: reduces the problem of estimating the <span class="math inline">\(p+1\)</span> coefficients <span class="math inline">\(β_0, β_1, . . . , β_p\)</span> to the simpler problem of estimating the <span class="math inline">\(M + 1\)</span> coefficients <span class="math inline">\(θ_0, θ_1, . . . , θ_M\)</span>, where M &lt; p. In other words, the dimension of the problem has been reduced from <span class="math inline">\(p + 1\)</span> to <span class="math inline">\(M + 1\)</span>. <span class="math display">\[\begin{align}\sum_{m=1}^M\theta_m z_{im}&amp;=\sum_{m=1}^M\theta_m \sum_{j=1}^p\phi_{jm}x_{ij}=\sum_{m=1}^M\sum_{j=1}^p\theta_m \phi_{jm}x_{ij}=\sum_{j=1}^p \beta_jx_{ij}  \\\beta_j&amp;=\sum_{m=1}^M\theta_m \phi_{jm}\end{align}\]</span> <strong>All dimension reduction methods work in two steps:</strong></p><ol type="1"><li>The transformed predictors <span class="math inline">\(Z_1,Z_2, . . . ,Z_M\)</span>are obtained.</li><li>The model is fit using these <span class="math inline">\(M\)</span> predictors. However, the choice of <span class="math inline">\(Z_1,Z_2, . . . ,Z_M\)</span>, or equivalently, the selection of the <span class="math inline">\(φ_{jm}\)</span>’s, can be achieved in different ways.</li></ol><h1 id="principal-components-regression">Principal Components Regression</h1><h2 id="an-overview-of-principal-components-analysis">An Overview of Principal Components Analysis</h2><p><strong>Principal component analysis (PCA)</strong> refers to the process by which principal components are computed, and the subsequent use of these components in understanding the data.</p><ul><li>PCA also serves as a tool for data visualization (visualization of the observations or visualization of the variables).</li></ul><h2 id="what-are-principal-components">What Are Principal Components?</h2><p><strong>PCA</strong> :finds a low-dimensional representation of a data set that contains as much as possible of the <strong>variation</strong></p><p>Each of the dimensions found by PCA is a linear combination of the <span class="math inline">\(p\)</span> features.</p><p><strong><em>The first principal component</em></strong> of a set of features <span class="math inline">\(X_1,X_2, . . . , X_p\)</span> is the normalized linear combination of the features <span class="math display">\[\begin{align}Z_1=\phi_{11}X_1+\phi_{21}X_2+,,,+\phi_{p1}X_p\end{align}\]</span> that has the <strong>largest variance</strong>.</p><p><strong>Normalized</strong>: <span class="math inline">\(\sum_{j=1}^p \phi_{j1}^2=1\)</span></p><p><strong>Loadings</strong>: <span class="math inline">\(\phi_{11}, . . . , \phi_{p1}\)</span> the loadings of the first principal component;</p><ul><li>Together, the loadings make up the principal component loading vector, <span class="math inline">\(\phi_1=(\phi_{11},\phi_{21},...,\phi_{p1})^T\)</span></li></ul><h3 id="st-principal-component">1st Principal Component</h3><h4 id="interpretation-1-greatest-variability">Interpretation 1: greatest variability</h4><p><strong>The first principal component</strong> direction of the data: is that along which the observations <strong>vary the most</strong>.</p><p><img src="./7.png" width="600"></p><p>The first principal component direction is the direction along which there is the greatest variability in the data. That is, if we projected the 100 observations onto this line (as shown in the left-hand panel of Figure 6.15), then the resulting projected observations would have the largest possible variance</p><p><img src="./8.png" width="600"></p><p>The first principal component is given by the formula</p><p><span class="math display">\[\begin{align}Z_1 = 0.839 × (pop − \bar{pop}) + 0.544 × (ad − \bar{ad})\end{align}\]</span> Here <span class="math inline">\(φ_{11} = 0.839\)</span> and <span class="math inline">\(φ_{21} = 0.544\)</span> are the <strong>principal component loadings</strong>, which define the direction referred to above.</p><blockquote><p>The idea is that out of every possible linear combination of pop and ad such that <span class="math inline">\(\phi_{11}^2+\phi_{21}^2=1\)</span>, this particular linear combination yields the highest variance: i.e. this is the linear combination for which <span class="math inline">\(Var(φ_{11} × (pop − \bar{pop}) + φ_{21} × (ad − \bar{ad}))\)</span> is maximized.</p></blockquote><p><strong>Principal Component Scores</strong></p><p>The values of <span class="math inline">\(z_{11}, . . . , z_{n1}\)</span> are known as the <strong>principal component scores</strong>, and can be seen in the right-hand panel of Figure 6.15. For example, <span class="math display">\[\begin{align}z_{i1} = 0.839 × (pop_i − \bar{pop}) + 0.544 × (ad_i − \bar{ad})\end{align}\]</span></p><h4 id="interpretation-2-closest-to-data">Interpretation 2: closest to data</h4><p>There is also another interpretation for PCA: the first principal component vector defines the line that is as close as possible to the data.</p><p>In Figure 6.14, the first principal component line minimizes the sum of the squared perpendicular distances between each point and the line.</p><p>In the right-hand panel of Figure 6.15, the left-hand panel has been rotated so that the first principal component direction coincides with the x-axis. It is possible to show that the <strong><em>first principal component score</em></strong> for the ith observation is the distance in the <span class="math inline">\(x\)</span>-direction of the ith cross from zero.</p><h4 id="interpretation-3-single-number-summarization">Interpretation 3: single number summarization</h4><p>We can think of the values of the principal component <span class="math inline">\(Z_1\)</span> as single number summaries of the joint pop and ad budgets for each location.</p><p>In this example, if <span class="math inline">\(z_{i1} = 0.839 × (pop_i − pop) + 0.544 × (ad_i − ad) &lt; 0\)</span>, then this indicates a city with below-average population size and belowaverage ad spending.</p><p><img src="./9.png" width="650"></p><p>Figure 6.16 displays <span class="math inline">\(z_{i1}\)</span> versus both pop and ad. The plots show a strong relationship between the first principal component and the two features. In other words, the first principal component appears to <em>capture most of the information</em> contained in the pop and ad predictors.</p><h4 id="compute-the-first-principal-component">Compute the first principal component</h4><ul><li><p>Assume that each of the variables in <span class="math inline">\(X\)</span> has been centered to have mean zero. We then look for the linear combination of the sample feature values of the form <span class="math display">\[\begin{align}z_{i1}=\phi_{11}x_{i1}+\phi_{21}x_{i2}+,,,+\phi_{p1}x_{ip} \quad \quad i=1,2,...,n\end{align}\]</span> that has largest sample variance, subject to the constraint that <span class="math inline">\(\sum_{j=1}^p \phi_{j1}^2=1\)</span></p></li><li><p>The first principal component loading vector solves the optimization problem <span class="math display">\[\begin{align}\max_{\phi_{11},...,\phi_{p1}}{\left\{ \frac{1}{n} \sum_{i=1}^n \left( \sum_{j=1}^p \phi_{j1}x_{ij}   \right)^2 \right\}} \, subject \, to \, \sum_{j=1}^p \phi_{j1}^2=1\end{align}\]</span></p></li><li><p>Since <span class="math inline">\(\sum_{i=1}^nx_{ij}/n=1\)</span>, the average of the <span class="math inline">\(z_{11}, . . . , z_{n1}\)</span> will be zero as well. Hence the objective that we are maximizing is just the <strong>sample variance</strong> of the <span class="math inline">\(n\)</span> values of zi1</p></li><li><p><strong>Scores</strong>: We refer to <span class="math inline">\(z_{11}, . . . , z_{n1}\)</span> as the scores of the first principal component.</p></li></ul><p><strong>Geometric interpretation</strong>: for the first principal component: The loading vector <span class="math inline">\(\phi_1\)</span> with elements <span class="math inline">\(\phi_{11},\phi_{21},...,\phi_{p1}\)</span> defines a direction in feature space along which the data <strong>vary the most</strong>. If we project the n data points <span class="math inline">\(x_1, . . . , x_n\)</span> onto this direction, the projected values are the principal component scores <span class="math inline">\(z_{11}, . . . , z_{n1}\)</span> themselves.</p><h3 id="nd-principal-component">2nd Principal Component</h3><p>The s<strong>econd principal component <span class="math inline">\(Z_2\)</span></strong> is a linear combination of the variables that is uncorrelated with <span class="math inline">\(Z_1\)</span>, and has largest variance subject to this constraint.</p><p>It turns out that the zero correlation condition of <span class="math inline">\(Z_1\)</span> with <span class="math inline">\(Z_2\)</span> is equivalent to the condition that the direction must be perpendicular, or orthogonal, to the first principal component direction.</p><p>The second principal component is given by the formula:</p><p><span class="math display">\[\begin{align}Z_2 = 0.544 × (pop − \bar{pop}) − 0.839 × (ad − \bar{ad}).\end{align}\]</span> Figure 6.15. The fact that the second principal component scores are much closer to zero indicates that this component captures far less information.</p><h4 id="compute-the-second-principal-component">Compute the second principal component</h4><p><strong>The second principal component <span class="math inline">\(Z_2\)</span></strong>: the linear combination of <span class="math inline">\(X_1,X_2, . . . , X_p\)</span> that has maximal variance out of all linear combinations that are <strong>uncorrelated with <span class="math inline">\(Z_1\)</span></strong>.</p><p>The second principal component scores <span class="math inline">\(z_{12}, . . . , z_{n2}\)</span> take the form <span class="math display">\[\begin{align}z_{i2}=\phi_{12}x_{i1}+\phi_{22}x_{i2}+,,,+\phi_{p2}x_{ip} \quad \quad i=1,2,...,n\end{align}\]</span> where <span class="math inline">\(\phi_2\)</span> is the second principal component <strong>loading</strong> vector, with elements <span class="math inline">\(\phi_{12},\phi_{22},...,\phi_{p2}\)</span>.</p><p>It turns out that constraining <span class="math inline">\(Z_2\)</span> to be uncorrelated with <span class="math inline">\(Z_1\)</span> is equivalent to constraining the direction <span class="math inline">\(\phi_2\)</span> to be <strong>orthogonal</strong> (perpendicular) to the direction <span class="math inline">\(\phi_1\)</span>.</p><p>To find <span class="math inline">\(\phi_2\)</span>, we solve a problem similar to (10.3) with <span class="math inline">\(\phi_2\)</span> replacing <span class="math inline">\(\phi_1\)</span>, and with the additional constraint that <span class="math inline">\(\phi_2\)</span> is orthogonal to <span class="math inline">\(\phi_1\)</span></p><p><img src="./1_v2.png" width="600"></p><p><strong>Interpretation:</strong></p><ul><li>1st loading vector places approximately equal weight on Assault, Murder, and Rape, with much less weight UrbanPop. Hence this component roughly corresponds to a measure of overall rates of serious crimes.</li><li>Overall, we see that the crime-related variables (Murder, Assault, and Rape) are located close to each other, and that the UrbanPop variable is far from the other three.</li><li>This indicates that the crime-related variables are correlated with each other—states with high murder rates tend to have high assault and rape rates—and that the UrbanPop variable is less correlated with the other three.</li></ul><h2 id="another-interpretation-of-principal-components">Another Interpretation of Principal Components</h2><p><strong>An alternative interpretation for principal components</strong>: principal components provide low-dimensional linear surfaces that are closest to the observations</p><ul><li><p><strong>The first principal component loading vector has a very special property</strong>: it is the line in p-dimensional space that is closest to the n observations (using average squared Euclidean distance as a measure of closeness).</p></li><li><p>The appeal of this interpretation : we seek a single dimension of the data that lies as close as possible to all of the data points, since such a line will likely provide a good summary of the data.</p></li><li><p><strong>The first two principal components</strong> of a data set <strong>span the plane</strong> that is closest to the n observations, in terms of average squared Euclidean distance</p></li><li><p>Together <strong>the first M principal component</strong> score vectors and the first M principal component loading vectors provide the best M-dimensional approximation (in terms of Euclidean distance) to the ith observation <span class="math inline">\(x_{ij}\)</span> . <span class="math display">\[\begin{align}x_{ij} \approx \sum_{m=1}^Mz_{im}\phi_{jm}\end{align}\]</span> (assuming the original data matrix X is column-centered).</p></li><li><p>When <span class="math inline">\(M = min(n − 1, p)\)</span>, then the representation is exact: <span class="math inline">\(x_{ij} = \sum_{m=1}^Mz_{im}\phi_{jm}\)</span></p></li></ul><h2 id="more-on-pca">More on PCA</h2><h3 id="scaling-the-variables">Scaling the Variables</h3><p>Before PCA is performed, the variables should be <strong>centered to have mean zero</strong>. Furthermore, the results obtained when we perform PCA will also depend on whether the variables have been <strong>individually scaled</strong> (each multiplied by a different constant)</p><p><img src="./2_v2.png" width="600"></p><h3 id="uniqueness-of-the-principal-components">Uniqueness of the Principal Components</h3><p><strong>Each principal component loading vector <span class="math inline">\(\phi_1=(\phi_{11},\phi_{21},...,\phi_{p1})^T\)</span> and the score vectors <span class="math inline">\(z_{11}, . . . , z_{n1}\)</span> is unique, up to a sign flip. </strong></p><ul><li>Two different software packages will yield the same principal component loading vectors and score vectors, although the signs of those loading vectors may differ.</li><li><strong>The signs may differ</strong> because each principal component loading vector specifies a direction in p-dimensional space: flipping the sign has no effect as the direction does not change.</li></ul><h3 id="the-proportion-of-variance-explained">The Proportion of Variance Explained</h3><p><strong>How much of the variance in the data is not contained in the first few principal components?</strong></p><p><strong>Proportion of variance explained (PVE)</strong> by each principal component:</p><ul><li>The total variance present in a data set (assuming that the variables have been centered to have mean zero) is defined as</li></ul><p><span class="math display">\[\begin{align}\sum_{j=1}^pVar(X_j)=\sum_{j=1}^p\frac{1}{n}\sum_{i=1}^nx_{ij}^2\end{align}\]</span></p><ul><li>The variance explained by the mth principal component is</li></ul><p><span class="math display">\[\begin{align}\frac{1}{n}\sum_{i=1}^nz_{im}^2=\frac{1}{n}\sum_{i=1}^n \left( \sum_{j=1}^p \phi_{jm}x_{ij} \right)^2\end{align}\]</span></p><ul><li>Therefore, the <strong>PVE of the mth principal component</strong> is given by</li></ul><p><span class="math display">\[\begin{align}\frac{\sum_{i=1}^n \left( \sum_{j=1}^p \phi_{jm}x_{ij} \right)^2}{\sum_{j=1}^p\sum_{i=1}^nx_{ij}^2}\end{align}\]</span></p><p>The PVE of each principal component is a positive quantity. In order to compute the <strong>cumulative PVE</strong> of the first <span class="math inline">\(M\)</span> principal components, we can simply sum (10.8) over each of the first <span class="math inline">\(M\)</span> PVEs. In total, there are <span class="math inline">\(min(n − 1, p)\)</span> principal components, and their PVEs sum to one.</p><p><img src="./3_v2.png" width="600"></p><h3 id="deciding-how-many-principal-components-to-use">Deciding How Many Principal Components to Use</h3><p>We would like to use the smallest number of principal components required to get a good understanding of the data.</p><p><strong>How many principal components are needed?</strong></p><ul><li>We typically decide on the number of principal components required to visualize the data by examining a <strong>scree plot</strong> (Right FIGURE 10.4)</li><li>We choose the smallest number of principal components that are required in order to explain a sizable amount of the variation in the data.</li><li>We tend to look at the first few principal components in order to find interesting patterns in the data. If no interesting patterns are found in the first few principal components, then further principal components are unlikely to be of interest.</li></ul><h2 id="the-principal-components-regression-approach">The Principal Components Regression Approach</h2><p>The principal components regression (PCR) approach involves constructing the first M principal components, <span class="math inline">\(Z_1,Z_2, . . . ,Z_M\)</span>, and then using these components as the predictors in a linear regression model that is fit using least squares</p><p><strong>The key idea</strong></p><p>Often a small number of principal components suffice to explain most of the variability in the data, as well as the relationship with the response. In other words, we assume that <strong><em>the directions in which <span class="math inline">\(X_1, . . .,X_p\)</span> show the most variation are the directions that are associated with <span class="math inline">\(Y\)</span></em></strong></p><p><strong>Example</strong>:</p><p><img src="./10.png" width="650"></p><ul><li>Performing PCR with an appropriate choice of M can result in a substantial improvement over least squares</li><li>PCR does not perform as well as the two shrinkage methods<ul><li><strong>Reason</strong>: The data were generated in such a way that many principal components are required in order to adequately model the response. In contrast, PCR will tend to do well in cases when the first few principal components are sufficient to capture most of the variation in the predictors as well as the relationship with the response.</li></ul></li></ul><p><strong>Note</strong>: even though PCR provides a simple way to perform regression using <span class="math inline">\(M &lt; p\)</span> predictors, it is not a <em>feature selection</em> method!</p><ul><li>This is because each of the <span class="math inline">\(M\)</span> principal components used in the regression is a linear combination of all p of the original features.</li><li>PCR is more closely related to ridge regression than to the lasso. One can even think of ridge regression as a continuous version of PCR!</li></ul><p><strong>Cross-validation</strong>: In PCR, the number of principal components, <span class="math inline">\(M\)</span>, is typically chosen by cross-validation.</p><p><img src="./11.png" width="650"></p><p><strong>Standardisation</strong>: When performing PCR, we generally recommend standardizing each predictor, prior to generating the principal components. - In the absence of standardization, the <em>high-variance variables</em> will tend to play a larger role in the principal components obtained, and the scale on which the variables are measured will ultimately have an effect on the final PCR model.</p><hr><p><strong>Ref:</strong></p><p>James, Gareth, et al. <em>An introduction to statistical learning</em>. Vol. 112. New York: springer, 2013.</p><p>Hastie, Trevor, et al. &quot;The elements of statistical learning: data mining, inference and prediction.&quot; <em>The Mathematical Intelligencer</em> 27.2 (2005): 83-85</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;dimension-reduction-methods&quot;&gt;Dimension Reduction Methods&lt;/h1&gt;
&lt;p&gt;Subset selection and shrinkage methods all use the original predictors, X1,X2, . . . , Xp.&lt;/p&gt;
&lt;p&gt;Dimension Reduction Methods &lt;strong&gt;&lt;em&gt;transform&lt;/em&gt;&lt;/strong&gt; the predictors and then fit a least squares model using the transformed variables.&lt;/p&gt;
&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;
&lt;p&gt;Let &lt;span class=&quot;math inline&quot;&gt;\(Z_1,Z_2, . . . ,Z_M\)&lt;/span&gt; represent &lt;span class=&quot;math inline&quot;&gt;\(M &amp;lt; p\)&lt;/span&gt; linear combinations of our original &lt;span class=&quot;math inline&quot;&gt;\(p\)&lt;/span&gt; predictors. That is,&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
Z_m=\sum_{j=1}^p\phi_{jm}X_j
\end{align}
\]&lt;/span&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Model Selection" scheme="https://nancyyanyu.github.io/tags/Model-Selection/"/>
    
      <category term="PCA" scheme="https://nancyyanyu.github.io/tags/PCA/"/>
    
      <category term="Dimension Reduction" scheme="https://nancyyanyu.github.io/tags/Dimension-Reduction/"/>
    
  </entry>
  
  <entry>
    <title>Study Note: Decision Trees, Random Forest, and Boosting</title>
    <link href="https://nancyyanyu.github.io/posts/6b588a86/"/>
    <id>https://nancyyanyu.github.io/posts/6b588a86/</id>
    <published>2019-10-19T23:02:53.811Z</published>
    <updated>2019-10-19T23:20:48.887Z</updated>
    
    <content type="html"><![CDATA[<h1 id="introduction-to-descision-tree">Introduction to Descision Tree</h1><h2 id="regression-trees">Regression Trees</h2><h3 id="predicting-baseball-players-salaries-using-regression-trees">Predicting Baseball Players’ Salaries Using Regression Trees</h3><p><strong>Terminal nodes</strong>: The regions R1, R2, and R3 are known as terminal nodes or leaves of the tree.</p><p><strong>Internal nodes</strong>: The points along the tree where the predictor space is split are referred to as internal nodes.</p><p><strong>Branches</strong>: The segments of the trees that connect the nodes as branches</p><a id="more"></a><p><img src="./2.png" width="500"> <img src="./1.png" width="500"></p><h3 id="prediction-via-stratification-of-the-feature-space">Prediction via Stratification of the Feature Space</h3><p><strong>Process of building a regression tree</strong></p><p><strong>Step 1</strong>: We divide the predictor space—that is, the set of possible values for X1,X2, . . .,Xp—into J distinct and non-overlapping regions, R1,R2, . . . , RJ .</p><p><strong>Step 2</strong>: For every observation that falls into the region Rj, we make the same prediction, which is simply the <em>mean of the response values</em> for the training observations in Rj .</p><h4 id="step-1">Step 1</h4><p><strong>How do we construct the regions R1, . . .,RJ?</strong></p><ul><li><p>We choose to divide the predictor space into high-dimensional rectangles, or <strong>boxes</strong>, for ease of interpretation of the resulting predictive model.</p></li><li><p>The goal is to find boxes R1, . . . , RJ that <strong>minimize the RSS</strong>, given by <span class="math display">\[\begin{align}\sum_{j=1}^J\sum_{i \in R_j} (y_i-\hat{y}_{R_j})^2\end{align}\]</span></p><p>where <span class="math inline">\(\hat{y}_{R_j}\)</span> is the mean response for the training observations within the jth box.</p></li></ul><p><strong>Recursive Binary Splitting</strong>: a <em>top-down, greedy</em> approach - <strong>Top-down</strong>: begins at the top of the tree (at which point all observations belong to a single region) and then successively splits the predictor space; each split is indicated via two new branches further down on the tree. - <strong>Greedy</strong>: at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step.</p><p><strong>Methods</strong>: 1. Select the predictor <span class="math inline">\(X_j\)</span> and the cutpoint <span class="math inline">\(s\)</span> such that splitting the predictor space into the regions <span class="math inline">\({X|X_j &lt; s}\)</span> and <span class="math inline">\({X|X_j ≥ s}\)</span> leads to the greatest possible reduction in RSS - In greater detail, for any <span class="math inline">\(j\)</span> and <span class="math inline">\(s\)</span>, we define the pair of half-planes <span class="math display">\[  \begin{align}  R_1(j, s) = {X|X_j &lt; s} ,\quad R_2(j, s) = {X|X_j ≥ s}  \end{align}\]</span> and we seek the value of <span class="math inline">\(j\)</span> and <span class="math inline">\(s\)</span> that <strong>minimize</strong> the equation <span class="math display">\[  \begin{align}  \sum_{:x_i \in R_1(j,s)}(y_i-\hat{y}_{R_1})^2+\sum_{:x_i \in R_2(j,s)}(y_i-\hat{y}_{R_2})^2  \end{align}\]</span> where <span class="math inline">\(\hat{y}_{R_1}\)</span>is the mean response for the training observations in <span class="math inline">\(R_1(j, s)\)</span>,</p><p>where <span class="math inline">\(\hat{y}_{R_1}\)</span>is the mean response for the training observations in <span class="math inline">\(R_1(j, s)\)</span>,</p><p>and we seek the value of <span class="math inline">\(j\)</span> and <span class="math inline">\(s\)</span> that <strong>minimize</strong> the equation <span class="math display">\[  \begin{align}  \sum_{:x_i \in R_1(j,s)}(y_i-\hat{y}_{R_1})^2+\sum_{:x_i \in R_2(j,s)}(y_i-\hat{y}_{R_2})^2  \end{align}\]</span> where <span class="math inline">\(\hat{y}_{R_1}\)</span>is the mean response for the training observations in <span class="math inline">\(R_1(j, s)\)</span>,</p><p>where <span class="math inline">\(\hat{y}_{R_1}\)</span>is the mean response for the training observations in <span class="math inline">\(R_1(j, s)\)</span>,</p><ol start="2" type="1"><li>Repeat the process, looking for the best predictor and best cutpoint in order to split the data further so as to minimize the RSS within each of the resulting regions.</li></ol><ul><li><p>However, this time, instead of splitting the entire predictor space, we split one of the two previously identified regions.</p></li><li><p>We now have three regions. Again, we look to split one of these three regions further, so as to minimize the RSS.</p></li></ul><ol start="3" type="1"><li>The process continues until a stopping criterion is reached; for instance, we may continue until no region contains more than five observations.</li></ol><p><img src="./3.png" width="600"></p><h4 id="step-2">Step 2</h4><p>Predict the response for a given test observation using the mean of the training observations in the region to which that test observation belongs.</p><h3 id="tree-pruning">Tree Pruning</h3><p>A better strategy is to grow a very large tree <span class="math inline">\(T_0\)</span>, and then <strong>prune</strong> it back in order to obtain a <strong>subtree</strong></p><h4 id="cost-complexity-pruning">Cost complexity pruning</h4><p>a.k.a.: <strong>weakest link pruning</strong></p><p>Consider a sequence of trees indexed by a nonnegative tuning parameter α</p><p><img src="./4.png" width="600"></p><p>For each value of α there corresponds a subtree <span class="math inline">\(T ⊂ T_0\)</span> such that</p><p><span class="math display">\[\begin{align}\sum_{m=1}^T\sum_{i:x_i \in R_m}(y_i − \hat{y}_{R_m})^2 + \alpha|T|  \quad \quad (8.4)\end{align}\]</span> is as small as possible.</p><ul><li><span class="math inline">\(|T|\)</span>: the number of terminal nodes of the tree T ,</li><li><span class="math inline">\(R_m\)</span>: the rectangle (i.e. the subset of predictor space) corresponding to the m-th <strong>terminal node</strong>,</li><li><span class="math inline">\(\hat{y}_{R_m}\)</span>: the predicted response associated with <span class="math inline">\(R_m\)</span>—that is, the mean of the training observations in <span class="math inline">\(R_m\)</span>.</li></ul><p>The tuning parameter <span class="math inline">\(α\)</span> controls a <em>trade-off</em> between the subtree’s <strong>complexity</strong> and its <strong>fit to the training data</strong>. When α = 0, then the subtree T will simply equal T0, because then (8.4) just measures the training error. However, as α increases, there is a price to pay for having a tree with many terminal nodes, and so the quantity (8.4) will tend to be minimized for a smaller subtree.</p><p>Equation 8.4 is reminiscent of the lasso, in which a similar formulation was used in order to control the complexity of a linear model.</p><p><img src="./5.png" width="600"> <img src="./6.png" width="600"></p><h2 id="classification-trees">Classification Trees</h2><p>For a classification tree, - We predict that each observation belongs to the <strong>most commonly occurring class</strong> of training observations in the region to which it belongs. - RSS cannot be used as a criterion for making the binary splits <span class="math inline">\(\Rightarrow\)</span> <strong>classification error rate</strong>.</p><h3 id="classification-error-rate">Classification Error Rate</h3><ul><li>Since we plan to assign an observation in a given region to the most commonly occurring class of training observations in that region, the classification error rate is simply the <strong>fraction of the training observations in that region that do not belong to the most common class</strong>:</li></ul><p><span class="math display">\[\begin{align}E=1-\max_k(\hat{p}_{mk})\end{align}\]</span></p><ul><li><span class="math inline">\(\hat{p}_{mk}\)</span> : the proportion of training observations in the mth region that are from the kth class.</li><li>classification error is not sufficiently sensitive for tree-growing, and in practice two other measures are preferable: <strong>Gini index, cross-entropy.</strong></li></ul><h3 id="gini-index">Gini index</h3><p><span class="math display">\[\begin{align}G=\sum_{k=1}^K\hat{p}_{mk}(1-\hat{p}_{mk})\end{align}\]</span></p><ul><li>A measure of total variance across the K classes. It is not hard to see that the Gini index takes on a small value if all of the <span class="math inline">\(\hat{p}_{mk}\)</span>’s are close to zero or one.</li><li>For this reason the Gini index is referred to as a measure of node <strong>purity</strong>—a small value indicates that a node contains predominantly observations from a single class.</li></ul><h3 id="cross-entropy">Cross-Entropy</h3><p><span class="math display">\[\begin{align}D=-\sum_{k=1}^K\hat{p}_{mk}\log{\hat{p}_{mk}}\end{align}\]</span></p><ul><li>Since 0 ≤ <span class="math inline">\(\hat{p}_{mk}\)</span> ≤ 1, it follows that <span class="math inline">\(0 ≤ −\hat{p}_{mk}\log{\hat{p}_{mk}}\)</span>.</li><li>Cross-entropy will take on a value near zero if the <span class="math inline">\(\hat{p}_{mk}\)</span>’s are all near zero or near one. Therefore, like the Gini index, the cross-entropy will take on a small value if the mth node is <strong>pure</strong>.</li></ul><hr><p><strong>Cross-Entropy v.s. Gini index v.s. Classification Error Rate</strong> - When building a classification tree, either the Gini index or the crossentropy are typically used to evaluate the quality of a particular split, since these two approaches are more sensitive to node purity than is the classification error rate. Any of these three approaches might be used when pruning the tree, but the classification error rate is preferable if prediction accuracy of the final pruned tree is the goal.</p><p><img src="./9.png" width="800"></p><ul><li><strong>A surprising characteristic</strong>: some of the splits yield two terminal nodes that have the same predicted value.</li><li><strong>Why is the split performed at all?</strong> The split is performed because it leads to <strong>increased node purity.</strong></li><li><strong>Why is node purity important?</strong> Suppose that we have a test observation that belongs to the region given by that right-hand leaf. Then we can be pretty certain that its response value is Yes. In contrast, if a test observation belongs to the region given by the left-hand leaf, then its response value is probably Yes, but we are much less certain. Even though the split RestECG&lt;1 does not reduce the classification error, it improves the <strong>Gini index and the cross-entropy</strong>, which are more sensitive to node purity.</li></ul><h2 id="trees-versus-linear-models">Trees Versus Linear Models</h2><p>Linear regression assumes a model of the form <span class="math display">\[\begin{align}f(X)=\beta_0+\sum_{i=1}^p\beta_iX_i\end{align}\]</span> Regression trees assume a model of the form <span class="math display">\[\begin{align}f(X)=\sum_{m=1}^Mc_m \cdot I_{X \in R_m}\end{align}\]</span> where R1, . . .,RM represent a partition of feature space</p><p>where R1, . . .,RM represent a partition of feature space</p><p><strong>Linear regression works better</strong>: If the relationship between the features and the response is well approximated by a linear model; regression tree does not exploit this linear structure.</p><p><strong>Regression tree works better</strong>: If instead there is a highly non-linear and complex relationship between the features and the response.</p><h2 id="advantages-and-disadvantages-of-trees">Advantages and Disadvantages of Trees</h2><p><strong>Advantages of decision trees for regression and classification:</strong></p><p>▲ <strong>Interpretation</strong>: Trees are very <strong>easy to explain</strong> to people. In fact, they are even easier to explain than linear regression!</p><p>▲ Some people believe that decision trees more closely <strong>mirror human decision-making</strong> than do the regression and classification approaches.</p><p>▲ <strong>Visualization</strong>: Trees can be <strong>displayed graphically</strong>, and are easily interpreted even by a non-expert.</p><p>▲ Trees can easily handle qualitative predictors without the need to create dummy variables.</p><p><strong>Disadvantages of decision trees for regression and classification:</strong></p><p>▼ Unfortunately, trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches seen in this book.</p><h1 id="bagging">Bagging</h1><p><strong>Bootstrap aggregation</strong>, or <strong>bagging</strong>, is a general-purpose procedure for reducing the variance of a statistical learning method, frequently used in the context of decision trees.</p><p><strong>Averaging a set of observations reduces variance</strong>: Recall that given a set of n independent observations Z1, . . . , Zn, each with variance <span class="math inline">\(σ^2\)</span>, the variance of the mean <span class="math inline">\(\bar{Z}\)</span> of the observations is given by <span class="math inline">\(σ^2/n\)</span>.</p><ul><li>A natural way to reduce the variance and hence increase the prediction accuracy of a statistical learning method is to <strong>take many training sets from the population</strong>, build a separate prediction model using each training set, and average the resulting predictions.</li></ul><p><strong>Bootstrap</strong> taking repeated samples from the (single) training data set</p><p><strong>Bagging</strong></p><ul><li>Generate B different bootstrapped training data sets.</li><li>Train our method on the bth bootstrapped training set in order to get <span class="math inline">\(\hat{f}^{*b}(x)\)</span></li><li>Finally average all the predictions, to obtain</li></ul><p><span class="math display">\[\begin{align}\hat{f}_{bag}(x)=\frac{1}{B}\sum_{b=1}^B\hat{f}^{*b}(x)\end{align}\]</span></p><p><strong>Apply bagging to regression trees</strong></p><ul><li>Construct B regression trees using B bootstrapped training sets</li><li>Average the resulting predictions. These trees are grown deep, and are not pruned. Hence each individual tree has high variance, but low bias. Averaging these B trees reduces the variance.</li></ul><p><strong>Bagging on Classification Tree</strong></p><ul><li>For a given test observation, we can record the class predicted by each of the B trees, and take a <strong>majority vote</strong>: the overall prediction is the most commonly occurring class among the B predictions.</li></ul><p><strong>B</strong></p><ul><li>In practice weuse a value of B sufficiently large that the error has settled down, like B=100.</li></ul><h2 id="out-of-bag-error-estimation">Out-of-Bag Error Estimation</h2><p>Recall that the key to bagging is that trees are repeatedly fit to bootstrapped subsets of the observations. One can show that on average, each bagged tree makes use of around 2/3 of the observations. The remaining one-third of the observations not used to fit a given bagged tree are referred to as the <strong>out-of-bag (OOB)</strong> observations.</p><blockquote><p>We can predict the response for the ith observation using each of the trees inwhich that observation was OOB.</p></blockquote><ul><li>This will yield around B/3 predictions for the ith observation.</li><li>To obtain a single prediction for the ith observation, we can <strong>average</strong> these predicted responses (regression) or can take a <strong>majority vote</strong> (classification).</li><li>This leads to a single OOB prediction for the ith observation.</li></ul><p>The OOB approach for estimating the test error is particularly convenient when performing bagging on large data sets for which <strong>cross-validation</strong> would be computationally onerous.</p><h2 id="variable-importance-measures">Variable Importance Measures</h2><p><strong>Bagging improves prediction accuracy at the expense of interpretability</strong></p><ul><li>When we bag a large number of trees, it is no longer possible to represent the resulting statistical learning procedure using a single tree, and it is no longer clear which variables are most important to the procedure</li></ul><p><strong>Variable Importance</strong></p><ul><li><p>One can obtain an overall summary of the importance of each predictor using the RSS (for bagging regression trees) or the Gini index (for bagging classification trees).</p></li><li><p><strong>Bagging regression trees</strong>: Record the total amount that the RSS is decreased due to splits over a given predictor, averaged over all B trees. A large value indicates an important predictor. <span class="math display">\[\begin{align}RSS=\sum_{j=1}^J\sum_{i \in R_j} (y_i-\hat{y}_{R_j})^2\end{align}\]</span></p></li><li><p><strong>Bagging classification trees</strong>: Add up the total amount that the <strong>Gini index</strong> is decreased by splits over a given predictor, averaged over all B trees.</p></li></ul><p><img src="./11.png" width="600"></p><h1 id="random-forest">Random Forest</h1><p><strong>Random forests</strong> provide an improvement over bagged trees by way of a small tweak that <strong>decorrelates</strong> the trees.</p><p>As in bagging, we build a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, <em>a random sample of m predictors is chosen as split candidates</em> from the full set of p predictors.</p><p><strong>The split is allowed to use only one of those m predictors.</strong> A fresh sample of m predictors is taken at each split, and typically we choose <span class="math inline">\(m ≈\sqrt{p}\)</span></p><p><strong>Rationale</strong>:</p><ul><li>Suppose that there is one very strong predictor in the data set, along with a number of other moderately strong predictors. Then in the collection of bagged trees, most or all of the trees will use this strong predictor in the top split. Consequently, <em>all of the bagged trees will look quite similar to each other.</em></li><li>Hence the predictions from the bagged trees will be highly correlated. Unfortunately, averaging many highly correlated quantities does not lead to as large of a reduction in variance as averaging many uncorrelated quantities.</li></ul><p><strong>Decorrelating</strong> the trees: Random forests forces each split to consider only a subset of the predictors, making the average of the resulting trees less variable and hence more reliable.</p><h1 id="boosting">Boosting</h1><p><strong>Boosting</strong>: another approach for improving the predictions resulting from a decision tree.</p><ul><li>Trees are grown <strong>sequentially</strong>: each tree is grown using information from previously grown trees.</li><li>Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set.</li></ul><p><img src="./12.png" width="600"></p><p><strong>Idea behind this procedure</strong></p><ul><li>Unlike fitting a single large decision tree to the data, which amounts to fitting the data hard and potentially overfitting, the boosting approach instead <strong>learns slowly</strong>.</li><li>Given the current model, we fit a decision tree to the residuals from the model. That is, we fit a tree using the current residuals, rather than the outcome Y , as the response.</li><li>We then add this new decision tree into the fitted function in order to update the residuals. Each of these trees can be rather small, with just a few terminal nodes, determined by the parameter <strong>d</strong> in the algorithm.</li><li>By fitting small trees to the residuals, we slowly improve <span class="math inline">\(\hat{f}\)</span> in areas where it does not perform well.</li><li>The shrinkage parameter <strong>λ</strong> slows the process down even further, allowing more and different shaped trees to attack the residuals.</li></ul><blockquote><p>Note that in boosting, unlike in bagging, the construction of each tree depends strongly on the trees that have already been grown.</p></blockquote><p><strong>Boosting has three tuning parameters:</strong></p><ol type="1"><li>The number of trees <span class="math inline">\(B\)</span>.</li><li>The shrinkage parameter <span class="math inline">\(λ\)</span>, a small positive number. This controls the rate at which boosting learns.</li><li>The number <span class="math inline">\(d\)</span> of splits in each tree, which controls the complexity of the boosted ensemble. Often d = 1 works well, in which case each tree is a <strong>stump</strong>, consisting of a single split. In this case, the boosted ensemble is fitting an <strong>additive model</strong>, since each term involves only a single variable. More generally <span class="math inline">\(d\)</span> is the <strong>interaction depth</strong>, and controls the interaction order of the boosted model, since <span class="math inline">\(d\)</span> splits can involve at most d variables.</li></ol><p><strong>Boosting V.S. Random forests:</strong></p><ul><li>In boosting, because the growth of a particular tree takes into account the other trees that have already been grown, smaller trees are typically sufficient.</li><li>Using smaller trees can aid in interpretability as well; for instance, using <strong>stumps</strong> leads to an additive model.</li></ul><hr><p><strong>Ref:</strong></p><p>James, Gareth, et al. <em>An introduction to statistical learning</em>. Vol. 112. New York: springer, 2013.</p><p>Hastie, Trevor, et al. &quot;The elements of statistical learning: data mining, inference and prediction.&quot; <em>The Mathematical Intelligencer</em> 27.2 (2005): 83-85</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;introduction-to-descision-tree&quot;&gt;Introduction to Descision Tree&lt;/h1&gt;
&lt;h2 id=&quot;regression-trees&quot;&gt;Regression Trees&lt;/h2&gt;
&lt;h3 id=&quot;predicting-baseball-players-salaries-using-regression-trees&quot;&gt;Predicting Baseball Players’ Salaries Using Regression Trees&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Terminal nodes&lt;/strong&gt;: The regions R1, R2, and R3 are known as terminal nodes or leaves of the tree.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Internal nodes&lt;/strong&gt;: The points along the tree where the predictor space is split are referred to as internal nodes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Branches&lt;/strong&gt;: The segments of the trees that connect the nodes as branches&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Trees" scheme="https://nancyyanyu.github.io/tags/Trees/"/>
    
  </entry>
  
  <entry>
    <title>Study Note: Model Selection and Regularization (Ridge &amp; Lasso)</title>
    <link href="https://nancyyanyu.github.io/posts/a065f58f/"/>
    <id>https://nancyyanyu.github.io/posts/a065f58f/</id>
    <published>2019-10-19T23:02:42.273Z</published>
    <updated>2019-10-19T23:21:22.427Z</updated>
    
    <content type="html"><![CDATA[<h1 id="introduction-to-model-selection">Introduction to Model Selection</h1><p><strong>Setting:</strong></p><ul><li><p>In the regression setting, the standard linear model <span class="math inline">\(Y = β_0 + β_1X_1 + · · · + β_pX_p + \epsilon\)</span></p></li><li><p>In the chapters that follow, we consider some approaches for extending the linear model framework.</p></li></ul><p><strong>Reason of using other fitting procedure than lease squares</strong>:</p><ul><li><strong><em>Prediction Accuracy:</em></strong><ul><li>Provided that the true relationship between the response and the predictors is approximately linear, the least squares estimates will have low bias.</li><li>If n <span class="math inline">\(\gg\)</span> p, least squares estimates tend to also have low variance <span class="math inline">\(\Rightarrow\)</span> perform well on test data.</li><li>If n is not much larger than p, least squares fit has large variance <span class="math inline">\(\Rightarrow\)</span> overfitting <span class="math inline">\(\Rightarrow\)</span> consequently poor predictions on test data</li><li>If p &gt; n, no more unique least squares coefficient estimate: the <strong>variance is infinite</strong> so the method cannot be used at all</li></ul><p>By <strong>constraining</strong> or <strong>shrinking</strong> the estimated coefficients, we can often substantially reduce the variance at the cost of a negligible increase in bias.</p></li><li><strong><em>Model Interpretability</em></strong>：<ul><li>irrelevant variables leads to unnecessary complexity in the resulting model. By removing these variables—that is, by setting the corresponding coefficient estimates to zero—we can obtain a model that is more easily interpreted.</li><li>least squares is extremely unlikely to yield any coefficient estimates that are exactly zero <span class="math inline">\(\Rightarrow\)</span> <strong>feature selection</strong></li></ul></li></ul><p><strong>Alternatives of lease squares:</strong></p><ol type="1"><li>Subset Selection</li><li>Shrinkage</li><li><a href="https://nancyyanyu.github.io/posts/cac93a23/">Dimension Reduction</a></li></ol><a id="more"></a><h1 id="subset-selection">Subset Selection</h1><h4 id="drawbacks-of-least-squares-estimates">Drawbacks of least squares estimates:</h4><ul><li><em>prediction accuracy</em>: the least squares estimates often have low bias but large variance. Prediction accuracy can sometimes be improved by shrinking or setting some coeﬃcients to zero.By doing so we sacriﬁce a little bit of bias to reduce the variance of the predicted values, and hence may improve the overall prediction accuracy.</li><li><em>interpretation</em>: With a large number of predictors, we often would like to determine a smaller subset that exhibit the strongest eﬀects. In order to get the “big picture,” we are willing to sacriﬁce some of the small details.</li></ul><h2 id="best-subset-selection">Best Subset Selection</h2><p>Best subset regression ﬁnds for each k ∈{0, 1, 2,...,p} the subset of size k that gives smallest residual sum of squares.</p><p>We choose the smallest model that minimizes an estimate of the expected prediction error.</p><p><img src="./best_subset.png" width="600"></p><blockquote><p>FIGURE 3.5.All possible subset models for the prostate cancer example. At each subset size is shown the residual sum-of-squares for each model of that size.</p></blockquote><p><strong>Approach</strong></p><ol type="1"><li>fit a separate least squares regression best subset for each possible combination of the p predictors. That is, we fit all p models selection that contain exactly one predictor, all <span class="math inline">\(\left(\begin{array}{c}p\\ 2\end{array}\right)= p(p−1)/2\)</span> models that contain exactly two predictors, and so forth.</li><li>We then look at all of the resulting models, with the goal of identifying the one that is best.</li></ol><p><img src="./1.png" width="600"></p><p><strong>Note</strong></p><ul><li><span class="math inline">\(RSS\)</span> of these p + 1 models decreases monotonically, and the <span class="math inline">\(R2\)</span> increases monotonically, as the number of features included in the models increases. Therefore, if we use these statistics to select the best model, then we will always end up with a model involving all of the variables</li><li>The problem of selecting the best model from among the <span class="math inline">\(2^p\)</span> possibilities considered by best subset selection is not trivial.</li></ul><h2 id="stepwise-selection">Stepwise Selection</h2><blockquote><p>Rather than search through all possible subsets (which becomes infeasible for p much larger than 40), we can seek a good path through them.</p></blockquote><h3 id="forward-stepwise-selection">Forward Stepwise Selection</h3><p><strong>Forward-stepwise selection</strong> starts with the intercept, and then sequentially adds into the model the predictor that most improves the ﬁt.</p><p>Forward-stepwise selection is a <em>greedy algorithm</em>, producing a nested sequence of models. In this sense it might seem sub-optimal compared to best-subset selection.</p><h4 id="advantages">Advantages:</h4><ul><li><strong>Computational</strong>: for large p we cannot compute the best subset sequence, but we can always compute the forward stepwise sequence</li><li><strong>Statistical</strong>: a price is paid in variance for selecting the best subset of each size; forward stepwise is a more constrained search, and will have lower variance, but perhaps more bias</li></ul><p><strong>Approach</strong></p><ol type="1"><li><strong>Forward stepwise selection</strong> begins with a model containing no predictors, and then adds predictors to the model, one-at-a-time, until all of the predictors are in the model.</li><li>In particular, at each step the variable that gives the greatest additional improvement to the fit is added to the model.</li></ol><p><img src="./2.png" width="600"></p><p><strong>Forward Stepwise Selection V.S. Best Subset Selection</strong></p><ul><li>Forward stepwise selection’s computational advantage over best subset selection is clear.</li><li>Forward stepwise is not guaranteed to find the best possible model out of all <span class="math inline">\(2^p\)</span> models containing subsets of the p predictors.</li></ul><h3 id="backward-stepwise-selection">Backward Stepwise Selection</h3><p><strong>Backward-stepwise selection</strong> starts with the full model, and sequentially deletes the predictor that has the least impact on the ﬁt. The candidate for dropping is the variable with the smallest Z-score</p><p><strong>Approach</strong></p><ol type="1"><li><strong>Backward Stepwise Selection</strong> begins with the full least squares model containing all p predictors, and then iteratively removes the least useful predictor, one-at-a-time</li></ol><p><img src="./3.png" width="600"></p><p><strong>Backward Stepwise Selection V.S. Forward Stepwise Selection</strong>:</p><ul><li>Like forward stepwise selection, the backward selection approach searches through only 1+p(p+1)/2 models, and so can be applied in settings where p is too large to apply best subset selection.</li><li>Like forward stepwise selection, backward stepwise selection is not guaranteed to yield the best model containing a subset of the p predictors.</li><li>Backward selection requires that the number of samples n is larger than the number of variables p (so that the full model can be fit). In contrast, forward stepwise can be used even when n &lt; p, and so is the only viable subset method when p is very large.</li></ul><h3 id="hybrid-approaches">Hybrid Approaches</h3><p><strong>Approach</strong></p><ol type="1"><li>Variables are added to the model sequentially, in analogy to forward selection.</li><li>However, after adding each new variable, the method may also remove any variables that no longer provide an improvement in the model fit.</li></ol><p><strong>Note</strong></p><p>Such an approach attempts to more closely mimic best subset selection while retaining the computational advantages of forward and backward stepwise selection.</p><h2 id="choosing-the-optimal-model">Choosing the Optimal Model</h2><p>The training error can be a poor estimate of the test error. Therefore, RSS and R2 are not suitable for selecting the best model among a collection of models with different numbers of predictors.</p><p><strong>2 Methods</strong>:</p><ol type="1"><li><em>indirectly</em> estimate test error by making an adjustment to the training error to account for the bias due to overfitting.</li><li><em>directly</em> estimate the test error, using either a validation set approach or a cross-validation approach</li></ol><h3 id="c_p-aic-bic-adjusted-r2"><span class="math inline">\(C_p\)</span>, <span class="math inline">\(AIC\)</span>, <span class="math inline">\(BIC\)</span>, Adjusted <span class="math inline">\(R^2\)</span></h3><ul><li>the training set MSE is generally an underestimate of the test MSE. (Recall that MSE = RSS/n.)</li><li>the training error will decrease as more variables are included in the model, but the test error may not.</li><li>Therefore, training set RSS and training set R2 cannot be used to select from among a set of models with different numbers of variables.</li></ul><h4 id="c_p"><span class="math inline">\(C_p\)</span></h4><p><span class="math inline">\(C_p\)</span> estimate of test MSE: <span class="math display">\[\begin{align}C_p=\frac{1}{n}(RSS+2d\hat{\sigma}^2)\end{align}\]</span> where <span class="math inline">\(\hat{\sigma}^2\)</span> is an estimate of the variance of the error <span class="math inline">\(\epsilon\)</span></p><p><strong>Note</strong>:</p><ul><li>The <span class="math inline">\(C_p\)</span> statistic adds a penalty of <span class="math inline">\(2d\hat{\sigma}^2\)</span> to the training RSS in order to adjust for the fact that the training error tends to underestimate the test error.</li><li>The penalty increases as the number of predictors in the model increases; this is intended to adjust for the corresponding decrease in training RSS.</li><li>If <span class="math inline">\(\hat{\sigma}^2\)</span> is an unbiased estimate of <span class="math inline">\(\sigma^2\)</span> in, then <span class="math inline">\(C_p\)</span> is an unbiased estimate of test MSE</li><li>When determining which of a set of models is best, we choose the model with the lowest <span class="math inline">\(C_p\)</span> value.</li></ul><h4 id="aic">AIC</h4><p>The AIC criterion is defined for a large class of models fit by maximum likelihood. In the case of the model <span class="math inline">\(Y = β_0 + β_1X_1 + · · · + β_pX_p + \epsilon\)</span> with Gaussian errors, maximum likelihood and least squares are the same thing.</p><p>In this case AIC is given by <span class="math display">\[\begin{align}AIC=\frac{1}{n\hat{\sigma}^2}(RSS+2d\hat{\sigma}^2)\end{align}\]</span> For least squares models, Cp and AIC are proportional to each other</p><h4 id="bic">BIC</h4><p>For the least squares model with d predictors, the BIC is, up to irrelevant constants, given by <span class="math display">\[\begin{align}BIC=\frac{1}{n}(RSS+\log(n)d\hat{\sigma}^2)\end{align}\]</span> Since log(n) &gt; 2 for any n &gt; 7, the BIC statistic generally places a heavier penalty on models with many variables, and hence results in the selection of smaller models than Cp.</p><h4 id="adjusted-r2">Adjusted <span class="math inline">\(R^2\)</span></h4><p>Recall: <span class="math display">\[\begin{align}R^2=1 − RSS/TSS=1-\frac{RSS}{\sum(y_i-\bar{y})^2}\end{align}\]</span> <strong>TSS</strong>: total sum of squares for the response</p><p>For a least squares model with d variables, <strong>the adjusted R2</strong> statistic is calculated as <span class="math display">\[Adjusted  \, R^2=1 − \frac{RSS/(n-d-1)}{TSS/(n-1)}\]</span> <strong>Note</strong>:</p><ul><li>a large value of adjusted R2 indicates a model with a small test error. Maximizing the adjusted R2 is equivalent to minimizing <span class="math inline">\(RSS/(n−d−1)\)</span></li><li><span class="math inline">\(RSS/(n−d−1)\)</span> may increase or decrease, due to the presence of d in the denominator.</li></ul><p><strong>Intuition</strong>:</p><ul><li>once all of the correct variables have been included in the model, adding additional noise variables will lead to only a very small decrease in RSS</li><li>Unlike the R2 statistic, the adjusted R2 statistic pays a price for the inclusion of unnecessary variables in the model</li></ul><h3 id="validation-and-cross-validation">Validation and Cross-Validation</h3><p>As an alternative to the approaches just discussed, we can compute the validation set error or the cross-validation error for each model under consideration, and then select the model for which the resulting estimated test error is smallest.</p><p><strong>Advantage over <span class="math inline">\(C_p, AIC, BIC\)</span></strong>:</p><ul><li>Direct estimate of the test error, and makes fewer assumptions about the true underlying model.</li><li>Used in a wider range of model selection tasks, even in cases where it is hard to pinpoint the model degrees of freedom or hard to estimate the error variance σ2.</li></ul><p><strong>One-standard-error rule</strong>: We first calculate the standard error of the estimated test MSE for each model size, and then select the smallest model for which the estimated test error is within one standard error of the lowest point on the curve.</p><ul><li><strong>Rationale</strong>: if a set of models appear to be more or less equally good, then we might as well choose the simplest model</li></ul><h1 id="shrinage-methods">Shrinage Methods</h1><h2 id="ridge-regression">Ridge Regression</h2><p><strong>Ridge regression</strong> shrinks the regression coeﬃcients by imposing a penalty on their size.The ridge coeﬃcients minimize a penalized residual sum of squares: <span class="math display">\[\begin{align}\hat{\beta}^{ridge}=argmin_\beta {\sum_{i=1}^N(y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j)^2+\lambda\sum_{j=1}^p\beta_j^2}\end{align}\]</span></p><ul><li>λ ≥ 0 is a complexity parameter that controls the amount of shrinkage</li></ul><p>Writing the criterion in matrix form: <span class="math display">\[\begin{align}RSS(\lambda)=(\mathbf{y}-\mathbf{X}\beta)^T(\mathbf{y}-\mathbf{X}\beta)+\lambda\beta^T\beta\end{align}\]</span> The ridge regression solutions: <span class="math display">\[\begin{align}\hat{\beta}^{ridge}=(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}\end{align}\]</span></p><ul><li><span class="math inline">\(\mathbf{I}\)</span> is the p×p identity matrix</li></ul><p>Note:</p><ul><li>the ridge regression solution is again a linear function of <span class="math inline">\(\mathbf{y}\)</span>;</li><li>The solution adds a positive constant to the diagonal of <span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> before inversion, which makes the problem nonsingular.</li></ul><p>Recall least squares: <span class="math display">\[\begin{align}RSS=\sum_{i=1}^n\left( y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j \right)^2\end{align}\]</span> <strong>Ridge regression</strong> coefficient estimates <span class="math inline">\(\hat{\beta}^R\)</span> are the values that minimize <span class="math display">\[\begin{align}\sum_{i=1}^n\left( y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j \right)^2+\lambda\sum_{j=1}^p\beta_j^2=RSS+\lambda\sum_{j=1}^p\beta_j^2\end{align}\]</span></p><p><strong>Trade-off:</strong></p><ol type="1"><li>Ridge regression seeks coefficient estimates that fit the data well, by making the RSS small.</li><li><strong>shrinkage penalty</strong> <span class="math inline">\(\lambda\sum_{j=1}^p\beta_j^2\)</span> is small when β1, . . . , βp are close to zero, and so it has the effect of shrinking the estimates of βj towards zero</li></ol><p><strong>Standardization</strong>:</p><ul><li><p><strong>scale equivariant</strong>: The standard least squares coefficient estimates are scale equivariant: multiplying Xj by a constant c simply leads to a scaling of the least squares coefficient estimates by a factor of 1/c.</p></li><li><p><span class="math inline">\(X_{j,\lambda}^\beta\)</span> will depend not only on the value of λ, but also on the scaling of the jth predictor, and the scaling of the other predictors. It is best to apply ridge regression after standardizing the predictors <span class="math display">\[\begin{align}\tilde{x}_{ij}=\frac{x_{ij}}{\sqrt{\frac{1}{n}\sum_{i=1}^n(x_{ij}-\bar{x}_j)^2}}\end{align}\]</span> The denominator is the estimated standard deviation of the jth predictor</p><p>The denominator is the estimated standard deviation of the jth predictor</p></li></ul><h3 id="ridge-regression-improves-over-least-squares">Ridge Regression Improves Over Least Squares</h3><ol type="1"><li><strong>bias-variance trade-off</strong></li></ol><ul><li>Ridge regression’s advantage over least squares is rooted in the bias-variance trade-off. As λ increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias.</li><li>At the least squares coefficient estimates, which correspond to ridge regression with λ = 0, the variance is high but there is no bias. But as λ increases, the shrinkage of the ridge coefficient estimates leads to a substantial reduction in the variance of the predictions, at the expense of a slight increase in bias. <img src="./4.png" width="600"></li></ul><blockquote><p>Ridge regression works best in situations where the least squares estimates have high variance</p></blockquote><ol start="2" type="1"><li><strong>computational advantages over best subset selection</strong></li></ol><h3 id="singular-value-decomposition-svd">Singular value decomposition (SVD)</h3><p>The <strong>singular value decomposition (SVD)</strong> of the centered input matrix X gives us some additional insight into the nature of ridge regression. The SVD of the N × p matrix X has the form: <span class="math display">\[\begin{align}X=UDV^T\end{align}\]</span></p><ul><li>U: N×p orthogonal matrices, with the columns of U spanning the column space of X</li><li>V: p×p orthogonal matrices, the columns of V spanning the row space of X</li><li>D: p×p diagonal matrix, with diagonal entries d1 ≥ d2 ≥···≥ dp ≥ 0 called the singular values of X. If one or more values dj =0,X is singular</li></ul><p>least squares ﬁtted vector: <span class="math display">\[\begin{align}\mathbf{X}\hat{\beta}^{ls}&amp;=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y} \\&amp;=UDV^T (VD^TU^TUDV^T)^{-1}VD^TU^Ty \\&amp;=UDV^T (VD^TDV^T)^{-1}VD^TU^Ty \\&amp;=UDV^T (V^T)^{-1}D^{-1}(D^T)^{-1}V^{-1}VD^TU^Ty \\&amp;=\mathbf{U}\mathbf{U}^T\mathbf{y}\end{align}\]</span> Note: <span class="math inline">\(\mathbf{U}^T\mathbf{y}\)</span> are the coordinates of y with respect to the orthonormal basis U.</p><p>The ridge solutions: <span class="math display">\[\begin{align}\mathbf{X}\hat{\beta}^{ridge}&amp;=\mathbf{X}(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y} \\&amp;=UD(D^2+\lambda\mathbf{I})^{-1}D^TU^Ty \\&amp;=\sum_{j=1}^p\mathbf{u}_j\frac{d^2_j}{d^2_j+\lambda}\mathbf{u}^T_j\mathbf{y}\end{align}\]</span></p><ul><li><span class="math inline">\(\mathbf{u}_j\)</span> are the columns of U</li></ul><p>Note: ridge regression computes the coordinates of y with respect to the orthonormal basis U. It then shrinks these coordinates by the factors <span class="math inline">\(\frac{d^2_j}{d^2_j+\lambda}\)</span></p><h4 id="what-does-a-small-value-of-d2_j-mean">What does a small value of <span class="math inline">\(d^2_j\)</span> mean?</h4><p>The SVD of the centered matrix X is another way of expressing the <strong>principal components</strong> of the variables in X. The sample covariance matrix is given by <span class="math inline">\(S=X^TX/N\)</span>, we have</p><p><strong>Eigen decomposition of <span class="math inline">\(X^TX\)</span>:</strong> <span class="math display">\[\begin{align}\mathbf{X}^T\mathbf{X}=VD^TU^TUDV^T=VD^2V^T\end{align}\]</span> The eigenvectors <span class="math inline">\(v_j\)</span> (columns of V) are also called the <strong>principal components</strong> (or Karhunen–Loeve) directions of X. The ﬁrst principal component direction <span class="math inline">\(v_1\)</span> has the property that <span class="math inline">\(z_1=Xv_1\)</span> has the largest sample variance amongst all normalized linear combinations of the columns of X, which is: <span class="math display">\[\begin{align}Var(z_1)=Var(Xv_1)=\frac{d^2_1}{N}\end{align}\]</span> and in fact <span class="math inline">\(z_1=Xv_1=u_1d_1\)</span>. The derived variable <span class="math inline">\(z_1\)</span> is called the ﬁrst principal component of X, and hence <span class="math inline">\(u_1\)</span> is the normalized ﬁrst principal component.Subsequent principal components <span class="math inline">\(z_j\)</span> have maximum variance <span class="math inline">\(\frac{d^2_j}{N}\)</span>, subject to being orthogonal to the earlier ones.</p><p>Hence the small singular values <span class="math inline">\(d_j\)</span> correspond to directions in the column space of X having small variance, and ridge regression shrinks these directions the most.</p><h3 id="eﬀective-degrees-of-freedom">Eﬀective degrees of freedom</h3><p><span class="math display">\[\begin{align}df(\lambda)&amp;=tr[\mathbf{X}(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})^{-1}\mathbf{X}^T] \\&amp;=tr[\mathbf{H}\lambda] \\&amp;=\sum^p_{j=1}\frac{d^2_j}{d^2_j+\lambda}\end{align}\]</span></p><p>This monotone decreasing function of λ is the eﬀective degrees of freedom of the ridge regression ﬁt. Usually in a linear-regression ﬁt with p variables,the degrees-of-freedom of the ﬁt is p, the number of free parameters.</p><p>Note that</p><blockquote><p>df(λ)= p as λ = 0 (no regularization)</p></blockquote><blockquote><p>df(λ) → 0 as λ →∞.</p></blockquote><h2 id="the-lasso">The Lasso</h2><p>The lasso coefficients, <span class="math inline">\(\hat{\beta}_\lambda^L\)</span>, minimize the quantity <span class="math display">\[\begin{align}\sum_{i=1}^n\left( y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j \right)^2+\lambda\sum_{j=1}^p|\beta_j|=RSS+\lambda\sum_{j=1}^p|\beta_j|\end{align}\]</span></p><p>The lasso is a shrinkage method like ridge, with subtle but important differences.The lasso estimate is deﬁned by: <span class="math display">\[\begin{align}\hat{\beta}^{lasso}&amp;=argmin_\beta\sum_{i=1}^N(y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j)^2 \\&amp; s.t. \sum_{j=1}^p|\beta_j|\leq t\end{align}\]</span> Lasso problem in <em>Lagrangian form</em>: <span class="math display">\[\begin{align}\hat{\beta}^{lasso}&amp;=argmin_\beta\{ \sum_{i=1}^N(y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j)^2+\lambda\sum_{j=1}^p|\beta_j| \}\end{align}\]</span></p><h4 id="section"></h4><h3 id="another-formulation-for-ridge-regression-and-the-lasso">Another Formulation for Ridge Regression and the Lasso</h3><p>The lasso and ridge regression coefficient estimates solve the problems <span class="math display">\[\begin{align}minimize_\beta \left\{\sum_{i=1}^n\left( y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j \right)^2\right\}\,\, subject\, to \, \sum_{j=1}^p|\beta_j|\leq s \\minimize_beta \left\{\sum_{i=1}^n\left( y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j \right)^2\right\}\,\, subject\, to \, \sum_{j=1}^p\beta_j^2\leq s\end{align}\]</span> When we perform the lasso we are trying to find the set of coefficient estimates that lead to the smallest RSS, subject to the constraint that there is a budget s for how large <span class="math inline">\(\sum_{j=1}^p|\beta_j|\)</span> can be. When s is extremely large, then this budget is not very restrictive, and so the coefficient estimates can be large</p><p><strong>A close connection between the lasso, ridge regression, and best subset selection</strong>:</p><p>best subset selection is equivelant to : <span class="math display">\[\begin{align}minimize_{beta} \left\{\sum_{i=1}^n\left( y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j \right)^2\right\}\,\, subject\, to \, \sum_{j=1}^pI(\beta_j\neq 0)\leq s\end{align}\]</span> Therefore, we can interpret <strong>ridge regression</strong> and <strong>the lasso</strong> as computationally feasible alternatives to <strong>best subset selection</strong>.</p><h3 id="the-variable-selection-property-of-the-lasso">The Variable Selection Property of the Lasso</h3><p>The lasso and ridge regression coefficient estimates are given by the first point at which an ellipse contacts the constraint region.</p><p><strong>ridge regression</strong>: <strong>circular</strong> constraint with no sharp points, so the ridge regression coefficient estimates will be exclusively non-zero.</p><p><strong>the lasso</strong>: constraint has <strong>corners</strong> at each of the axes, and so the ellipse will often intersect the constraint region at an axis.</p><ul><li>the <span class="math inline">\(l_1\)</span> penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter λ is sufficiently large.</li><li>Hence, much like best subset selection, the lasso performs <strong>variable selection</strong></li></ul><blockquote><p>lasso yields <strong>sparse</strong> models</p></blockquote><p><img src="./5.png" width="600"></p><h3 id="comparing-the-lasso-and-ridge-regression">Comparing the Lasso and Ridge Regression</h3><p><strong>SAME</strong>: Ridge &amp; Lasso all can yield a reduction in variance at the expense of a small increase in bias, and consequently can generate more accurate predictions.</p><p><strong>DIFFERENCES</strong>:</p><ul><li>Unlike ridge regression, the <strong>lasso performs variable selection</strong>, and hence results in models that are easier to interpret.</li><li>Ridge regression outperforms the lasso in terms of prediction error in this setting</li></ul><p><strong>Suitable setting</strong>:</p><ul><li><strong>Lasso</strong>: perform better in a setting where a relatively small number of predictors have substantial coefficients, and the remaining predictors have coefficients that are very small or that equal zero.</li><li><strong>Ridge regression</strong>: perform better when the response is a function of many predictors, all with coefficients of roughly equal size.</li><li>The number of predictors that is related to the response is never known a <strong>priori</strong> for real data sets. Cross-validation can be used in order to determine which approach is better on a particular data set.</li></ul><p><img src="./6.png" width="600"></p><p>The L2 ridge penalty <span class="math inline">\(\sum_{j=1}^p\beta_j^2\)</span> is replaced by the L1 lasso penalty <span class="math inline">\(\sum_{j=1}^p|\beta_j|\)</span>. This latter constraint makes the solutions nonlinear in the <span class="math inline">\(y_i\)</span>, and there is no closed form expression as in ridge regression.</p><blockquote><p>t should be adaptively chosen to minimize an estimate of expected prediction error.</p></blockquote><ul><li>if <span class="math inline">\(t&gt;t_0=\sum_{j=1}^p|\hat{\beta_j^{ls}}|\)</span>, then the lasso estimates are the <span class="math inline">\(\hat{\beta_j^{ls}}\)</span></li><li>if <span class="math inline">\(t&gt;t_0/2\)</span>, the least squares coeﬃcients are shrunk by about 50% on average</li></ul><p>The standardized parameter: <span class="math inline">\(s=t/\sum_1^p|\hat{\beta_j}|\)</span></p><ul><li>s=1.0, the lasso coeﬃcients are the least squares estimates</li><li>s-&gt;0, as the lasso coeﬃcients -&gt;0</li></ul><h2 id="shrinkage-methods-v.s.-subset-selection"><strong>Shrinkage Methods v.s. Subset Selection</strong>:</h2><ul><li><strong>Subset selection</strong><ul><li>described involve using least squares to fit a linear model that contains a subset of the predictors.</li><li>are discrete process—variables are either retained or discarded—it often exhibits high variance,and so doesn’t reduce the prediction error of the full model.</li></ul></li><li><strong>Shrinkage Methods</strong><ul><li>fit a model containing all p predictors by constraining or regularizing the coefficient estimates, or equivalently, that shrinks the coefficient estimates towards zero.</li><li>are more continuous, and don’t suﬀer as much from high variability.</li></ul></li></ul><h2 id="discussion-subset-selection-ridge-regression-and-the-lasso">Discussion: Subset Selection, Ridge Regression and the Lasso</h2><ul><li>Ridge regression: does a proportional shrinkage</li><li>Lasso: translates each coeﬃcient by a constant factor λ, truncating at zero --“soft thresholding,”</li><li>Best-subset selection: drops all variables with coeﬃcients smaller than the Mth largest --“hard-thresholding.” <img src="./lass_ridge.png" width="550"></li></ul><h3 id="bayes-view">Bayes View</h3><p>Consider the criterion <span class="math display">\[\begin{align}\tilde{\beta}&amp;=argmin_\beta\{ \sum_{i=1}^N(y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j)^2+\lambda\sum_{j=1}^p|\beta_j|^q \}\end{align}\]</span> for q ≥ 0. The contours of constant value of <span class="math inline">\(\sum_{j=1}^p|\beta_j|^q\)</span> are shown in Figure 3.12, for the case of two inputs. &lt;img src=&quot;./images/q.png&quot;,width=550&gt;</p><p><font color="red">The lasso, ridge regression and best subset selection are Bayes estimates with diﬀerent priors:</font>Thinking of <span class="math inline">\(\sum_{j=1}^p|\beta_j|^q\)</span> as the log-prior density for βj , these are also the equi-contours of the prior distribution of the parameters.</p><ul><li>q = 0 :variable subset selection, as the penalty simply counts the number of nonzero parameters;</li><li>q = 1 :the lasso, also Laplace distribution for each input, with density <span class="math inline">\(\frac{1}{2\tau}exp(-|\beta|/\tau)\)</span>, where <span class="math inline">\(\tau=1/\lambda\)</span></li><li>q = 2 :the ridge</li></ul><h1 id="considerations-in-high-dimensions">Considerations In High Dimensions</h1><h2 id="high-dimensional-data">High-Dimensional Data</h2><p><strong>High-dimensional</strong>: Data sets containing more features than observations are often referred to as high-dimensional.</p><ul><li>Classical approaches such as least squares linear regression are not appropriate in this setting</li></ul><h2 id="what-goes-wrong-in-high-dimensions">What Goes Wrong in High Dimensions?</h2><ol type="1"><li>When the number of features p is as large as, or &gt;n, least squares cannot be performed.</li></ol><p><strong>Reason</strong>: regardless of whether or not there truly is a relationship between the features and the response, least squares will yield a set of coefficient estimates that result in a perfect fit to the data, such that the residuals are zero.</p><ul><li>This perfect fit will almost certainly lead to overfitting of the data</li><li>The problem is simple: when p &gt; n or p ≈ n, a simple least squares regression line is too <strong><em>flexible</em></strong> and hence overfits the data.</li></ul><ol start="2" type="1"><li>Examines only the R2 or the training set MSE might erroneously conclude that the model with the greatest number of variables is best. <img src="./12 2.png" width="650"></li></ol><ul><li><strong>Cp, AIC, and BIC</strong> approaches are not appropriate in the high-dimensional setting, because estimating <span class="math inline">\(\hat{σ}^2\)</span> is problematic.(For instance, the formula for <span class="math inline">\(\hat{σ}^2\)</span> from Chapter 3 yields an estimate <span class="math inline">\(\hat{σ}^2\)</span> = 0 in this setting.)</li><li><strong>Adjusted <span class="math inline">\(R^2\)</span> </strong> in the high-dimensional setting is problematic, since one can easily obtain a model with an adjusted <span class="math inline">\(R^2\)</span> value of 1.</li></ul><h2 id="regression-in-high-dimensions">Regression in High Dimensions</h2><p><strong>Alternative approaches better-suited to the high-dimensional setting:</strong></p><ul><li>Forward stepwise selection</li><li>Ridge regression</li><li>The lasso</li><li>Principal components regression</li></ul><p><strong>Reason:</strong> These approaches avoid overfitting by using a less flexible fitting approach than least squares.</p><p><strong>Three important points:</strong> (1) Regularization or shrinkage plays a key role in high-dimensional problems,</p><ol start="2" type="1"><li><p>Appropriate tuning parameter selection is crucial for good predictive performance, and</p></li><li><p>The test error tends to increase as the dimensionality of the problem (i.e. the number of features or predictors) increases, unless the additional features are truly associated with the response.<span class="math inline">\(\Rightarrow\)</span> <strong>curse of dimensionality</strong></p></li></ol><h3 id="curse-of-dimensionality">Curse of dimensionality</h3><p>Adding additional signal features that are truly associated with the response will improve the fitted model; However, adding noise features that are not truly associated with the response will lead to a deterioration in the fitted model.</p><p><strong>Reason</strong>: This is because noise features increase the dimensionality of the problem, exacerbating the risk of overfitting (since noise features may be assigned nonzero coefficients due to chance associations with the response on the training set) without any potential upside in terms of improved test set error.</p><h2 id="interpreting-results-in-high-dimensions">Interpreting Results in High Dimensions</h2><ol type="1"><li>Be cautious in reporting the results obtained when we perform the lasso, ridge regression, or other regression procedures in the high-dimensional setting.</li></ol><ul><li>In the high-dimensional setting, the <strong>multicollinearity</strong> problem is extreme: any variable in the model can be written as a linear combination of all of the other variables in the model. This means that we can never know exactly which variables (if any) truly are predictive of the outcome, and we can never identify the best coefficients for use in the regression.</li></ul><ol start="2" type="1"><li>Be cautious in reporting errors and measures of model fit in the high-dimensional setting</li></ol><ul><li>e.g.: when p &gt; n, it is easy to obtain a useless model that has zero residuals.</li><li><strong>One should never use sum of squared errors, p-values, R2 statistics, or other traditional measures of model fit on the training data as evidence of a good model fit in the high-dimensional setting</strong></li><li>It is important to instead report results on an independent test set, or cross-validation errors. For instance, the MSE or R2 on an independent test set is a valid measure of model fit, but the MSE on the training set certainly is not.</li></ul><hr><p><strong>Ref:</strong></p><p>James, Gareth, et al. <em>An introduction to statistical learning</em>. Vol. 112. New York: springer, 2013.</p><p>Hastie, Trevor, et al. &quot;The elements of statistical learning: data mining, inference and prediction.&quot; <em>The Mathematical Intelligencer</em> 27.2 (2005): 83-85</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;introduction-to-model-selection&quot;&gt;Introduction to Model Selection&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Setting:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In the regression setting, the standard linear model &lt;span class=&quot;math inline&quot;&gt;\(Y = β_0 + β_1X_1 + · · · + β_pX_p + \epsilon\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the chapters that follow, we consider some approaches for extending the linear model framework.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Reason of using other fitting procedure than lease squares&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;em&gt;Prediction Accuracy:&lt;/em&gt;&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Provided that the true relationship between the response and the predictors is approximately linear, the least squares estimates will have low bias.&lt;/li&gt;
&lt;li&gt;If n &lt;span class=&quot;math inline&quot;&gt;\(\gg\)&lt;/span&gt; p, least squares estimates tend to also have low variance &lt;span class=&quot;math inline&quot;&gt;\(\Rightarrow\)&lt;/span&gt; perform well on test data.&lt;/li&gt;
&lt;li&gt;If n is not much larger than p, least squares fit has large variance &lt;span class=&quot;math inline&quot;&gt;\(\Rightarrow\)&lt;/span&gt; overfitting &lt;span class=&quot;math inline&quot;&gt;\(\Rightarrow\)&lt;/span&gt; consequently poor predictions on test data&lt;/li&gt;
&lt;li&gt;If p &amp;gt; n, no more unique least squares coefficient estimate: the &lt;strong&gt;variance is infinite&lt;/strong&gt; so the method cannot be used at all&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By &lt;strong&gt;constraining&lt;/strong&gt; or &lt;strong&gt;shrinking&lt;/strong&gt; the estimated coefficients, we can often substantially reduce the variance at the cost of a negligible increase in bias.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;em&gt;Model Interpretability&lt;/em&gt;&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;irrelevant variables leads to unnecessary complexity in the resulting model. By removing these variables—that is, by setting the corresponding coefficient estimates to zero—we can obtain a model that is more easily interpreted.&lt;/li&gt;
&lt;li&gt;least squares is extremely unlikely to yield any coefficient estimates that are exactly zero &lt;span class=&quot;math inline&quot;&gt;\(\Rightarrow\)&lt;/span&gt; &lt;strong&gt;feature selection&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Alternatives of lease squares:&lt;/strong&gt;&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;Subset Selection&lt;/li&gt;
&lt;li&gt;Shrinkage&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://nancyyanyu.github.io/posts/cac93a23/&quot;&gt;Dimension Reduction&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Model Selection" scheme="https://nancyyanyu.github.io/tags/Model-Selection/"/>
    
  </entry>
  
  <entry>
    <title>Study Note: Resampling Methods - Cross Validation, Bootstrap</title>
    <link href="https://nancyyanyu.github.io/posts/6d11b2f4/"/>
    <id>https://nancyyanyu.github.io/posts/6d11b2f4/</id>
    <published>2019-10-19T23:01:12.678Z</published>
    <updated>2019-10-19T23:21:29.596Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Resampling methods</strong>:involve repeatedly drawing samples from a training set and refitting a mode of interest on each sample in order to obtain additional information about the fitted model.</p><p><strong>model assessment</strong>： The process of evaluating a model’s performance</p><p><strong>model selection</strong>：The process of selecting the proper level of flexibility for a model</p><p><strong>cross-validation</strong>: can be used to estimate the test error associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility.</p><p><strong>bootstrap</strong>:provide a measure of accuracy of a parameter estimate or of a given selection statistical learning method.</p><a id="more"></a><h1 id="cross-validation">Cross Validation</h1><h2 id="the-validation-set-approach">The Validation Set Approach</h2><p><strong>The Validation Set Approach</strong>:</p><ol type="1"><li><p>Randomly dividing the available set of observations into two parts, a <strong>training set</strong> and a <strong>validation set</strong> or hold-out set.</p></li><li><p>The model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set.</p></li><li><p>The resulting validation set error rate—typically assessed using MSE in the case of a quantitative response—provides an estimate of the test error rate.</p></li></ol><p><strong>Disadvantage</strong>:</p><ol type="1"><li><p>The validation estimate of the test error rate can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set.</p></li><li><p>In the validation approach, only a subset of the observations—those that are included in the training set rather than in the validation set—are used to fit the model. Since statistical methods tend to perform worse when trained on <em>fewer observations</em>, this suggests that the validation set error rate may tend to overestimate the test error rate for the model fit on the entire data set.</p></li></ol><h2 id="k-fold-cross-validation">K-Fold Cross-Validation</h2><p><strong>Approach</strong>:</p><ol type="1"><li><p>Randomly k-fold CV dividing the set of observations into k groups, or folds, of approximately equal size.</p></li><li><p>The first fold is treated as a validation set, and the method is fit on the remaining k − 1 folds.</p></li><li><p>The mean squared error, MSE1, is then computed on the observations in the held-out fold. This procedure is repeated k times; each time, a different group of observations is treated as a validation set.</p></li><li><p>This process results in k estimates of the test error, MSE1,MSE2, . . . ,MSEk.</p></li><li><p>The k-fold CV estimate is computed by averaging these values,</p></li></ol><p><span class="math display">\[\begin{align}CV_{(k)}=\frac{1}{k}\sum_{i=1}^kMSE_i\end{align}\]</span></p><p><strong>Goal</strong>：</p><ol type="1"><li><p>Determine how well a given statistical learning procedure can be expected to perform on independent data</p></li><li><p>We are interested only in the location of the minimum point in the estimated test MSE curve. This is because we might be performing cross-validation on a number of statistical learning methods, or on a single method using different levels of flexibility, in order to identify the method that results in the lowest test error.</p></li></ol><h2 id="bias-variance-trade-off-for-k-fold-cross-validation">Bias-Variance Trade-Off for k-Fold Cross-Validation</h2><p><strong>Leave-One-Out Cross-Validation V.S. k-Fold Cross-Validation</strong>: - k-Fold more biased than LOOCV - LOOCV will give approximately unbiased estimates of the test error, since each training set contains n − 1 observations, which is almost as many as the number of observations in the full data set. - k-fold CV for, say, k = 5 or k = 10 will lead to an intermediate level of bias</p><ul><li>k-Fold less variance than LOOCV</li><li>When we perform LOOCV, we are in effect averaging the outputs of n fitted models, each of which is trained on an almost identical set of observations; therefore, these outputs are highly (positively) correlated with each other.</li><li>the mean of many highly correlated quantities has higher variance than does the mean of many quantities that are not as highly correlated</li></ul><h1 id="bootstrap">Bootstrap</h1><p><strong>Approach</strong>:</p><ol type="1"><li>A data set, which we call Z, that contains n observations. We randomly select n observations from the data set in order to produce a bootstrap data set, <span class="math inline">\(Z^{∗1}\)</span>.</li><li>The sampling is performed with <strong>replacement</strong>, which means that the replacement same observation can occur more than once in the bootstrap data set.</li></ol><ul><li>In this example, <span class="math inline">\(Z^{∗1}\)</span> contains the third observation twice, the first observation once, and no instances of the second observation.</li><li>Note that if an observation is contained in <span class="math inline">\(Z^{∗1}\)</span>, then both its X and Y values are included.</li></ul><ol start="3" type="1"><li><p>We can use <span class="math inline">\(Z^{∗1}\)</span> to produce a new bootstrap estimate for α, which we call <span class="math inline">\(\alpha^{∗1}\)</span>. This procedure is repeated B times for some large value of B, in order to produce B different bootstrap data sets, <span class="math inline">\(Z^{∗1}\)</span>,<span class="math inline">\(Z^{∗2}\)</span>, . . . , <span class="math inline">\(Z^{∗B}\)</span>, and B corresponding α estimates, <span class="math inline">\(\alpha^{∗1}\)</span>, <span class="math inline">\(\alpha^{∗2}\)</span>, . . . , <span class="math inline">\(\alpha^{∗B}\)</span>.</p></li><li><p>We can compute the standard error of these bootstrap estimates using the formula <span class="math display">\[\begin{align}SE_B(\hat{\alpha})=\sqrt{\frac{1}{B-1}\sum_{i=1}^B\left( \hat{\alpha}^{*i}-\frac{1}{B}\sum^{B}_{j=1}\hat{\alpha}^{*j} \right)}\end{align}\]</span> This serves as an estimate of the standard error of <span class="math inline">\(\hat{\alpha}\)</span> estimated from the original data set.</p><p>This serves as an estimate of the standard error of <span class="math inline">\(\hat{\alpha}\)</span> estimated from the original data set.</p></li></ol><p><img src="./1.png" width="600"></p><hr><p><strong>Ref:</strong></p><p>James, Gareth, et al. <em>An introduction to statistical learning</em>. Vol. 112. New York: springer, 2013.</p><p>Hastie, Trevor, et al. &quot;The elements of statistical learning: data mining, inference and prediction.&quot; <em>The Mathematical Intelligencer</em> 27.2 (2005): 83-85</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Resampling methods&lt;/strong&gt;:involve repeatedly drawing samples from a training set and refitting a mode of interest on each sample in order to obtain additional information about the fitted model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;model assessment&lt;/strong&gt;： The process of evaluating a model’s performance&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;model selection&lt;/strong&gt;：The process of selecting the proper level of flexibility for a model&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;cross-validation&lt;/strong&gt;: can be used to estimate the test error associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;bootstrap&lt;/strong&gt;:provide a measure of accuracy of a parameter estimate or of a given selection statistical learning method.&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Model Selection" scheme="https://nancyyanyu.github.io/tags/Model-Selection/"/>
    
      <category term="Cross Validation" scheme="https://nancyyanyu.github.io/tags/Cross-Validation/"/>
    
  </entry>
  
  <entry>
    <title>Study Note: Comparing Logistic Regression, LDA, QDA, and KNN</title>
    <link href="https://nancyyanyu.github.io/posts/6084c2b2/"/>
    <id>https://nancyyanyu.github.io/posts/6084c2b2/</id>
    <published>2019-10-19T23:00:54.556Z</published>
    <updated>2019-10-19T23:20:44.353Z</updated>
    
    <content type="html"><![CDATA[<h2 id="logistic-regression-and-lda-methods-are-closely-connected.">Logistic regression and LDA methods are closely connected.</h2><p><strong>Setting</strong>: Consider the two-class setting with \(p = 1\) predictor, and let \(p_1(x)\) and \(p_2(x) = 1−p_1(x)\) be the probabilities that the observation \(X = x\) belongs to class 1 and class 2, respectively.</p><p>In LDA, from</p><p><span class="math display">\[\begin{align} p_k(x)=\frac{\pi_k \frac{1}{\sqrt{2\pi}\sigma}\exp{\left( -\frac{1}{2\sigma^2}(x-\mu_k)^2 \right)}}{\sum_{l=1}^K\pi\_l\frac{1}{\sqrt{2\pi}\sigma}\exp{\left( -\frac{1}{2\sigma^2}(x-\mu_l)^2 \right)}} \end{align}\]</span></p><p><span class="math display">\[\begin{align} \delta\_k(x)=x\frac{\\mu\_k}{\sigma^2}-\frac{\mu_k^2}{2\sigma^2}+\log(\pi_k) \end{align}\]</span> The <strong>log odds</strong> is given by</p><p><span class="math display">\[\begin{align}\log{\frac{p_1(x)}{1-p_1(x)}}=\log{\frac{p_1(x)}{p_2(x)}}=c_0+c_1x \end{align}\]</span> where c0 and c1 are functions of μ1, μ2, and σ2.</p><p>In Logistic Regression,</p><p><span class="math display">\[\begin{align} \log{\frac{p_1}{1-p_1}}=\beta\_0+\beta_1x \end{align}\]</span> <a id="more"></a></p><p><strong>SAME</strong></p><ul><li><strong>Both logistic regression and LDA produce linear decision boundaries.</strong></li></ul><p><strong>DIFFERENCES</strong></p><ul><li><p>The only difference between the two approaches lies in the fact that β0 and β1 are estimated using maximum likelihood, whereas c0 and c1 are computed using the estimated mean and variance from a normal distribution. This same connection between LDA and logistic regression also holds for multidimensional data with p &gt; 1.</p></li><li><p>LDA assumes that the observations are drawn from a Gaussian distribution with a common covariance matrix in each class, and so can provide some improvements over logistic regression when this assumption approximately holds. Conversely, logistic regression can outperform LDA if these Gaussian assumptions are not met.</p></li></ul><h2 id="knn-dominate-lda-and-logistic-in-non-linear-setting">KNN dominate LDA and Logistic in non-linear setting</h2><p>In order to make a prediction for an observation X = x, the K training observations that are closest to x are identified. Then X is assigned to the class to which the plurality of these observations belong. Hence KNN is a completely <strong>non-parametric</strong> approach: <em>no assumptions are made about the shape of the decision boundary</em>.</p><blockquote><p>Therefore, we can expect KNN to dominate LDA and logistic regression when the decision boundary is highly non-linear.</p></blockquote><p>On the other hand, KNN does not tell us which predictors are important</p><h2 id="qda-serves-as-a-compromise-between-knn-lda-and-logistic-regression">QDA serves as a compromise between KNN, LDA and logistic regression</h2><p>QDA serves as a compromise between the non-parametric KNN method and the linear LDA and logistic regression approaches. Since QDA assumes a quadratic decision boundary, it can accurately model a wider range of problems than can the linear methods. Though not as flexible as KNN, QDA can perform better in the presence of a <em>limited number of training observations</em> because it does make some assumptions about the form of the decision boundary.</p><p><img src="./images/17.png"></p><p><strong>Scenario 1</strong>: - 20 training observations in each of two classes. The observations within each class were uncorrelated random normal variables with a different mean in each class. - LDA performed well in this setting. KNN performed poorly because it paid a price in terms of variance that was not offset by a reduction in bias.</p><p><strong>Scenario 2</strong>: - Details are as in Scenario 1, except that within each class, the two predictors had a correlation of −0.5. - Little change in the relative performances of the methods as compared to the previous scenario.</p><p><strong>Scenario 3</strong>: - X1 and X2 are from the t-distribution, with 50 observations per class.</p><blockquote><p>The <strong>t-distribution</strong> has a similar shape to the normal distribution, but it has a tendency to yield more extreme points—that is, more points that are far from the mean.</p></blockquote><ul><li>The decision boundary was still linear, and so fit into the logistic regression framework. The set-up violated the assumptions of LDA, since the observations were not drawn from a normal distribution. QDA results deteriorated considerably as a consequence of non-normality.</li></ul><p><strong>Scenario 4</strong>: - The data were generated from a normal distribution, with a correlation of 0.5 between the predictors in the first class, and correlation of −0.5 between the predictors in the second class. - This setup corresponded to the QDA assumption, and resulted in quadratic decision boundaries.</p><p><strong>Scenario 5</strong>: - Within each class, the observations were generated from a normal distribution with uncorrelated predictors. However, the responses were sampled from the logistic function using \(X^2_1 , X^2_2, and \, X1 × X2\) as predictors. - Consequently, there is a quadratic decision boundary. QDA once again performed best, followed closely by KNN-CV. The linear methods had poor performance.</p><p><strong>Scenario 6</strong>: - Details are as in the previous scenario, but the responses were sampled from a more complicated non-linear function. - Even the quadratic decision boundaries of QDA could not adequately model the data. - Much more flexible KNN-CV method gave the best results. But KNN with K = 1 gave the worst results out of all methods.</p><blockquote><p>This highlights the fact that <strong>even when the data exhibits a complex nonlinear relationship, a non-parametric method such as KNN can still give poor results if the level of smoothness is not chosen correctly.</strong></p></blockquote><h2 id="conclusion">Conclusion</h2><ul><li><p>When the true decision boundaries are linear, then the LDA and logistic regression approaches will tend to perform well.</p></li><li><p>When the boundaries are moderately non-linear, QDA may give better results.</p></li><li><p>For much more complicated decision boundaries, a non-parametric approach such as KNN can be superior. But the level of smoothness for a non-parametric approach must be chosen carefully.</p></li></ul><hr><p><strong>Ref:</strong></p><p>James, Gareth, et al. <em>An introduction to statistical learning</em>. Vol. 112. New York: springer, 2013.</p><p>Hastie, Trevor, et al. &quot;The elements of statistical learning: data mining, inference and prediction.&quot; <em>The Mathematical Intelligencer</em> 27.2 (2005): 83-85</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;logistic-regression-and-lda-methods-are-closely-connected.&quot;&gt;Logistic regression and LDA methods are closely connected.&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Setting&lt;/strong&gt;: Consider the two-class setting with \(p = 1\) predictor, and let \(p_1(x)\) and \(p_2(x) = 1−p_1(x)\) be the probabilities that the observation \(X = x\) belongs to class 1 and class 2, respectively.&lt;/p&gt;
&lt;p&gt;In LDA, from&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
\begin{align} p_k(x)=\frac{\pi_k \frac{1}{\sqrt{2\pi}\sigma}\exp{\left( -\frac{1}{2\sigma^2}(x-\mu_k)^2 \right)}}{\sum_{l=1}^K\pi\_l\frac{1}{\sqrt{2\pi}\sigma}\exp{\left( -\frac{1}{2\sigma^2}(x-\mu_l)^2 \right)}} \end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
\begin{align} \delta\_k(x)=x\frac{\\mu\_k}{\sigma^2}-\frac{\mu_k^2}{2\sigma^2}+\log(\pi_k) \end{align}
\]&lt;/span&gt; The &lt;strong&gt;log odds&lt;/strong&gt; is given by&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
\begin{align}\log{\frac{p_1(x)}{1-p_1(x)}}=\log{\frac{p_1(x)}{p_2(x)}}=c_0+c_1x \end{align}
\]&lt;/span&gt; where c0 and c1 are functions of μ1, μ2, and σ2.&lt;/p&gt;
&lt;p&gt;In Logistic Regression,&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
\begin{align} \log{\frac{p_1}{1-p_1}}=\beta\_0+\beta_1x \end{align}
\]&lt;/span&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Study Note: Linear Discriminant Analysis, ROC &amp; AUC, Confusion Matrix</title>
    <link href="https://nancyyanyu.github.io/posts/5b711fed/"/>
    <id>https://nancyyanyu.github.io/posts/5b711fed/</id>
    <published>2019-10-19T22:59:57.484Z</published>
    <updated>2019-10-19T23:21:05.413Z</updated>
    
    <content type="html"><![CDATA[<p><strong>LDA V.S. Logistic Regression</strong>:</p><ol type="1"><li><p>When the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem.</p></li><li><p>If n is small and the distribution of the predictors X is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.</p></li><li><p>Linear discriminant analysis is popular when we have more than two response classes.</p></li></ol><h1 id="using-bayes-theorem-for-classification">Using Bayes’ Theorem for Classification</h1><p>Suppose that we wish to classify an observation into one of K classes, where K ≥ 2.</p><a id="more"></a><p><strong>Prior</strong>:Let <span class="math inline">\(\pi_k=Pr(Y=k)\)</span> represent the overall or <strong><em>prior</em></strong> probability that a randomly chosen observation comes from the kth class. This is the probability that a given observation is associated with the kth category of the response variable Y .</p><p>Let <span class="math inline">\(f_k(X) ≡ Pr(X = x|Y = k)\)</span> denote the <strong><em>density function</em></strong> of X for an observation that comes from the kth class. In other words, fk(x) is relatively large if there is a high probability that an observation in the kth class has X ≈ x.</p><p><strong>Bayes’ theorem</strong> states that <span class="math display">\[\begin{align}Pr(Y=k|X=x)=\frac{\pi_k f_k(x)}{\sum_{l=1}^K\pi_lf_l(x)} \end{align}\]</span> <strong>Posterior</strong>:<span class="math inline">\(p_k(X) = Pr(Y = k|X)\)</span> an observation X = x belongs to the kth class, given the predictor value for that observation</p><p><strong>Estimating <span class="math inline">\(π_k\)</span>:</strong> simply compute the fraction of the training observations that belong to the kth class.</p><p><strong>Estimating <span class="math inline">\(f_k(X)\)</span>:</strong> more challenging</p><h1 id="linear-discriminant-analysis-for-p-1">Linear Discriminant Analysis for p = 1</h1><p>Assume p = 1—that is, we have only one predictor. We would like to obtain an estimate for <span class="math inline">\(f_k(x)\)</span> that we can estimate <span class="math inline">\(p_k(x)\)</span>. We will then classify an observation to the class for which <span class="math inline">\(p_k(x)\)</span> is greatest.</p><h2 id="assumptions">Assumptions</h2><p>In order to estimate <span class="math inline">\(f_k(x)\)</span>, we will first make some assumptions about its form:</p><ol type="1"><li>Assume that <span class="math inline">\(f_k(x)\)</span> is normal or Gaussian. <span class="math display">\[  \begin{align}  f_k(x)=\frac{1}{\sqrt{2\pi}\sigma_k}\exp{\left( -\frac{1}{2\sigma_k^2}(x-\mu_k)^2 \right)}  \end{align}  \]</span></li></ol><p>where <span class="math inline">\(μ_k\)</span> and <span class="math inline">\(σ_k^2\)</span> are the mean and variance parameters for the kth class.</p><ol start="2" type="1"><li><dl><dt>Assume that <span class="math inline">\(\sigma_1^2=...=\sigma_k^2\)</span></dt><dd>that is, there is a shared variance term across all K classes, which for simplicity we can denote by <span class="math inline">\(\sigma^2\)</span>.</dd></dl></li></ol><p>So <span class="math display">\[\begin{align}p_k(x)=\frac{\pi_k \frac{1}{\sqrt{2\pi}\sigma}\exp{\left( -\frac{1}{2\sigma^2}(x-\mu_k)^2 \right)}}{\sum_{l=1}^K\pi_l\frac{1}{\sqrt{2\pi}\sigma}\exp{\left( -\frac{1}{2\sigma^2}(x-\mu_l)^2 \right)}}\end{align}\]</span> The <strong>Bayes classifier</strong> involves assigning an observation X = x to the class for which <span class="math inline">\(p_k(x)\)</span> is largest. Taking the log of <span class="math inline">\(p_k(x)\)</span> and rearranging the terms, it is not hard to show that this is equivalent to assigning the observation to the class for which <span class="math display">\[\begin{align}\delta_k(x)=x\frac{\mu_k}{\sigma^2}-\frac{\mu_k^2}{2\sigma^2}+\log(\pi_k) \quad\quad (4.13)\end{align}\]</span> is largest.</p><p>For instance, if K = 2 and π1 = π2, then the Bayes classifier assigns an observation to class 1 if <span class="math inline">\(2x (μ_1 − μ_2) &gt; μ^2_1 − μ^2_2\)</span>, and to class 2 otherwise. In this case, the Bayes decision boundary corresponds to the point where <span class="math display">\[\begin{align}x=\frac{\mu_1^2-\mu_2^2}{2(\mu_1-\mu_2)}=\frac{\mu_1+\mu_2}{2}\end{align}\]</span></p><h2 id="parameters-estimation">Parameters Estimation</h2><p>In practice, even if we are quite certain of our assumption that X is drawn from a Gaussian distribution within each class, we still have to estimate the parameters <span class="math inline">\(μ_1, . . . , μ_K, π_1, . . . , π_K\)</span>, and <span class="math inline">\(σ^2\)</span>.</p><p><strong>Linear discriminant analysis (LDA)</strong> method approximates the Bayes classifier by plugging estimates for <span class="math inline">\(μ_1, . . . , μ_K, π_1, . . . , π_K\)</span>, and <span class="math inline">\(σ^2\)</span> into (4.13)</p><p><span class="math display">\[\begin{align}\hat{\mu}_k=\frac{1}{n_k}\sum_{i:y_i=k}x_i  \quad (4.15) \\\hat{\sigma}^2=\frac{1}{n-K}\sum_{k=1}^K\sum_{i:y_i=k}(x_i-\hat{\mu_k})^2 \quad (4.16)\\\hat{\pi_k}=\frac{n_k}{n}\end{align}\]</span></p><p>where n is the total number of training observations, and <span class="math inline">\(n_k\)</span> is the number of training observations in the kth class.</p><p><span class="math inline">\(\hat{\mu}_k\)</span>: average of all the training observations from the kth class;</p><p><span class="math inline">\(\hat{\sigma}^2\)</span>: a weighted average of the sample variances for each of the K classes.</p><p><span class="math inline">\(\hat{\pi_k}\)</span>: the proportion of the training observations that belong to the kth class</p><h2 id="lda-classifier">LDA classifier</h2><p>The LDA classifier assigns an observation X = x to the class for which</p><p><span class="math display">\[\begin{align}\hat{\delta}_k(x)=x\frac{\hat{\mu}_k}{\hat{\sigma}^2}-\frac{\hat{\mu}_k^2}{2\hat{\sigma}^2}+\log(\hat{\pi}_k)\end{align}\]</span> is largest.</p><p>The word <strong><em>linear</em></strong> in the classifier’s name stems from the fact that the <strong><em>discriminant functions</em></strong> <span class="math inline">\(\hat{\delta}_k(x)\)</span> are linear functions of x.</p><p><img src="./6.png" width="600"></p><p>The right-hand panel of Figure 4.4 displays a histogram of a random sample of 20 observations from each class.</p><p>To implement LDA,</p><ol type="1"><li>Estimating πk, μk, and σ2 using (4.15) and (4.16).</li><li>Compute the decision boundary, shown as a black solid line, that results from assigning an observation to the class for which <span class="math inline">\(\hat{\delta}_k(x)\)</span> is largest.</li></ol><p>In this case, since n1 = n2 = 20, we have <span class="math inline">\(\hat{\pi_1}\)</span> = <span class="math inline">\(\hat{\pi_2}\)</span>. As a result, the decision boundary corresponds to the midpoint between the sample means for the two classes,<span class="math inline">\(\frac{\mu_1+\mu_2}{2}\)</span></p><h1 id="linear-discriminant-analysis-for-p-1-1">Linear Discriminant Analysis for p &gt;1</h1><p>Assume that X = (X1,X2, . . .,Xp) is drawn from a <strong>multivariate Gaussian</strong> (or multivariate normal) distribution, with a class-specific mean vector and a common covariance matrix.</p><h2 id="multivariate-gaussian-distribution">Multivariate Gaussian Distribution</h2><p>Assumes that each individual predictor follows a one-dimensional normal distribution with some correlation between each pair of predictors.</p><p><img src="./7.png" width="600"></p><p>To indicate that a p-dimensional random variable X has a multivariate Gaussian distribution, we write X ∼ N(μ,Σ). Here E(X) = μ is the mean of X (a vector with p components), and Cov(X) = Σ is the p × p <strong>covariance matrix</strong> of X. Formally, the <strong>multivariate Gaussian density</strong> is defined as <span class="math display">\[\begin{align}f(x)=\frac{1}{\sqrt{(2\pi)^{p}|Σ|}}\exp{\left( \frac{1}{2}(x-\mu)^TΣ^{-1}(x-\mu) \right)}\end{align}\]</span> In the case of p &gt; 1 predictors, the <strong>LDA classifier</strong> assumes that the observations in the kth class are drawn from a multivariate Gaussian distribution <span class="math inline">\(N(μ_k,Σ)\)</span>, where <span class="math inline">\(μ_k\)</span> is a class-specific mean vector, and Σ is a covariance matrix that is common to all K classes.</p><p>Plugging the density function for the kth class, <span class="math inline">\(f_k(X = x)\)</span>, into <span class="math inline">\(Pr(Y = k|X = x)\)</span>, the Bayes classifier assigns an observation X = x to the class for which <span class="math display">\[\begin{align}\delta_k(x)=x^TΣ^{-1}\mu_k-\frac{1}{2}\mu_k^TΣ^{-1}\mu_k+\log{\pi_k}  \quad \quad (4.19)\end{align}\]</span> is largest.</p><p><img src="./8.png" width="600"></p><p>Once again, we need to estimate the unknown parameters <span class="math inline">\(μ_1, . . . , μ_K\)</span>, <span class="math inline">\(π_1, . . . , π_K\)</span>, and Σ; the formulas are similar to those used in the one dimensional case, given in (4.15). To assign a new observation X = x, <strong>LDA</strong> plugs these estimates into (4.19) and classifies to the class for which <span class="math inline">\(\hat{\delta}_k(x)\)</span> is largest.</p><blockquote><p>Overall, the LDA decision boundaries are pretty close to the Bayes decision boundaries, shown again as dashed lines.</p></blockquote><h2 id="caveats">Caveats</h2><ol type="1"><li><p>Training error rates will usually be lower than test error rates. The higher the ratio of parameters p to number of samples n, the more we expect this overfitting to play a role.</p></li><li><p>Second, since only 3.33% of the individuals in the training sample defaulted, a simple but useless classifier that always predicts that each individual will not default, regardless of his or her credit card balance and student status, will result in an error rate of 3.33%. In other words, the trivial <strong>null classifier</strong> will achieve an error rate that is only a bit higher than the LDA training set error rate.</p></li></ol><h3 id="two-types-of-error-confusion-matrix">Two Types of Error, Confusion Matrix</h3><p>Binary classifier can make two types of errors:</p><ol type="1"><li>it can incorrectly assign an individual who defaults to the no default category;</li><li>it can incorrectly assign an individual who does not default to the default category.</li></ol><p><strong>Confusion Matrix</strong></p><p>*注意这张图不是标准的confusion matrix，看下面那张 <img src="./10.png" width="600"></p><p><strong>Explanation</strong>： The matrix table reveals that LDA predicted that a total of 104 people would default. Of these people, 81 actually defaulted and 23 did not.</p><p><strong>Type I Error</strong>： Of the 333 individuals who defaulted, 252 (or 75.7%) were missed by LDA. So while the overall error rate is low, the error rate among individuals who defaulted is very high. From the perspective of a credit card company that is trying to identify high-risk individuals, an error rate of 252/333 = 75.7% among individuals who default may well be unacceptable.</p><p><strong>Type II Error</strong>：Only 23 out of 9, 667 of the individuals who did not default were incorrectly labeled. This looks like a pretty low error rate!</p><p><img src="./11.png" width="1000"></p><p><strong>Sensitivity</strong>:the percentage of true defaulters that are identified, a low 24.3% in this case.</p><p><strong>Specificity</strong>:the percentage of non-defaulters that are correctly identified, here (1 − 23/9, 667)× 100 = 99.8%.</p><h3 id="why-does-lda-do-such-a-poor-job-of-classifying-the-customers-who-default">Why does LDA do such a poor job of classifying the customers who default?</h3><blockquote><p>In other words, why does it have such a low sensitivity?</p></blockquote><p>LDA is trying to approximate the Bayes classifier, which has the lowest total error rate out of all classifiers (if the Gaussian model is correct). That is, the Bayes classifier will yield the smallest possible total number of misclassified observations, irrespective of which class the errors come from.</p><p>The Bayes classifier works by assigning an observation to the class for which the posterior probability pk(X) is greatest. In the two-class case, this amounts to assigning an observation to the default class if <span class="math inline">\(Pr(default = Yes|X = x) &gt; 0.5.\)</span></p><p>Thus, the Bayes classifier, and by extension LDA, uses a threshold of 50% for the posterior probability of default in order to assign an observation to the default class.</p><p><strong>Modify LDA</strong></p><p>If we are concerned about incorrectly predicting the default status for individuals who default, then we can consider lowering this threshold.</p><p><span class="math display">\[P(default = Yes|X = x) &gt; 0.2\]</span></p><p>Figure 4.7 illustrates the trade-off that results from modifying the threshold value for the posterior probability of default</p><p><img src="./12.png" width="700"></p><p>How can we decide which threshold value is best? Such a decision must be based on <strong>domain knowledge</strong>, such as detailed information about the costs associated with default.</p><h3 id="roc-auc">ROC &amp; AUC</h3><p><strong>ROC</strong>:The ROC curve is a popular graphic for simultaneously displaying the two types of errors for all possible thresholds.</p><p><strong>AUC</strong>: The overall performance of a classifier, summarized over all possible thresholds, is given by the area under the (ROC) curve (AUC).</p><ul><li><p>An ideal ROC curve will hug the top left corner, so the larger the AUC the better the classifier. We expect a classifier that performs no better than chance to have an AUC of 0.5</p></li><li><p>ROC curves are useful for comparing different classifiers, since they take into account all possible thresholds.</p></li></ul><p><img src="./13.png" width="700"></p><p><img src="./14.png" width="700"></p><p><img src="./15.png" width="700"></p><h1 id="quadratic-discriminant-analysis">Quadratic Discriminant Analysis</h1><p><strong>Quadratic discriminant analysis (QDA)</strong> classifier results from assuming that the observations from each class are drawn from a Gaussian distribution, and plugging estimates for the parameters into Bayes’ theorem in order to perform prediction.</p><p>However, unlike LDA, <strong>QDA assumes that each class has its own covariance matrix</strong>. That is, it assumes that an observation from the kth class is of the form <span class="math inline">\(X ∼ N(μ_k,Σ_k)\)</span>, where <span class="math inline">\(Σ_k\)</span> is a covariance matrix for the kth class. Under this assumption, the Bayes classifier assigns an observation <span class="math inline">\(X = x\)</span> to the class for which</p><p><span class="math display">\[\begin{align}\delta_k(x)&amp;=-\frac{1}{2}(x-\mu_k)^TΣ_k^{-1}(x-\mu_k)-\frac{1}{2}\log{|Σ_k|}+\log{\pi_k} \\&amp;=-\frac{1}{2}x^TΣ_k^{-1}x+x^TΣ_k^{-1}\mu_k-\frac{1}{2}\mu_k^TΣ_k^{-1}\mu_k-\frac{1}{2}\log{|Σ_k|}+\log{\pi_k}\end{align}\]</span></p><p>is largest.</p><p>So the QDA classifier involves plugging estimates for <span class="math inline">\(Σ_k, μ_k, π_k\)</span> into <span class="math inline">\(\delta_k(x)\)</span>, and then assigning an observation <span class="math inline">\(X = x\)</span> to the class for which this quantity is largest. Unlike LDA, the quantity <span class="math inline">\(x\)</span> appears as a quadratic function.</p><h2 id="why-does-it-matter-whether-or-not-we-assume-that-the-k-classes-share-a-common-covariance-matrix">Why does it matter whether or not we assume that the K classes share a common covariance matrix?</h2><p>The answer lies in the <strong>bias-variance trade-off</strong>: - When there are p predictors, then estimating a covariance matrix requires estimating p(p+1)/2 parameters. QDA estimates a separate covariance matrix for each class, for a total of Kp(p+1)/2 parameters. - Consequently, LDA is a much less flexible classifier than QDA, and so has substantially lower variance. - But there is a trade-off: if LDA’s assumption that the K classes share a common covariance matrix is badly off, then LDA can suffer from high bias.</p><p><strong>Conclusion</strong></p><ul><li>LDA tends to be a better bet than QDA if there are relatively few training observations and so reducing variance is crucial.</li><li>QDA is recommended if the training set is very large, so that the variance of the classifier is not a major concern, or if the assumption of a common covariance matrix for the K classes is clearly untenable</li></ul><p><img src="./16.png" width="700"></p><hr><p><strong>Ref:</strong></p><p>James, Gareth, et al. <em>An introduction to statistical learning</em>. Vol. 112. New York: springer, 2013.</p><p>Hastie, Trevor, et al. &quot;The elements of statistical learning: data mining, inference and prediction.&quot; <em>The Mathematical Intelligencer</em> 27.2 (2005): 83-85</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;LDA V.S. Logistic Regression&lt;/strong&gt;:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;When the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If n is small and the distribution of the predictors X is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Linear discriminant analysis is popular when we have more than two response classes.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&quot;using-bayes-theorem-for-classification&quot;&gt;Using Bayes’ Theorem for Classification&lt;/h1&gt;
&lt;p&gt;Suppose that we wish to classify an observation into one of K classes, where K ≥ 2.&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="LDA" scheme="https://nancyyanyu.github.io/tags/LDA/"/>
    
      <category term="Classification" scheme="https://nancyyanyu.github.io/tags/Classification/"/>
    
      <category term="Model Evaluation" scheme="https://nancyyanyu.github.io/tags/Model-Evaluation/"/>
    
  </entry>
  
  <entry>
    <title>Study Note: Logistic Regression</title>
    <link href="https://nancyyanyu.github.io/posts/b358d10f/"/>
    <id>https://nancyyanyu.github.io/posts/b358d10f/</id>
    <published>2019-10-19T22:59:42.997Z</published>
    <updated>2019-10-19T23:21:40.819Z</updated>
    
    <content type="html"><![CDATA[<h1 id="why-not-linear-regression">Why Not Linear Regression?</h1><p>The logistic regression model arises from the desire to model the posterior probabilities of the K classes via linear functions in x, while at the same time ensuring that they sum to one and remain in [0, 1]. The model has the form: <span class="math display">\[\begin{align}\log\frac{\text{Pr}(G=1|X=x)}{\text{Pr}(G=K|X=x)} &amp;= \beta_{10} + \beta_1^Tx \\\log\frac{\text{Pr}(G=2|X=x)}{\text{Pr}(G=K|X=x)} &amp;= \beta_{20} + \beta_2^Tx \\&amp;\vdots \\\log\frac{\text{Pr}(G=K-1|X=x)}{\text{Pr}(G=K|X=x)} &amp;= \beta_{(K-1)0} + \beta_{K-1}^Tx \\\end{align}\]</span></p><p>Linear regression is not appropriate in the case of a qualitative response.**</p><p><strong>Reason:</strong> there is no natural way to convert a qualitative response variable with more than two levels into a quantitative response that is ready for linear regression.</p><p><strong>Setting</strong>: For the Default data, logistic regression models the probability of default. For example, the probability of default given balance can be written as <span class="math inline">\(Pr(default = Yes|balance).\)</span></p><a id="more"></a><p><img src="./1.png" width="600"></p><h1 id="the-logistic-model">The Logistic Model</h1><p>Logistic regression involves directly modeling Pr(Y = k|X = x) using the logistic function for the case of two response classes</p><p><strong>Logistic function:</strong></p><p><span class="math display">\[\begin{align}p(X)=\frac{e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}} \\\frac{p(X)}{1-p(X)}=e^{\beta_0+\beta_1X}\end{align}\]</span> <strong>Odds</strong></p><p>The quantity p(X)/[1−p(X)] is called the <strong>odds</strong>, and can take on any value odds between 0 and ∞. Values</p><p><strong>Log-odds (Logit)</strong></p><p><span class="math display">\[\begin{align}\log{\frac{p(X)}{1-p(X)}}=\beta_0+\beta_1X\end{align}\]</span> We see that the logistic model (4.2) has a logit that is linear in X.</p><ul><li><p>There is not a straight-line relationship between p(X) and X,</p></li><li><p>The rate of change in p(X) per unit change in X depends on the current value of X,</p></li></ul><h1 id="estimating-the-regression-coefficients">Estimating the Regression Coefficients</h1><p>The basic intuition behind using <strong>maximum likelihood</strong> to fit a logistic regression model is as follows: - We seek estimates for <span class="math inline">\(β_0\)</span> and <span class="math inline">\(β_1\)</span> such that the predicted probability <span class="math inline">\(\hat{p}(x_i)\)</span> of class &quot;default&quot; for each individual, using (4.2), corresponds as closely as possible to the individual’s observed &quot;default&quot; status. In other words, we try to find <span class="math inline">\(\hat{β}_0\)</span> and <span class="math inline">\(\hat{β}_1\)</span> such that plugging these estimates into the model for <span class="math inline">\(p(X)\)</span>, given in (4.2), yields a number close to one for all individuals who &quot;defaulted&quot;, and a number close to zero for all individuals who did not.</p><p><strong>Likelihood function</strong>:</p><p><span class="math display">\[\begin{align}l(\beta_0,\beta_1)=\prod_{i:y_i=1}p(x_i) \prod_{i^{&#39;}:y_{i^{&#39;}}=0}(1-p(x_{i^{&#39;}}))\end{align}\]</span> The estimates <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span> are chosen to maximize this <strong><em>likelihood function.</em></strong></p><p>In the linear regression setting, the least squares approach is in fact a special case of maximum likelihood.</p><h1 id="making-predictions">Making Predictions</h1><p>Once the coefficients have been estimated, it is a simple matter to compute the probability of default for any given credit card balance.</p><p><img src="./2.png" width="600"></p><p>For example, using the coefficient estimates given in Table 4.1, we predict that the default probability for an individual with a balance of $1, 000 is <span class="math display">\[\begin{align}\hat{p}(X)=\frac{e^{\hat{\beta_0}+\hat{\beta_1}X}}{1+e^{\hat{\beta_0}+\hat{\beta_1}X}}=\frac{e^{−10.6513+0.0055×1,000}}{1+e^{−10.6513+0.0055×1,000}}=0.00576\end{align}\]</span></p><h1 id="multiple-logistic-regression">Multiple Logistic Regression</h1><p>We now consider the problem of predicting a binary response using multiple predictors</p><p><strong>Log-odds (Logit)</strong></p><p><span class="math display">\[\begin{align}\log{\frac{p(X)}{1-p(X)}}=\beta_0+\sum_{i=1}^p\beta_iX\end{align}\]</span> where X = (X1, . . .,Xp) are p predictors</p><p><strong>Logistic function:</strong></p><p><span class="math display">\[\begin{align}p(X)=\frac{e^{\beta_0+\sum_{i=1}^p\beta_iX}}{1+e^{\beta_0+\sum_{i=1}^p\beta_iX}} \\\frac{p(X)}{1-p(X)}=e^{\beta_0+\sum_{i=1}^p\beta_iX}\end{align}\]</span></p><h2 id="confounding">Confounding</h2><p>In single variable setting: <img src="./3.png" width="600"></p><p>In multiple variables setting: <img src="./4.png" width="600"></p><blockquote><p>How is it possible for student status to be associated with an increase in probability of default in Table 4.2 and a decrease in probability of default in Table 4.3?</p></blockquote><p><img src="./5.png" width="600"></p><ul><li>The positive coefficient for student in the single variable logistic regression : the overall student default rate is higher than the non-student default rate</li><li>The negative coefficient for student in the multiple logistic regression: for a fixed value of balance and income, a student is less likely to default than a non-student.</li></ul><p><strong>Reason</strong>:The variables <em>student</em> and <em>balance</em> are correlated.</p><p><strong>Intuition</strong>: A student is riskier than a non-student if no information about the student’s credit card balance is available. However, that student is less risky than a non-student with the same credit card balance!</p><hr><p><strong>Ref:</strong></p><p>James, Gareth, et al. <em>An introduction to statistical learning</em>. Vol. 112. New York: springer, 2013.</p><p>Hastie, Trevor, et al. &quot;The elements of statistical learning: data mining, inference and prediction.&quot; <em>The Mathematical Intelligencer</em> 27.2 (2005): 83-85</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;why-not-linear-regression&quot;&gt;Why Not Linear Regression?&lt;/h1&gt;
&lt;p&gt;The logistic regression model arises from the desire to model the posterior probabilities of the K classes via linear functions in x, while at the same time ensuring that they sum to one and remain in [0, 1]. The model has the form: &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
\log\frac{\text{Pr}(G=1|X=x)}{\text{Pr}(G=K|X=x)} &amp;amp;= \beta_{10} + \beta_1^Tx \\
\log\frac{\text{Pr}(G=2|X=x)}{\text{Pr}(G=K|X=x)} &amp;amp;= \beta_{20} + \beta_2^Tx \\
&amp;amp;\vdots \\
\log\frac{\text{Pr}(G=K-1|X=x)}{\text{Pr}(G=K|X=x)} &amp;amp;= \beta_{(K-1)0} + \beta_{K-1}^Tx \\
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Linear regression is not appropriate in the case of a qualitative response.**&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reason:&lt;/strong&gt; there is no natural way to convert a qualitative response variable with more than two levels into a quantitative response that is ready for linear regression.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Setting&lt;/strong&gt;: For the Default data, logistic regression models the probability of default. For example, the probability of default given balance can be written as &lt;span class=&quot;math inline&quot;&gt;\(Pr(default = Yes|balance).\)&lt;/span&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Classification" scheme="https://nancyyanyu.github.io/tags/Classification/"/>
    
      <category term="Logistic Regression" scheme="https://nancyyanyu.github.io/tags/Logistic-Regression/"/>
    
  </entry>
  
  <entry>
    <title>Study Note: Linear Regression Part II - Potential Problems</title>
    <link href="https://nancyyanyu.github.io/posts/4df00c7b/"/>
    <id>https://nancyyanyu.github.io/posts/4df00c7b/</id>
    <published>2019-10-19T22:56:41.781Z</published>
    <updated>2019-10-19T23:21:26.079Z</updated>
    
    <content type="html"><![CDATA[<h1 id="qualitative-predictors">Qualitative Predictors</h1><h2 id="predictors-with-only-two-levels">Predictors with Only Two Levels</h2><p>Suppose that we wish to investigate differences in credit card balance between males and females, ignoring the other variables for the moment. If a qualitative predictor (also known as a <strong>factor</strong>) only has two <strong>levels</strong>, or possible values, then incorporating it into a regression model is very simple. We simply create an indicator or <strong>dummy variable</strong> that takes on two possible numerical values.</p><p><img src="./8.png" width="300"> and use this variable as a predictor in the regression equation. This results in the model</p><p><img src="./9.png" width="600"></p><a id="more"></a><p>Now β0 can be interpreted as the average credit card balance among males, β0 + β1 as the average credit card balance among females</p><h2 id="qualitative-predictors-with-more-than-two-levels">Qualitative Predictors with More than Two Levels</h2><p>When a qualitative predictor has more than two levels, we can create additional dummy variables. For example, for the ethnicity variable we create two dummy variables. The first could be</p><p><img src="./10.png" width="300"> and the second could be <img src="./11.png" width="300"></p><p>Then both of these variables can be used in the regression equation, in order to obtain the model</p><p><img src="./12.png" width="600"></p><p><strong>Baseline</strong></p><ul><li>There will always be <strong>one fewer</strong> dummy variable than the number of levels. The level with no dummy variable—African American in this example—is known as the baseline.</li></ul><p><img src="./13.png" width="600"></p><p>The p-values associated with the coefficient estimates for the two dummy variables are very large, suggesting no statistical evidence of a real difference in credit card balance between the ethnicities</p><blockquote><p>The coefficients and their p-values do depend on the choice of dummy variable coding</p></blockquote><p>Rather than rely on the individual coefficients, we can use an <strong>F-test</strong> to test H0 : β1 = β2 = 0; this does not depend on the coding.</p><p>This F-test has a p-value of 0.96, indicating that we cannot reject the null hypothesis that there is no relationship between balance and ethnicity.</p><h1 id="extensions-of-the-linear-model">Extensions of the Linear Model</h1><p>Two of the most important assumptions state that the relationship between the predictors and response are <strong>additive</strong> and <strong>linear</strong>. - <strong>Additive</strong>: the effect of changes in a predictor <span class="math inline">\(X_j\)</span> on the response <span class="math inline">\(Y\)</span> is independent of the values of the other predictors - <strong>Linear</strong>: the change in the response <span class="math inline">\(Y\)</span> due to a one-unit change in <span class="math inline">\(X_j\)</span> is constant, regardless of the value of <span class="math inline">\(X_j\)</span></p><h2 id="removing-the-additive-assumption">Removing the Additive Assumption</h2><p>Consider the standard linear regression model with two variables, <span class="math display">\[\begin{align}Y = β_0 + β_1X_1 + β_2X_2 + \epsilon\end{align}\]</span></p><p>One way of extending this model to allow for interaction effects is to include a third predictor, called an <strong>interaction term</strong>:</p><p><span class="math display">\[\begin{align}Y = β_0 + β_1X_1 + β_2X_2 +  β_3X_1X_2 + \epsilon \end{align}\]</span></p><p><strong>How does inclusion of this interaction term relax the additive assumption?</strong></p><p>The model above could be written as: <span class="math display">\[\begin{align}Y &amp;= β_0 + (β_1+β_3X_2)X_1 + β_2X_2 + \epsilon  \\&amp;= β_0 + \tilde{β}_1X_1 + β_2X_2 + \epsilon\end{align}\]</span></p><p>Since <span class="math inline">\(\tilde{β}_1\)</span> changes with <span class="math inline">\(X_2\)</span>, the effect of <span class="math inline">\(X_1\)</span> on <span class="math inline">\(Y\)</span> is no longer constant: adjusting <span class="math inline">\(X_2\)</span> will change the impact of <span class="math inline">\(X_1\)</span> on <span class="math inline">\(Y\)</span>.</p><p><img src="./14.png" width="600"></p><ul><li>Sometimes the case that an interaction term has a very small p-value, but the associated main effects (in this case, TV and radio) do not.</li><li>The <strong>hierarchical principle</strong> states that if we include an interaction in a model, we should also include the <strong>main effects</strong>, even if the p-values associated with their coefficients are not significant. (If the interaction between X1 and X2 seems important, we should include both X1 and X2 in the model even if their coefficient estimates have large p-values)</li></ul><p><strong>Concept of interactions applies on qualitative variables</strong> <img src="./15.png" width="600"></p><p>Adding an interaction variable, model now becomes: <img src="./17.png" width="600"> <img src="./16.png" width="600"></p><h2 id="non-linear-relationships">Non-linear Relationships</h2><p>Extending the linear model to accommodate non-linear relationships is known as <strong>polynomial regression</strong>, since we have included <strong>polynomial functions</strong> of the predictors in the regression model</p><h1 id="potential-problems">Potential Problems</h1><h2 id="non-linearity-of-the-data">Non-linearity of the Data</h2><p><strong>Assumption</strong>: The linear regression model assumes that there is a straight-line relationship between the predictors and the response.</p><p><strong>Residual plots</strong>: graphical tool for identifying non-linearity - Given a simple linear regression model, we can plot the residuals,<span class="math inline">\(e_i = y_i-\hat{y_i}\)</span> versus the predictor <span class="math inline">\(x_i\)</span>, or <span class="math inline">\(\hat{y_i}\)</span> when there are multiple predictors</p><p><img src="./18.png" width="600"></p><ul><li>Ideally, the residual plot will show no discernible pattern.</li><li>If the residual plot indicates non-linear associations in the data, then a simple approach is to use <strong>non-linear transformations</strong> of the predictors, such as <span class="math inline">\(\log{X},\sqrt{X}, X^2\)</span>, in the regression model.</li></ul><h2 id="correlation-of-error-terms">Correlation of Error Terms</h2><p><strong>Assumption</strong>: The error terms, <span class="math inline">\(\epsilon_1,\epsilon_2,...,\epsilon_n\)</span>, are uncorrelated. - If the errors are uncorrelated, then the fact that i is positive provides little or no information about the sign of i+1. - If the error terms are correlated, we may have an unwarranted sense of confidence in our model. - <strong>estimated standard errors</strong> will underestimate the true standard errors. - <strong>confidence and prediction intervals</strong> will be narrower than they should be. For example, a 95% confidence interval may in reality have a much lower probability than 0.95 of containing the true value of the parameter. - <strong>p-values</strong> will be lower than they should be - Lead to erroneously conclude that a parameter is statistically significant.</p><p><strong>Why might correlations among the error terms occur?</strong> - Such correlations frequently occur in the context of <strong>time series</strong> data - In many cases, observations that are obtained at adjacent time points will have <strong>positively correlated errors</strong>. - Plot the residuals from our model as a function of time to identify this correlation. - <strong>Tracking</strong>: If the error terms are positively correlated, then we may see <strong>tracking</strong> in the residuals—that is, adjacent residuals may have similar values. <img src="./19.png" width="600"></p><h2 id="non-constant-variance-of-error-terms">Non-constant Variance of Error Terms</h2><p><strong>Assumption</strong>: the error terms have a constant variance, <span class="math inline">\(Var(\epsilon_i) = σ^2\)</span>. - The standard errors, confidence intervals, and hypothesis tests associated with the linear model rely upon this assumption.</p><p>The variances of the error terms are non-constant. - For instance, the variances of the error terms may increase with the value of the response. - One can identify non-constant variances in the errors, or <strong>heteroscedasticity</strong>异方差性,from the presence of a funnel shape in residual plot. - <strong>Solution</strong>: transform the response Y using a concave function such as <span class="math inline">\(\log{Y}\)</span> or <span class="math inline">\(\sqrt{Y}\)</span> . Such a transformation results in a greater amount of shrinkage of the larger responses, leading to a reduction in <strong>heteroscedasticity</strong>.</p><p><img src="./20.png" width="600"></p><h2 id="outliers">Outliers</h2><p><strong>Outlier</strong>: is a point for which <span class="math inline">\(y_i\)</span> is far from the value predicted by the model. Outliers can arise for a variety of reasons, such as incorrect recording of an observation during data collection.</p><p><strong>Problems of Outlier</strong>: - Effect on the least squares fit, - Effect on interpretation of the fit - For instance, in this example, the RSE is 1.09 when the outlier is included in the regression, but it is only 0.77 when the outlier is removed.</p><p><strong>Residual Plots</strong> can be used to identify outliers</p><p><img src="./21.png" width="600"></p><ul><li>Difficult to decide how large a residual needs to be</li></ul><p><strong>Studentized residuals</strong>: computed by dividing each residual <span class="math inline">\(e_i\)</span> by its estimated standard error. - Observations whose studentized residuals are greater than 3 in abso- residual lute value are possible outliers.</p><h2 id="high-leverage-points">High Leverage Points</h2><p><strong>High Leverage</strong>:Observations with high leverage have an unusual value for xi - removing the high leverage observation has a much more substantial impact on the least squares line than removing the outlier. <img src="./23.png" width="600"></p><p><strong>Leverage Statistic</strong>: quantify an observation’s leverage</p><p>For a simple linear regression</p><p><span class="math display">\[\begin{align}h_i=\frac{1}{n}+\frac{(x_i-\bar{x})^2}{\sum_{i^{&#39;}=1}^n (x_{i^{&#39;}}-\bar{x})^2}\end{align}\]</span></p><ul><li><span class="math inline">\(h_i\)</span> increases with the distance of <span class="math inline">\(x_i\)</span> from <span class="math inline">\(\bar{x}\)</span>.</li><li><span class="math inline">\(h_i\)</span> is always between 1/n and 1, and the <strong>average leverage</strong> for all the observations is always equal to <span class="math inline">\((p+1)/n\)</span>.</li><li><strong>High leverage</strong>: a leverage statistic that greatly exceeds <span class="math inline">\((p+1)/n\)</span>, high leverage.</li></ul><p><img src="./22.png" width="600"></p><p>The right-hand panel of Figure 3.13 provides a plot of the studentized residuals versus <span class="math inline">\(h_i\)</span> for the data in the left-hand panel of Figure 3.13. Observation 41 stands out as having a very high leverage statistic as well as a high studentized residual. In other words, it is an outlier as well as a high leverage observation.</p><h2 id="collinearity">Collinearity</h2><p>Collinearity: situation in which two or more predictor variables are closely related to one another.</p><p><strong>Problems of Collinearity</strong></p><ul><li>Difficult to separate out the individual effects of collinear variables on the response</li><li>Uncertainty in the coefficient estimates.</li><li>Causes the standard error for <span class="math inline">\(\hat{β_j}\)</span> to grow</li><li>Recall that the t-statistic for each predictor is calculated by dividing <span class="math inline">\(\hat{β_j}\)</span> by its standard error. Consequently, collinearity results in a decline in the t-statistic. As a result, in the presence of collinearity, we may fail to reject H0 : βj = 0. This means that the <strong>power</strong> of the hypothesis test—the probability of correctly detecting a non-zero coefficient—is reduced by collinearity.</li></ul><p><img src="./24.png" width="600"></p><p><strong>Detection of Collinearity</strong></p><ul><li><strong>Correlation matrix</strong> of the predictors.</li><li>An element of this matrix that is large in absolute value indicates a pair of highly correlated variables.</li><li><strong>Situation Multicollinearity</strong>: it is possible for collinearity to exist between three or more variables even if no pair of variables has a particularly high correlation</li><li><strong>Variance Inflation Factor (VIF)</strong></li><li>The ratio of the variance of <span class="math inline">\(\hat{β_j}\)</span> when fitting the full model divided by the variance of <span class="math inline">\(\hat{β_j}\)</span> if fit on its own. The smallest possible value for VIF is 1, which indicates the complete absence of collinearity.</li><li>A VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity.</li></ul><p>The VIF for each variable,</p><p>where <span class="math inline">\(R^2_{X_j|X_{-j}}\)</span> is the <span class="math inline">\(R^2\)</span> from a regression of <span class="math inline">\(X_j\)</span> onto all of the other predictors. If <span class="math inline">\(R^2_{X_j|X_{-j}}\)</span> is close to one, then collinearity is present, and so the VIF will be large.</p><p><strong>Solution of Collinearity</strong></p><ul><li>Drop one of the problematic variables from the regression.</li><li>Combine the collinear variables together into a single predicto</li><li>E.g.: take the average of standardized versions of limit and rating in order to create a new variable that measures credit worthiness</li></ul><hr><p><strong>Ref:</strong></p><p>James, Gareth, et al. <em>An introduction to statistical learning</em>. Vol. 112. New York: springer, 2013.</p><p>Hastie, Trevor, et al. &quot;The elements of statistical learning: data mining, inference and prediction.&quot; <em>The Mathematical Intelligencer</em> 27.2 (2005): 83-85</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;qualitative-predictors&quot;&gt;Qualitative Predictors&lt;/h1&gt;
&lt;h2 id=&quot;predictors-with-only-two-levels&quot;&gt;Predictors with Only Two Levels&lt;/h2&gt;
&lt;p&gt;Suppose that we wish to investigate differences in credit card balance between males and females, ignoring the other variables for the moment. If a qualitative predictor (also known as a &lt;strong&gt;factor&lt;/strong&gt;) only has two &lt;strong&gt;levels&lt;/strong&gt;, or possible values, then incorporating it into a regression model is very simple. We simply create an indicator or &lt;strong&gt;dummy variable&lt;/strong&gt; that takes on two possible numerical values.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;./8.png&quot; width=&quot;300&quot;&gt; and use this variable as a predictor in the regression equation. This results in the model&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;./9.png&quot; width=&quot;600&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Linear Regression" scheme="https://nancyyanyu.github.io/tags/Linear-Regression/"/>
    
      <category term="Regression" scheme="https://nancyyanyu.github.io/tags/Regression/"/>
    
  </entry>
  
  <entry>
    <title>Study Note: Linear Regression Part I - Linear Regression Models</title>
    <link href="https://nancyyanyu.github.io/posts/9b5af8e1/"/>
    <id>https://nancyyanyu.github.io/posts/9b5af8e1/</id>
    <published>2019-10-19T22:56:00.145Z</published>
    <updated>2019-10-19T23:21:12.394Z</updated>
    
    <content type="html"><![CDATA[<h1 id="simple-linear-regression-models">Simple Linear Regression Models</h1><h2 id="linear-regression-model">Linear Regression Model</h2><ul><li><p>Form of the linear regression model: <em><span class="math inline">\(f(X)=\beta_{0}+\sum_{j=1}^{p}X_{j}\beta_{j}\)</span></em>.</p></li><li><p>Training data: (<span class="math inline">\(x_1\)</span>,<span class="math inline">\(y_1\)</span>) ... (<span class="math inline">\(x_N\)</span>,<span class="math inline">\(y_N\)</span>). Each <span class="math inline">\(x_{i} =(x_{i1},x_{i2},...,x_{ip})^{T}\)</span> is a vector of feature measurements for the <span class="math inline">\(i\)</span>-th case.</p></li><li><p>Goal: estimate the parameters <span class="math inline">\(β\)</span></p></li><li><p>Estimation method: <strong>Least Squares</strong>, we pick the coeﬃcients <span class="math inline">\(β =(β_0,β_1,...,β_p)^{T}\)</span> to minimize the <strong>residual sum of squares</strong></p></li></ul><p><strong>Assumptions:</strong></p><ul><li>Observations <span class="math inline">\(y_i\)</span> are uncorrelated and have constant variance <span class="math inline">\(\sigma^2\)</span>;</li><li><span class="math inline">\(x_i\)</span> are ﬁxed (non random)</li><li>The regression function E(Y |X) is linear, or the linear model is a reasonable approximation.</li></ul><a id="more"></a><h2 id="residual-sum-of-squares">Residual Sum of Squares</h2><p><strong>Residual</strong>: <span class="math inline">\(e_i = y_i−\hat{y_i}\)</span> represents the ith residual—this is the difference between residual the ith observed response value and the ith response value that is predicted by our linear model. <span class="math display">\[\begin{align}RSS(\beta)&amp;=e_1^2+e_2^2+e_3^2+...e_n^2 \\&amp;=\sum_{i=1}^{N}(y_{i}-f(x_{i}))^2 \\&amp;=\sum_{i=1}^{N}(y_{i}-\beta_{0}-\sum_{j=1}^{p}X_{ij}\beta_{j})^2\end{align}\]</span></p><h4 id="solution">Solution</h4><p>Denote by <span class="math inline">\(X\)</span> the $N × (p + 1) $matrix with each row an input vector (with a 1 in the ﬁrst position), and similarly let <span class="math inline">\(y\)</span> be the <span class="math inline">\(N\)</span>-vector of outputs in the training set.</p><p><span class="math display">\[\begin{align} \min RSS(\beta)= (y-\mathbf{X}\beta)^T(y-\mathbf{X}\beta) \end{align}\]</span> A quadratic function in the <span class="math inline">\(p + 1\)</span> parameters</p><p>Taking derivatives:</p><p><span class="math display">\[\begin{align} \frac{\partial RSS}{\partial \beta}=-2\mathbf{X}^T(y-\mathbf{X}\beta) \end{align}\]</span></p><p><span class="math display">\[\begin{align} \frac{\partial^2 RSS}{\partial \beta \partial \beta^T}=2\mathbf{X}^T\mathbf{X}  \end{align}\]</span></p><p>Assuming (for the moment) that <span class="math inline">\(\mathbf{X}\)</span> has full column rank, and hence <span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> is positive deﬁnite, we set the ﬁrst derivative to zero: <span class="math inline">\(\mathbf{X}^T(y-\mathbf{X}\beta)=0\)</span></p><p><span class="math display">\[\begin{align}\Rightarrow \hat{\beta_1}&amp;=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^Ty \\&amp;=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2} \\\hat{\beta_0}&amp;=\bar{y}-\hat{\beta_1}\bar{x} \end{align}\]</span> where <span class="math inline">\(\bar{y}=\sum_{i=1}^ny_i/n\)</span>, <span class="math inline">\(\bar{x}=\sum_{i=1}^nx_i/n\)</span> are the <strong>sample means</strong>.</p><p>Fitted values at the training inputs: <span class="math inline">\(\hat{y}=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^Ty\)</span></p><p>Hat matrix: <span class="math inline">\(H=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\)</span></p><h2 id="assessing-the-accuracy-of-the-coefficient-estimates">Assessing the Accuracy of the Coefficient Estimates</h2><p>Assume that the true relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> takes the form <span class="math inline">\(Y = f(X) + \epsilon\)</span> for some unknown function <span class="math inline">\(f\)</span>, where <span class="math inline">\(\epsilon\)</span> is a mean-zero random error term.</p><p><strong>Least squares line</strong>: <span class="math display">\[\begin{align}\hat{y_i} = \hat{β_0} + \hat{β_1}x_i\end{align}\]</span> <strong>Population regression line</strong>: <span class="math display">\[\begin{align}Y=\beta_0+\beta_1X+\epsilon\end{align}\]</span> The <strong>error term</strong> is a catch-all for what we miss with this simple model: the true relationship is probably not linear, there may be other variables that cause variation in <span class="math inline">\(Y\)</span> , and there may be measurement error. We typically assume that the error term is independent of <span class="math inline">\(X\)</span>.</p><p><img src="./1_v2.png" width="600"></p><h3 id="population-v.s.-sample">Population V.S. Sample</h3><p>The true relationship is generally not known for real data, but the least squares line can always be computed using the coefficient estimates.</p><p><strong>Why there are two different lines describe the relationship between the predictor and the response?</strong></p><ul><li>The concept of these two lines is a natural extension of the standard statistical approach of using information from a sample to estimate characteristics of a large population.</li><li>The <strong>sample mean</strong> <span class="math inline">\(\bar{x}=\sum_{i=1}^nx_i/n\)</span> and the <strong>population mean</strong> <span class="math inline">\(\mu\)</span> are different, but in general the sample mean <span class="math inline">\(\bar{x}\)</span> will provide a good estimate of the population mean <span class="math inline">\(\hat{\mu}\)</span>.</li></ul><p><strong>Unbiased</strong></p><ul><li>If we use the sample mean <span class="math inline">\(\hat{\mu}\)</span> to estimate μ, this estimate is <strong>unbiased</strong>, in the sense that on average, we expect <span class="math inline">\(\hat{\mu}\)</span> to equal <span class="math inline">\(μ\)</span>.</li><li>An unbiased estimator does not systematically over- or under-estimate the true parameter.</li></ul><h3 id="standard-error">Standard Error</h3><p><strong>How accurate is the sample mean <span class="math inline">\(\hat{\mu}\)</span> as an estimate of μ?</strong></p><ul><li><strong>Standard error of <span class="math inline">\(\hat{\mu}\)</span>(SE(<span class="math inline">\(\hat{\mu}\)</span>)</strong>): average amount that this estimate <span class="math inline">\(\hat{\mu}\)</span> differs from the actual value of μ. <span class="math display">\[\begin{align}Var(\hat{\mu})=SE(\hat{\mu})^2=\frac{\sigma^2}{n}\end{align}\]</span> where <span class="math inline">\(σ\)</span> is the standard deviation of each of the realizations <span class="math inline">\(y_i\)</span> of <span class="math inline">\(Y\)</span> provided that the n observations are <strong>uncorrelated</strong>.</li></ul><p><strong>Standard Deviation V.S. Standard Error</strong></p><ul><li>The <strong>standard deviation (SD)</strong> measures the amount of variability, or dispersion, for a subject set of data from the mean</li><li>The <strong>standard error of the mean (SEM)</strong> measures how far the sample mean of the data is likely to be from the true population mean.</li></ul><p><img src="./2_v2.png" width="300"></p><p><strong>How close <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span> are to the true values <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>?</strong></p><ul><li><p><span class="math display">\[\begin{align}SE(\hat{\beta_0})^2&amp;=\sigma^2\left[\frac{1}{n}+\frac{\bar{x}^2}{\sum_{i=1}^n(x_i-\bar{x})^2}  \right]  \\SE(\hat{\beta_1})^2&amp;=\frac{\sigma^2}{\sum_{i=1}^n(x_i-\bar{x})^2} \end{align}\]</span></p><p>where <span class="math inline">\(\sigma^2 = Var(\epsilon)\)</span></p></li><li><p>For these formulas to be strictly valid, we need to assume that the errors <span class="math inline">\(\epsilon_i\)</span> for each observation are uncorrelated with common variance <span class="math inline">\(σ^2\)</span>.</p></li></ul><p><strong>Estimate <span class="math inline">\(\sigma^2\)</span></strong></p><ul><li><p><strong>residual standard error(RSE)</strong>: <span class="math inline">\(\sigma^2\)</span> is not known, but can be estimated from the data. This estimate is known as the <strong>residual standard error</strong></p></li><li><p><span class="math display">\[\begin{align}RSE=\sqrt{RSS/(n-2)}\end{align}\]</span></p></li></ul><h2 id="sampling-properties-of-beta">Sampling Properties of <span class="math inline">\(\beta\)</span></h2><p>The <u>variance–covariance</u> matrix of the least squares parameter estimates: <span class="math display">\[\begin{align} Var(\hat{\beta})=(\mathbf{X}^T\mathbf{X})^{-1}\sigma^2 \end{align}\]</span> <strong>Unbiased estimate of <span class="math inline">\(\sigma^2\)</span>:</strong> <span class="math display">\[\begin{align} \hat{\sigma}^2=\frac{1}{N-p-1}\sum^{N}_{i=1}(y_i-\hat{y_i})^2 \end{align}\]</span> Assume the deviations of <span class="math inline">\(\mathbf{Y}\)</span> around its expectation are <u>additive and Gaussian</u>: <span class="math display">\[\begin{align} Y=E(Y|X_1,...,X_p)+\epsilon=\beta_0+\sum_{j=1}^{p}X_j\beta_j+\epsilon \end{align}\]</span> where <span class="math inline">\(\epsilon \sim N(0,\sigma^2)\)</span></p><p>Thus, <span class="math inline">\(\beta\)</span> follows <u>multivariate normal distribution</u> with mean vector and variance–covariance matrix: <span class="math display">\[\begin{align}\hat{\beta} \sim N(\beta,(\mathbf{X}^T\mathbf{X})^{-1}\sigma^2 ) \end{align}\]</span> Also, a chi-squared distribution with <span class="math inline">\(N −p−1\)</span> degrees of freedom: <span class="math display">\[\begin{align} (N-p-1)\hat{\sigma}^2 \sim \sigma^2 \chi_{N-p-1}^{2} \end{align}\]</span> (<span class="math inline">\(\hat{\beta}\)</span> and <span class="math inline">\(\hat{\sigma^2}\)</span> are indep.)</p><p>We use these distributional properties to form tests of hypothesis and conﬁdence intervals for the parameters <span class="math inline">\(\beta_j\)</span>:</p><p><strong>Confidence Intervals</strong></p><ul><li><p><strong>A 95% confidence confidence interval</strong>: is defined as a range of values such that with 95% interval probability, the range will contain the true unknown value of the parameter.</p></li><li><p>For linear regression, the 95% confidence interval for <span class="math inline">\(β_1\)</span> approximately takes the form <span class="math display">\[\begin{align}&amp;\hat{\beta_1} \pm 2 \cdot SE(\hat{\beta_1})     \\&amp;SE(\hat{\beta_1}) =\frac{\sigma^2}{\sum_{i=1}^n(x_i-\bar{x})^2} \end{align}\]</span> (which relies on the assumption that the errors are Gaussian. Also, the factor of 2 in front of the <span class="math inline">\(SE(\hat{\beta_1})\)</span> term will vary slightly depending on the number of observations n in the linear regression. To be precise, rather than the number 2, it should contain the 97.5% quantile of a t-distribution with n−2 degrees of freedom.)</p><p>(which relies on the assumption that the errors are Gaussian. Also, the factor of 2 in front of the <span class="math inline">\(SE(\hat{\beta_1})\)</span> term will vary slightly depending on the number of observations n in the linear regression. To be precise, rather than the number 2, it should contain the 97.5% quantile of a t-distribution with n−2 degrees of freedom.)</p></li></ul><p><strong>In ESL book:</strong></p><p><span class="math inline">\(1-2\alpha\)</span> conﬁdence interval for <span class="math inline">\(\beta_j\)</span>: <span class="math display">\[\begin{align} (\hat{\beta_j}-z^{(1-\alpha)}\upsilon^{0.5}_j \hat{\sigma},\hat{\beta_j}+z^{(1-\alpha)}\upsilon^{0.5}_j \hat{\sigma}) \end{align}\]</span> where <span class="math inline">\(z^{(1-\alpha)}\)</span> is the 1 − α percentile of the normal distribution. <span class="math inline">\(\alpha\)</span> could be 0.025, 0.5, etc.</p><p>where <span class="math inline">\(z^{(1-\alpha)}\)</span> is the 1 − α percentile of the normal distribution. <span class="math inline">\(\alpha\)</span> could be 0.025, 0.5, etc.</p><p>In a similar fashion we can obtain an approximate confidence set for the entire parameter vector <span class="math inline">\(\beta\)</span>, namely <span class="math display">\[\begin{equation}C_\beta = \left\{ \beta \big| (\hat\beta-\beta)^T\mathbf{X}^T\mathbf{X}(\hat\beta-\beta) \le \hat\sigma^2{\chi^2_{p+1}}^{(1-\alpha)}\right\},\end{equation}\]</span> This condifence set for <span class="math inline">\(\beta\)</span> generates a corresponding confidence set for the true function <span class="math inline">\(f(x) = x^T\beta\)</span>: <span class="math inline">\(\left\{ x^T\beta \big| \beta \in C_\beta \right\}\)</span></p><h2 id="inference">Inference</h2><h3 id="hypothesis-tests">Hypothesis Tests</h3><p>The most common hypothesis test involves testing the <strong>null test hypothesis</strong> of</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">H_0: There is no relationship between X and Y or β1=0</span><br></pre></td></tr></table></figure><p>versus the <strong>alternative hypothesis</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">H_a : There is some relationship between X and Y or β1≠0</span><br></pre></td></tr></table></figure><p>To test the null hypothesis, we need to determine whether <span class="math inline">\(\hat{\beta_1}\)</span>, our estimate for <span class="math inline">\(\beta_1\)</span>, is sufficiently far from zero that we can be confident that <span class="math inline">\(\beta_1\)</span> is non-zero <span class="math inline">\(\Rightarrow\)</span> it depends on <span class="math inline">\(SE( \hat{\beta_1}\)</span>)</p><ul><li>If <span class="math inline">\(SE( \hat{\beta_1}\)</span>) is small, then even relatively small values of <span class="math inline">\(\hat{\beta_1}\)</span> may provide strong evidence that <span class="math inline">\(\beta_1 \neq 0\)</span>, and hence that there is a relationship between X and Y</li></ul><h4 id="t-statistic"><strong>T-statistic</strong>:</h4><p>To test the hypothesis that a particular coeﬃcient <span class="math inline">\(\beta_j= 0\)</span>, we form the standardized coeﬃcient or Z-score <span class="math display">\[\begin{align}t=\frac{\hat{\beta_1}-0}{SE(\hat{\beta_1})}  \\ or \quad z_j=\frac{\hat{\beta_j}-0}{\hat{\sigma}\sqrt{\upsilon_j}}\end{align}\]</span> where <span class="math inline">\(\upsilon_j\)</span> is the j-th diagonal element of <span class="math inline">\((\mathbf{X}^T\mathbf{X})^{-1}\)</span></p><p>which measures the number of standard deviations that <span class="math inline">\(\hat{\beta_1}\)</span> is away from 0.If there really is no relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> , then we expect it will have a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n−2\)</span> degrees of freedom.</p><p>Under the null hypothesis that <span class="math inline">\(\beta_j= 0\)</span>, <span class="math inline">\(z_j\)</span> is distributed as <span class="math inline">\(t_{N-p-1}\)</span>, and hence a large (absolute) value of <span class="math inline">\(z_j\)</span> will lead to rejection of this null hypothesis. If <span class="math inline">\(\hat{\sigma}\)</span> is replaced by a known value <span class="math inline">\(σ\)</span>, then <span class="math inline">\(z_j\)</span> would have a standard normal distribution.</p><h4 id="p-value"><strong>p-value</strong></h4><ul><li>The probability of observing any value <span class="math inline">\(≥ t\)</span> or <span class="math inline">\(≤ -t\)</span>, assuming <span class="math inline">\(β_1 = 0\)</span>.</li></ul><p><img src="./3_v2.png" width="300"> (Here |t|=2.17, p-value=0.015.The area in red is 0.015 + 0.015 = 0.030, 3%. If we had chosen a significance level of 5%, this would mean that we had achieved statistical significance. We would reject the null hypothesis in favor of the alternative hypothesis.)</p><ul><li><strong>Interpretation</strong>:a small p-value indicates<ul><li>It is unlikely to observe such a substantial association between the predictor and the response due to LUCK, in the absence of any real association between the predictor and the response.</li><li>There is an association between the predictor and the response.</li><li>We reject the null hypothesis—that is, we declare a relationship to exist between X and Y</li></ul></li></ul><h4 id="f-statistic"><strong>F-statistic</strong>:</h4><p>To test if a categorical variable with <span class="math inline">\(k\)</span> levels can be excluded from a model, we need to test whether the coeﬃcients of the dummy variables used to represent the levels can all be set to zero. Here we use the <span class="math inline">\(F\)</span> statistic：</p><p><span class="math display">\[\begin{align} F=\frac{(RSS_0-RSS_1)/(p_1-p_0)}{RSS_1/(N-p_1-1)} \end{align}\]</span></p><ul><li><span class="math inline">\(RSS_1\)</span> is the residual sum-of-squares for the least squares ﬁt of the bigger model with <span class="math inline">\(p_1+1\)</span> parameters;</li><li><span class="math inline">\(RSS_0\)</span> the same for the nested smaller model with <span class="math inline">\(p_0 +1\)</span> parameters, having <span class="math inline">\(p_1 −p_0\)</span> parameters constrained to be zero.</li></ul><p>The <span class="math inline">\(F\)</span> statistic measures the change in residual sum-of-squares per additional parameter in the bigger model, and it is normalized by an estimate of <span class="math inline">\(\sigma^2\)</span>.</p><p>Under the Gaussian assumptions, and the null hypothesis that the smaller model is correct, the <span class="math inline">\(F\)</span> statistic will have a <span class="math inline">\(F_{p_1-p_0,N-p_1-1}\)</span> distribution.</p><h2 id="the-gaussmarkov-theorem">The Gauss–Markov Theorem</h2><p>We focus on estimation of any linear combination of the parameters <span class="math inline">\(\theta=\alpha^T\beta\)</span>, for example, predictions <span class="math inline">\(f(x_0)=x^T_0\beta\)</span> are of this form.The least squares estimate of <span class="math inline">\(\alpha^T\beta\)</span> is: <span class="math display">\[\begin{equation}\hat{\theta}=\alpha^T\hat{\beta}=\alpha^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\end{equation}\]</span> Considering <span class="math inline">\(\mathbf{X}\)</span> to be ﬁxed, this is a linear function <span class="math inline">\(\mathbf{c}^T_0\mathbf{y}\)</span> of the response vector <span class="math inline">\(\mathbf{y}\)</span>.If we assume that the linear model is correct, <span class="math inline">\(\alpha^T\hat{\beta}\)</span> is unbiased.</p><p>The Gauss–Markov theorem states that if we have any other linear estimator <span class="math inline">\(\tilde{\theta}=\mathbf{c}^T\mathbf{y}\)</span> that is unbiased for <span class="math inline">\(\alpha^T\beta\)</span>, that is, <span class="math inline">\(E(\mathbf{c}^T\mathbf{y})= \alpha^T\beta\)</span>, then: <span class="math display">\[\begin{equation}Var(\alpha^T\hat{\beta})\leq Var(\mathbf{c}^T\mathbf{y})\end{equation}\]</span></p><h2 id="assessing-the-accuracy-of-the-model">Assessing the Accuracy of the Model</h2><p>The quality of a linear regression fit is typically assessed using two related quantities: <strong>the residual standard error (RSE)</strong> and the <strong>R2</strong> statistic.</p><h3 id="residual-standard-error">Residual Standard Error</h3><p>The <span class="math inline">\(RSE\)</span> is <u>an estimate of the standard deviation of <span class="math inline">\(\epsilon\)</span>: t</u>he average amount that the response will deviate from the true regression line <span class="math display">\[\begin{align}RSS&amp;=\sum_{i=1}^n(y_i-\hat{y})^2  \\RSE&amp;=\sqrt{\frac{1}{n-2}RSS}=\sqrt{\frac{1}{n-2}\sum_{i=1}^n(y_i-\hat{y})^2}\end{align}\]</span> <img src="./4_v2.png" width="600"></p><p>In the case of the advertising data, we see from the linear regression output in Table 3.2 that the RSE is 3.26. In other words, actual sales in each market deviate from the true regression line by approximately 3,260 units, on average.</p><p>The mean value of sales over all markets is approximately 14,000 units, and so the percentage error is 3,260/14,000 = 23%.</p><p><u>The RSE is considered a measure of the <strong>lack of fit</strong> of the model <span class="math inline">\(Y = β_0 + β_1X + \epsilon\)</span> to the data</u>.</p><h3 id="r2-statistic">R2 Statistic</h3><p>The <span class="math inline">\(RSE\)</span> is measured in the units of <span class="math inline">\(Y\)</span> , it is not always clear what constitutes a good <span class="math inline">\(RSE\)</span>.</p><p>The <span class="math inline">\(R^2\)</span> statistic takes the form of a <strong>proportion</strong>—<u>the proportion of variance explained</u>—and so it always takes on a value between 0 and 1, and isi ndependent of the scale of <span class="math inline">\(Y\)</span> . <span class="math display">\[\begin{align}R^2 = (TSS − RSS)/TSS= 1− RSS/TSS =  1-\frac{\sum(y_i-\hat{y})^2}{\sum(y_i-\bar{y})^2}\end{align}\]</span> <strong>TSS(total sum of squares)</strong>: <span class="math inline">\(\sum(y_i-\bar{y})^2\)</span> - the amount of variability inherent in the response before the regression is performed</p><p><strong>RSS</strong>: <span class="math inline">\(\sum_{i=1}^n(y_i-\hat{y})^2\)</span> - the amount of variabilityt hat is left unexplained after performing the regression</p><p><strong>(TSS−RSS)</strong>: measures the amount of variability in the response that is explained (or removed) by performing the regression, and <strong><span class="math inline">\(R^2\)</span> measures the proportion of variability in <span class="math inline">\(Y\)</span> that can be explained using <span class="math inline">\(X\)</span>.</strong></p><p><strong>Interpretation</strong>：</p><ul><li><u>close to 1</u> : a large proportion of the variability in the response has been explained by the regression.</li><li><u>close to 0</u> : the regression did not explain much of the variability in the response<ul><li>The linear model is wrong</li><li>The inherent error <span class="math inline">\(σ^2\)</span> is high, or both.</li></ul></li></ul><h2 id="squared-correlation-v.s.-r2-statistic">Squared Correlation V.S. R2 Statistic</h2><p><strong>Correlation</strong>: <span class="math display">\[\begin{align}Cor(X,Y)=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^n(x_i-\bar{x})^2}\sqrt{\sum_{i=1}^n(y_i-\bar{y})^2}}\end{align}\]</span> is also a measure of the linear relationship between X and Y.</p><blockquote><p>In the simple linear regression setting, <span class="math inline">\(R^2 = [Cor]^2\)</span>. In other words, the squared correlation and the R2 statistic are identical</p></blockquote><h2 id="note-of-p-value-v.s.-r-square">Note of P-Value v.s. R-Square</h2><p>Referring answer fromn <a href="https://www.researchgate.net/profile/Faye_Anderson3" target="_blank" rel="noopener">Faye Anderson</a>:</p><p>There is no established association/relationship between p-value and R-square. This all depends on the data (i.e.; contextual).</p><p>R-square value tells you how much variation is explained by your model. So 0.1 R-square means that your model explains 10% of variation within the data. The greater R-square the better the model. Whereas p-value tells you about the F statistic hypothesis testing of the &quot;fit of the intercept-only model and your model are equal&quot;. So if the p-value is less than the significance level (usually 0.05) then your model fits the data well.</p><p><u>Thus you have four scenarios:</u></p><p><strong>1) low R-square and low p-value (p-value &lt;= 0.05) :</strong> means that your model doesn't explain much of variation of the data but it is significant (better than not having a model)</p><p><strong>2) low R-square and high p-value (p-value &gt; 0.05) :</strong> means that your model doesn't explain much of variation of the data and it is not significant (worst scenario)</p><p><strong>3) high R-square and low p-value :</strong> means your model explains a lot of variation within the data and is significant (best scenario)</p><p><strong>4) high R-square and high p-value :</strong> means that your model explains a lot of variation within the data but is not significant (model is worthless)</p><h2 id="variance-bias">Variance &amp; Bias</h2><p>Consider the mean squared error of an estimator <span class="math inline">\(\tilde{\theta}\)</span> in estimating <span class="math inline">\(\theta\)</span>:</p><p><span class="math display">\[\begin{align}MSE(\tilde{\theta})&amp;= E(\tilde{\theta}-\theta)^2 \\&amp;= E(\tilde{\theta^2}+\theta^2-2\theta\tilde{\theta}) \\&amp;= E(\tilde{\theta^2})-E^2(\tilde{\theta})+E^2(\tilde{\theta})+E(\theta^2-2\theta\tilde{\theta})\\&amp;= Var(\tilde{\theta})+[E(\tilde{\theta})-\theta]^2\end{align}\]</span> The ﬁrst term is the <strong>variance</strong>, while the second term is the <strong>squared bias</strong>.</p><p>The <strong>Gauss-Markov theorem</strong> implies that the <em>least squares estimator</em> has the smallest mean squared error of all linear estimators with no bias. However, there may well exist a biased estimator with smaller mean squared error. <font color="red">Such an estimator would trade a little bias for a larger reduction in variance.</font></p><p>From a more pragmatic point of view, most models are distortions of the truth, and hence are biased; picking the right model amounts to creating the right balance between bias and variance.</p><p>Mean squared error is intimately related to prediction accuracy. Consider the prediction of the new response at input <span class="math inline">\(x_0\)</span>, <span class="math display">\[\begin{equation}y_0=f(x_0)+\epsilon_0\end{equation}\]</span></p><h2 id="prediction-error-mse">Prediction error &amp; MSE</h2><p>The expected prediction error of an estimate <span class="math inline">\(\tilde{f}(x_0)=x_0^T\tilde{\beta}\)</span>:</p><p><span class="math display">\[\begin{align}E(y_0-\tilde{f}(x_0))^2 &amp;=E(f(x_0)+\epsilon_0-x_0^T\tilde{\beta})^2 \\&amp;=E(\epsilon_0^2)+E(f(x_0)-x_0^T\tilde{\beta})^2-2E(\epsilon_0(f(x_0)-x_0^T\tilde{\beta})) \\&amp;=\sigma^2+E(f(x_0)-x_0^T\tilde{\beta})^2 \\&amp;=\sigma^2+MSE(\tilde{f}(x_0))\end{align}\]</span> Therefore, expected prediction error and mean squared error diﬀer only by the constant <span class="math inline">\(\sigma^2\)</span>, representing the variance of the new observation <span class="math inline">\(y_0\)</span>.</p><h1 id="multiple-linear-regression">Multiple Linear Regression</h1><p>Multiple linear regression model takes the form: <span class="math display">\[\begin{align}Y=\beta_0+\beta_1X_1+,,,+\beta_pX_p+\epsilon\end{align}\]</span></p><h2 id="estimating-the-regression-coefficients">Estimating the Regression Coefficients</h2><p>We choose <span class="math inline">\(β_0, β_1, . . . , β_p\)</span> to minimize the sum of squared residuals <span class="math display">\[\begin{align}RSS&amp;=\sum_{i=1}^n(y_i-\hat{y}_i)^2 \\&amp;=\sum_{i=1}^n(y_i-\hat{\beta_0}-\hat{\beta_1}x_{i1}-,,,-\hat{\beta_p}x_{ip})^2\end{align}\]</span> <img src="./5_v3.png" width="600"></p><p><strong>Does it make sense for the multiple regression to suggest no relationship between <em>sales</em> and <em>newspaper</em> while the simple linear regression implies the opposite?</strong></p><ul><li>Notice that the correlation between radio and newspaper is 0.35.</li><li>In markets where we spend more on radio our sales will tend to be higher, and as our correlation matrix shows, we also tend to spend more on newspaper advertising in those same markets.</li><li>Hence, in a simple linear regression which only examines sales versus newspaper, we will observe that higher values of newspaper tend to be associated with higher values of sales, even though newspaper advertising does not actually affect sales.</li></ul><h2 id="some-important-questions">Some Important Questions</h2><h3 id="is-there-a-relationship-between-the-response-and-predictors">1. Is There a Relationship Between the Response and Predictors?</h3><h4 id="hypothesis-test"><strong>Hypothesis Test</strong></h4><p>We use a hypothesis test to answer this question.</p><p>We test the <strong>null hypothesis</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">H_0 : β1 = β2 = · · · = βp = 0</span><br></pre></td></tr></table></figure><p>versus the <strong>alternative</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">H_a : at least one βj is non-zero</span><br></pre></td></tr></table></figure><p>This hypothesis test is performed by computing the <strong>F-statistic</strong>, <span class="math display">\[\begin{align}F=\frac{(TSS-RSS)/p}{RSS/(n-p-1)}\end{align}\]</span> where <span class="math inline">\(TSS =\sum(y_i − \bar{y})^2\)</span> and <span class="math inline">\(RSS =\sum(y_i−\hat{y}_i)^2\)</span>.</p><p>If the linear model assumptions are correct, one can show that <span class="math display">\[\begin{align}E[RSS/(n-p-1)]=\sigma^2\end{align}\]</span> and that, provided <span class="math inline">\(H_0\)</span> is true, <span class="math display">\[\begin{align}E[(TSS-RSS)/p]=\sigma^2\end{align}\]</span></p><ul><li>When there is no relationship between the response and predictors, one would expect the F-statistic to take on a value close to 1.</li><li>On the other hand, if <span class="math inline">\(H_a\)</span> is true, then <span class="math inline">\(E[(TSS-RSS)/p]&gt;\sigma^2\)</span>, so we expect <span class="math inline">\(F\)</span> to be greater than 1.</li></ul><p><img src="./6_v3.png" width="600"></p><h3 id="how-large-does-the-f-statistic-need-to-be-before-we-can-reject-h0-and-conclude-that-there-is-a-relationship"><strong>2. How large does the F-statistic need to be before we can reject H0 and conclude that there is a relationship?</strong></h3><ul><li>When n is large, an F-statistic that is just a little larger than 1 might still provide evidence against H_0.</li><li>In contrast, a larger F-statistic is needed to reject H_0 if n is small.</li><li>For the advertising data, the <strong>p-value</strong> associated with the F-statistic in Table 3.6 is essentially zero, so we have extremely strong evidence that at least one of the media is associated with increased sales.</li></ul><p><strong>To test that a particular subset of q of the coefficients are zero</strong></p><p>This corresponds to a null hypothesis</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">H_0 : β(p-q+1) = β(p-q+2) = · · · = βp = 0</span><br></pre></td></tr></table></figure><p>In this case we fit a second model that uses all the variables <strong>except those last q</strong>. Suppose that the residual sum of squares for that model is <span class="math inline">\(RSS_0\)</span>. Then the appropriate F-statistic is <span class="math display">\[\begin{align}F=\frac{(RSS_0-RSS)/q}{RSS/(n-p-1)}\end{align}\]</span></p><h4 id="f-statistics-v.s.-t-statistics"><strong>F-statistics v.s. t-statistics</strong></h4><ul><li><strong>Equivalency</strong>: In Table 3.4, for each individual predictor a t-statistic and a p-value were reported. These provide information about whether each individual predictor is related to the response, after adjusting for the other predictors. It turns out that each of these are exactly equivalent to the F-test that omits that single variable from the model, leaving all the others in—i.e. q=1 in the model. So it reports the <strong>partial effect</strong> of adding that variable to the model.</li></ul><blockquote><p>The square of each <em>t-statistic</em> is the corresponding <em>F-statistic</em>.</p></blockquote><ul><li><strong>p is large</strong>: If any one of the p-values for the individual variables is very small, then <strong><em>at least one of the predictors is related to the response</em></strong>. However, this logic is flawed, especially when the number of predictors p is large.<ul><li>If we use the individual t-statistics and associated p-values to decide whether there is any association between the variables and the response, high chance we will incorrectly conclude there is a relationship.</li><li>However, the F-statistic does not suffer from this problem because it adjusts for the number of predictors.</li></ul></li><li><strong>p &gt; n</strong>: more coefficients βj to estimate than observations from which to estimate them.<ul><li>cannot even fit the multiple linear regression model using least squares,</li></ul></li></ul><h3 id="do-all-the-predictors-help-to-explain-y-or-is-only-a-subset-of-the-predictors-useful">3. Do all the predictors help to explain Y , or is only a subset of the predictors useful?</h3><h4 id="variable-selection"><strong>Variable Selection</strong></h4><ul><li>Various statistics can be used to judge the quality of a model:<ul><li><strong>Mallow’s Cp, Akaike informa-Mallow’s Cp tion criterion (AIC)</strong></li><li><strong>Bayesian information criterion (BIC)</strong></li><li><strong>adjusted R2</strong></li></ul></li><li>There are three classical approaches to select models:<ul><li><strong>Forward selection</strong></li><li><strong>Backward selection</strong></li><li><strong>Mixed selection</strong></li></ul></li></ul><h3 id="how-well-does-the-model-fit-the-data">4. How well does the model fit the data?</h3><p>Two of the most common numerical measures of model fit are the <strong>RSE</strong> and <strong><span class="math inline">\(R^2\)</span></strong></p><h4 id="r2-statistics">R2 Statistics</h4><p>An <span class="math inline">\(R^2\)</span> value close to 1 indicates that the model explains a large portion of the variance in the response variable. <span class="math display">\[\begin{align}R^2 = (TSS − RSS)/TSS= 1− RSS/TSS\end{align}\]</span> Recall that in simple regression, <span class="math inline">\(R^2\)</span> is the square of the correlation of the response and the variable. In multiple linear regression, it turns out that it equals <span class="math inline">\(Cor(Y, \hat{Y} )^2\)</span>, the square of the correlation between the response and the fitted linear model; in fact one property of the fitted linear model is that it maximizes this correlation among all possible linear models.</p><p><strong><span class="math inline">\(R^2\)</span> will always increase when more variables are added to the model, even if those variables are only weakly associated with the response.</strong></p><ul><li>This is due to the fact that adding another variable tothe least squares equations must allow us to fit the training data (though not necessarily the testing data) more accurately.</li><li>The fact that adding <em>newspaper</em> advertising to the model containing only TV and radio advertising leads to just a tiny increase in R2 provides additional evidence that newspaper can be dropped from the model.</li></ul><h4 id="rse">RSE</h4><p><strong>RSE</strong> is defined as <span class="math display">\[\begin{align}RSE=\sqrt{\frac{RSS}{n-p-1}}\end{align}\]</span> Models with more variables can have higher RSE if the decrease in RSS is small relative to the increase in p.</p><h4 id="graphical-summaries">Graphical summaries</h4><p><img src="./7_v3.png" width="600"> It suggests a <strong>synergy</strong> or <strong>interaction</strong> effect between the advertising media, whereby combining the media together results in a bigger boost to sales than using any single medium</p><h3 id="given-a-set-of-predictor-values-what-response-value-should-we-predict-and-how-accurate-is-our-prediction">5. Given a set of predictor values, what response value should we predict, and how accurate is our prediction?</h3><h4 id="uncertainty-associated-with-prediction"><strong>Uncertainty associated with prediction</strong></h4><ol type="1"><li>The coefficient estimates <span class="math inline">\(\hat{\beta_0},\hat{\beta_1},...,\hat{\beta_p}\)</span> are estimates for <span class="math inline">\(β_0, β_1, . . . , β_p\)</span>. That is, the <strong>least squares plane</strong> <span class="math display">\[\begin{align}\hat{Y}=\hat{\beta_0}+\hat{\beta_1}X_1+,...+\hat{\beta_p}X_p\end{align}\]</span> is only an estimate for the <strong>true population regression plane</strong> <span class="math display">\[\begin{align}f(X)=\beta_0+\beta_1X_1+,...+\beta_pX_p\end{align}\]</span></li></ol><ul><li>The inaccuracy in the coefficient estimates is related to the <strong>reducible error</strong>.</li><li>We can compute a <strong>confidence interval</strong> in order to determine how close <span class="math inline">\(\hat{Y}\)</span> will be to f(X).</li></ul><ol start="2" type="1"><li>In practice assuming a linear model for <span class="math inline">\(f(X)\)</span> is almost always an approximation of reality, so there is an additional source of potentially <strong>reducible error</strong> which we call <strong>model bias</strong>.</li><li>Even if we knew <span class="math inline">\(f(X)\)</span>—true values for <span class="math inline">\(β_0, β_1, . . . , β_p\)</span>—the response value cannot be predicted perfectly because of the random error <span class="math inline">\(\epsilon\)</span> --<strong>irreducible error</strong>.</li></ol><ul><li>How much will Y vary from <span class="math inline">\(\hat{Y}\)</span>? -- <strong>prediction intervals</strong></li></ul><h4 id="prediction-intervals"><strong>Prediction intervals</strong></h4><p><strong>Prediction intervals</strong> are always wider than <strong>confidence intervals</strong></p><ul><li>Because they incorporate both <em>the error in the estimate for f(X) (the reducible error) and the uncertainty as to how much an individual point will differ from the population regression plane (the irreducible error).</em></li></ul><p>E.g.</p><ul><li><strong>confidence interval</strong> : quantify the uncertainty surrounding the average sales over a large number of cities.</li><li><strong>prediction interval</strong> : quantify the uncertainty surrounding sales for a particular city.</li></ul><h2 id="multiple-regression-from-simple-univariate-regression">Multiple Regression from Simple Univariate Regression</h2><p>Gram-Schmidt正交化</p><h3 id="simple-univariate-regression">Simple Univariate Regression</h3><p>Suppose ﬁrst that we have a univariate model with no intercept: <span class="math display">\[\begin{align}\mathbf{Y}=\mathbf{X}\beta+\epsilon\end{align}\]</span> The least squares estimate and residuals are: <span class="math display">\[\begin{align}\hat{\beta}&amp;=\frac{\sum_1^Nx_iy_i}{\sum_1^Nx_i^2} \\r_i&amp;=y_i-x_i\hat{\beta}\end{align}\]</span> The least squares estimate and residuals are: <span class="math display">\[\begin{align}\hat{\beta}&amp;=\frac{\sum_1^Nx_iy_i}{\sum_1^Nx_i^2} \\r_i&amp;=y_i-x_i\hat{\beta}\end{align}\]</span> Let <span class="math inline">\(\mathbf{y}=(y_1,...,y_N)^T\)</span>,<span class="math inline">\(\mathbf{x}=(x_1,...,x_N)^T\)</span></p><p>Define the <strong>inner product</strong> between <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>: <span class="math display">\[\begin{align}&lt;\mathbf{x},\mathbf{y}&gt;=\sum_1^Nx_iy_i=\mathbf{x}^T\mathbf{y}\end{align}\]</span> Then, <span class="math display">\[\begin{align}\hat{\beta}&amp;=\frac{&lt;\mathbf{x},\mathbf{y}&gt;}{&lt;\mathbf{x},\mathbf{x}&gt;} \\\mathbf{r}&amp;=\mathbf{y}-\mathbf{x}\hat{\beta}\end{align}\]</span> Suppose the inputs <span class="math inline">\(x_1, x_2,..., x_p\)</span> (the columns of the data matrix <span class="math inline">\(\mathbf{X}\)</span>) are orthogonal; that is <span class="math inline">\(&lt;\mathbf{x_k},\mathbf{x_j}&gt;=0\)</span>. Then the multiple least squares estimates <span class="math inline">\(\hat{\beta_j}\)</span> are equal to <span class="math inline">\(\frac{&lt;\mathbf{x_j},\mathbf{y}&gt;}{&lt;\mathbf{x_j},\mathbf{x_j}&gt;}\)</span>—the univariate estimates. In other words, <font color="red">when the inputs are orthogonal, they have no eﬀect on each other’s parameter estimates in the model.</font></p><h3 id="orthogonalization">Orthogonalization</h3><p><span class="math display">\[\begin{align}\hat{\beta}_1&amp;=\frac{&lt;\mathbf{x}-\bar{x}\mathbf{1},\mathbf{y}&gt;}{&lt;\mathbf{x}-\bar{x}\mathbf{1},\mathbf{x}-\bar{x}\mathbf{1}&gt;} \\\end{align}\]</span></p><ul><li><span class="math inline">\(\bar{x}=\sum_ix_i/N\)</span>;</li><li><span class="math inline">\(\mathbf{1}\)</span>, the vector of N ones;</li></ul><p><strong>Steps:</strong> 1. regress <span class="math inline">\(\mathbf{x}\)</span> on <span class="math inline">\(\mathbf{1}\)</span> to produce the residual <span class="math inline">\(\mathbf{z}=\mathbf{x}-\bar{x}\mathbf{1}\)</span>; 2. regress <span class="math inline">\(\mathbf{y}\)</span> on the residual <span class="math inline">\(\mathbf{z}\)</span> to give the coeﬃcient <span class="math inline">\(\hat{\beta}_1\)</span></p><p>Regress <span class="math inline">\(\mathbf{a}\)</span> on <span class="math inline">\(\mathbf{b}\)</span> (<span class="math inline">\(\mathbf{b}\)</span> is adjusted for <span class="math inline">\(\mathbf{a}\)</span>),(or <span class="math inline">\(\mathbf{b}\)</span> is <strong>“orthogonalized”</strong> with respect to <span class="math inline">\(\mathbf{a}\)</span>); a simple univariate regression of <span class="math inline">\(\mathbf{b}\)</span> on a with no intercept, producing coeﬃcient <span class="math inline">\(\hat{\lambda}=\frac{&lt;\mathbf{a},\mathbf{b}&gt;}{&lt;\mathbf{a},\mathbf{a}&gt;}\)</span> and residual vector $ - $.</p><p>The orthogonalization does not change the subspace spanned by <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, it simply produces an <strong>orthogonal basis</strong> for representing it.</p><h3 id="gramschmidt-procedure-for-multiple-regression">Gram–Schmidt procedure for multiple regression</h3><p><strong>ALGORITHM 3.1 Regression by Successive Orthogonalization</strong></p><ol type="1"><li>Initialize <span class="math inline">\(\mathbf{z_0}=\mathbf{x_0}=\mathbf{1}\)</span>.</li><li>For j=1,2,...,1,,...,p,<br> Regress <span class="math inline">\(\mathbf{x_j}\)</span> on <span class="math inline">\(\mathbf{z_0},\mathbf{z_1},...,\mathbf{z_{j-1}}\)</span> to produce coeﬃcients <span class="math inline">\(\hat{\lambda}_{l,j}=\frac{&lt;\mathbf{z_l},\mathbf{x_j}&gt;}{&lt;\mathbf{z_l},\mathbf{z_l}&gt;}\)</span>, l=0,1,...,j-1, and residual vector <span class="math inline">\(\mathbf{z_j}=\mathbf{x_j}-\sum_{k=0}^{j-1}\hat{\lambda_{kj}}\mathbf{z_k}\)</span></li><li>Regress <span class="math inline">\(\mathbf{y}\)</span> on the residual <span class="math inline">\(\mathbf{z_p}\)</span> to give the estimate <span class="math inline">\(\hat{\beta_p}=\frac{&lt;\mathbf{z_p},\mathbf{y}&gt;}{&lt;\mathbf{z_p},\mathbf{z_p}&gt;}\)</span>.</li></ol><p><strong>Note:</strong></p><ul><li>Each of the <span class="math inline">\(\mathbf{x}_j\)</span> is a linear combination of the <span class="math inline">\(\mathbf{z}_k\)</span>, <span class="math inline">\(k ≤ j\)</span>.</li><li>Since the <span class="math inline">\(\mathbf{z}_j\)</span> are all orthogonal, they form a basis for the column space of <span class="math inline">\(\mathbf{X}\)</span>, and hence the least squares projection onto this subspace is <span class="math inline">\(\mathbf{\hat{y}}\)</span>.</li><li>By rearranging the <span class="math inline">\(x_j\)</span> , any one of them could be in the last position, and a similar results holds.</li><li>The multiple regression coeﬃcient <span class="math inline">\(\mathbf{x}_j\)</span> represents the additional contribution of <span class="math inline">\(\mathbf{x}_j\)</span> on <span class="math inline">\(\mathbf{y}\)</span>, after <span class="math inline">\(\mathbf{x}_j\)</span> has been adjusted for <span class="math inline">\(x_0, x_1,..., x_{j−1},x_{j+1},..., x_p\)</span>.</li></ul><h3 id="precision-of-coefficient-estimation">Precision of Coefficient Estimation</h3><p>If <span class="math inline">\(\mathbf{x}_p\)</span> is highly correlated with some of the other <span class="math inline">\(\mathbf{x}_k\)</span>’s, the residual vector <span class="math inline">\(\mathbf{z}_p\)</span> will be close to zero, and the coeﬃcient <span class="math inline">\(\mathbf{x}_j\)</span> will be very unstable.</p><p>From <span class="math inline">\(\hat{\beta_p}=\frac{&lt;\mathbf{z_p},\mathbf{y}&gt;}{&lt;\mathbf{z_p},\mathbf{z_p}&gt;}\)</span>, we also obtain an alternate formula for the variance estimates:</p><p><span class="math display">\[\begin{align}Var(\hat{\beta}_p)=\frac{\sigma^2}{&lt;\mathbf{z_p},\mathbf{z_p}&gt;}=\frac{\sigma^2}{||\mathbf{z_p}||^2} \end{align}\]</span></p><p>The precision with which we can estimate <span class="math inline">\(\hat{\beta_p}\)</span> depends on the length of the residual vector <span class="math inline">\(\mathbf{z_p}\)</span>; this represents how much of <span class="math inline">\(\mathbf{x_p}\)</span> is unexplained by the other <span class="math inline">\(\mathbf{x_k}\)</span>’s</p><h3 id="qr-decomposition">QR decomposition</h3><p>We can represent step 2 of Algorithm 3.1 in matrix form:</p><p><span class="math display">\[\begin{align}\mathbf{X}=\mathbf{Z}\mathbf{Γ}\end{align}\]</span></p><ul><li><span class="math inline">\(\mathbf{Z}\)</span> has as columns the <span class="math inline">\(\mathbf{z_j}\)</span> (in order)</li><li><span class="math inline">\(\mathbf{Γ}\)</span> is the upper triangular matrix with entries <span class="math inline">\(\hat{\lambda}_{kj}\)</span></li></ul><p>Introducing the diagonal matrix D with jth diagonal entry <span class="math inline">\(D_{jj} = ||\mathbf{z_j}||\)</span>, we get</p><p><strong>QR decomposition of X</strong>:</p><p><span class="math display">\[\begin{align}\mathbf{X}=\mathbf{Z}\mathbf{D}^{-1}\mathbf{D}\mathbf{Γ}=\mathbf{Q}\mathbf{R}\end{align}\]</span></p><ul><li><span class="math inline">\(\mathbf{Q}\)</span> is an <span class="math inline">\(N ×(p+1)\)</span> orthogonal matrix, <span class="math inline">\(Q^TQ=I\)</span>;</li><li><span class="math inline">\(\mathbf{R}\)</span> is a $(p +1) × (p + 1) $Vupper triangular matrix.</li></ul><p><strong>Least squares solution:</strong> <span class="math display">\[\begin{align}\hat{\beta}&amp;=R^{-1}Q^T\mathbf{y} \\\mathbf{\hat{y}}&amp;=QQ^T\mathbf{y}\end{align}\]</span></p><h2 id="multiple-outputs">3.2.4 Multiple Outputs</h2><p>Suppose we have multiple outputs Y1,Y2,...,YK that we wish to predict from our inputs X0,X1,X2,...,Xp. We assume a linear model for each output:</p><p><span class="math display">\[\begin{align}Y_k&amp;=\beta_{0k}+\sum_{j=1}^pX_j\beta_{jk}+\epsilon_k \\&amp;=f_k(X)+\epsilon_k\end{align}\]</span></p><p>With N training cases we can write the model in matrix notation:</p><ul><li><p><span class="math display">\[\begin{align}Y=XB+E\end{align}\]</span></p></li><li><p>Y: N×K response matrix</p></li><li><p>X: N×(p+1) input matrix</p></li><li><p>B: (p+1)× K matrix of parameters</p></li><li><p>E: N×K matrix of errors</p></li></ul><p>A straightforward generalization of the univariate loss function:</p><p><span class="math display">\[\begin{align}RSS(B)&amp;=\sum_{k=1}^K\sum_{i=1}^N(y_{ik}-f_k(x_i))^2 \\&amp;=tr[(Y-XB)^T(Y-XB)]\end{align}\]</span> The least squares estimates have exactly the same form as before: <span class="math display">\[\begin{align}\hat{B}=(X^TX)^{-1}X^Ty\end{align}\]</span> If the errors <span class="math inline">\(\epsilon =(\epsilon_1,...,\epsilon_K)\)</span> in are correlated, suppose <span class="math inline">\(Cov(\epsilon)= Σ\)</span>, then the multivariate weighted criterion: <span class="math display">\[\begin{align}RSS(B;Σ)&amp;=\sum_{i=1}^N(y_{ik}-f_k(x_i))^TΣ^{-1}(y_{ik}-f_k(x_i)) \end{align}\]</span></p><h1 id="comparison-of-linear-regression-with-k-nearest-neighbors">Comparison of Linear Regression with K-Nearest Neighbors</h1><h2 id="parametric-v.s.-non-parametric">Parametric v.s. Non-parametric</h2><p>Linear regression is an example of a parametric approach because it assumes a linear functional form for f(X).</p><p><strong>Parametric methods</strong></p><ul><li><strong>Advantages</strong>:</li><li>Easy to fit, because one need estimate only a small number of coefficients.</li><li>Simple interpretations, and tests of statistical significance can be easily performed</li><li><strong>Disadvantage</strong>:</li><li>Strong assumptions about the form of f(X). If the specified functional form is far from the truth, and prediction accuracy is our goal, then the parametric method will perform poorly.</li></ul><p><strong>Non-parametric methods</strong></p><ul><li>Do not explicitly assume a parametric form for f(X), and thereby provide an alternative and more flexible approach for performing regression.</li><li>K-nearest neighbors regression (KNN regression)</li></ul><h2 id="knn-regression">KNN Regression</h2><p>Given a value for <span class="math inline">\(K\)</span> and a prediction point <span class="math inline">\(x_0\)</span>, KNN regression first identifies the <span class="math inline">\(K\)</span> training observations that are closest to <span class="math inline">\(x_0\)</span>, represented by <span class="math inline">\(N_0\)</span>. It then estimates <span class="math inline">\(f(x_0)\)</span> using the average of all the training responses in <span class="math inline">\(N_0\)</span>. <span class="math display">\[\begin{align}\hat{f}(x_0)=\frac{1}{K}\sum_{x_i\in N_0}y_i\end{align}\]</span></p><ul><li>The optimal value for K will depend on the <strong>bias-variance trade-off</strong>.</li><li>A small value for K provides the most flexible fit, which will have low bias but high variance. This variance is due to the fact that the prediction in a given region is entirely dependent on just one observation.</li><li>A larger values of K provide a smoother and less variable fit; the prediction in a region is an average of several points, and so changing one observation has a smaller effect. However, the smoothing may cause bias by masking some of the structure in f(X)</li></ul><p><strong>The parametric approach will outperform the nonparametric approach if the parametric form that has been selected is close to the true form of f.</strong></p><ul><li>A non-parametric approach incurs a cost in variance that is not offset by a reduction in bias</li><li>KNN performs slightly worse than linear regression when the relationship is linear, but much better than linear regression for non-linear situations. <img src="./26.png" width="600"></li></ul><p><strong>The increase in dimension has only caused a small deterioration in the linear regression test set MSE, but it has caused more than a ten-fold increase in the MSE for KNN.</strong></p><ul><li>This decrease in performance as the dimension increases is a common problem for KNN, and results from the fact that in higher dimensions there is effectively a reduction in sample size.<span class="math inline">\(\Rightarrow\)</span> <strong>curse of dimensionality</strong></li><li>As a general rule, parametric methods will tend to outperform non-parametric approaches when there is a small number of observations per predictor. <img src="./25.png" width="600"></li></ul><hr><p><strong>Ref:</strong></p><p>James, Gareth, et al. <em>An introduction to statistical learning</em>. Vol. 112. New York: springer, 2013.</p><p>Hastie, Trevor, et al. &quot;The elements of statistical learning: data mining, inference and prediction.&quot; <em>The Mathematical Intelligencer</em> 27.2 (2005): 83-85</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;simple-linear-regression-models&quot;&gt;Simple Linear Regression Models&lt;/h1&gt;
&lt;h2 id=&quot;linear-regression-model&quot;&gt;Linear Regression Model&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Form of the linear regression model: &lt;em&gt;&lt;span class=&quot;math inline&quot;&gt;\(f(X)=\beta_{0}+\sum_{j=1}^{p}X_{j}\beta_{j}\)&lt;/span&gt;&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Training data: (&lt;span class=&quot;math inline&quot;&gt;\(x_1\)&lt;/span&gt;,&lt;span class=&quot;math inline&quot;&gt;\(y_1\)&lt;/span&gt;) ... (&lt;span class=&quot;math inline&quot;&gt;\(x_N\)&lt;/span&gt;,&lt;span class=&quot;math inline&quot;&gt;\(y_N\)&lt;/span&gt;). Each &lt;span class=&quot;math inline&quot;&gt;\(x_{i} =(x_{i1},x_{i2},...,x_{ip})^{T}\)&lt;/span&gt; is a vector of feature measurements for the &lt;span class=&quot;math inline&quot;&gt;\(i\)&lt;/span&gt;-th case.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Goal: estimate the parameters &lt;span class=&quot;math inline&quot;&gt;\(β\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Estimation method: &lt;strong&gt;Least Squares&lt;/strong&gt;, we pick the coeﬃcients &lt;span class=&quot;math inline&quot;&gt;\(β =(β_0,β_1,...,β_p)^{T}\)&lt;/span&gt; to minimize the &lt;strong&gt;residual sum of squares&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Assumptions:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Observations &lt;span class=&quot;math inline&quot;&gt;\(y_i\)&lt;/span&gt; are uncorrelated and have constant variance &lt;span class=&quot;math inline&quot;&gt;\(\sigma^2\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(x_i\)&lt;/span&gt; are ﬁxed (non random)&lt;/li&gt;
&lt;li&gt;The regression function E(Y |X) is linear, or the linear model is a reasonable approximation.&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Linear Regression" scheme="https://nancyyanyu.github.io/tags/Linear-Regression/"/>
    
      <category term="Regression" scheme="https://nancyyanyu.github.io/tags/Regression/"/>
    
  </entry>
  
  <entry>
    <title>August 2019 | 八月日志</title>
    <link href="https://nancyyanyu.github.io/posts/8e8e46fb/"/>
    <id>https://nancyyanyu.github.io/posts/8e8e46fb/</id>
    <published>2019-09-07T20:11:24.000Z</published>
    <updated>2019-09-07T22:37:31.085Z</updated>
    
    <content type="html"><![CDATA[<p> </p><a id="more"></a><center><b>❤ 8月手帳 ❤</b></center><table><colgroup><col style="width: 6%"><col style="width: 82%"><col style="width: 10%"></colgroup><thead><tr class="header"><th style="text-align: left;">Date</th><th>Study</th><th>Workout</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">08/01</td><td>Work with economy / news api; write news streaming module and economy data visualization, geomap</td><td>/</td></tr><tr class="even"><td style="text-align: left;">08/02</td><td>Write news streaming code</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">08/03</td><td>Learn docker, write dockerfile</td><td>/</td></tr><tr class="even"><td style="text-align: left;">08/04</td><td>Write dockerfile</td><td>PingPong</td></tr><tr class="odd"><td style="text-align: left;">08/05</td><td>Write dockerfile; buy a domain name; deploy the web application on server</td><td>/</td></tr><tr class="even"><td style="text-align: left;">08/06</td><td>Test S&amp;P500 price streaming during trading hours; debug;</td><td>PingPong</td></tr><tr class="odd"><td style="text-align: left;">08/07</td><td><strong><em>Start job hunting and no more update on this page (but I will keep coding and practicing LeetCode SQL)</em></strong></td><td>/</td></tr></tbody></table><p> </p><p> </p><center><b>❤ Plan ❤</b></center><table><colgroup><col style="width: 12%"><col style="width: 74%"><col style="width: 12%"></colgroup><thead><tr class="header"><th style="text-align: center;">Categories</th><th style="text-align: center;">Content</th><th style="text-align: center;">Progress</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">Project</td><td style="text-align: center;">Realtime financial market data visualization and analysis</td><td style="text-align: center;">■■■■■■■■■□</td></tr></tbody></table><p> </p><p> </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; &lt;/p&gt;
    
    </summary>
    
      <category term="Journal" scheme="https://nancyyanyu.github.io/categories/Journal/"/>
    
    
      <category term="Journal" scheme="https://nancyyanyu.github.io/tags/Journal/"/>
    
  </entry>
  
  <entry>
    <title>Realtime Financial Market Data Visualization and Analysis</title>
    <link href="https://nancyyanyu.github.io/posts/109fc1d1/"/>
    <id>https://nancyyanyu.github.io/posts/109fc1d1/</id>
    <published>2019-08-05T22:36:32.000Z</published>
    <updated>2019-10-20T01:51:45.596Z</updated>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>In this project, I developed a financial data processing and visualization platform using <strong><u>Apache Kafka</u></strong>, <u><strong>Apache Cassandra</strong></u>, and <strong><u>Bokeh</u></strong>. I used Kafka for realtime stock price and market news streaming, Cassandra for historical and realtime stock data warehousing, and Bokeh for visualization on web browsers. I also wrote a web crawler to scrape companys' financial statements and basic information from Yahoo Finance, and played with various economy data APIs.</p><p><em>Please check the <a href="https://github.com/nancyyanyu/kafka_stock" target="_blank" rel="noopener">GitHub repo</a> of this project, and <strong>most importantly, please check this platform's website and play with each plot</strong> <span class="math inline">\(\rightarrow\)</span> <a href="http://magiconch.me/" target="_blank" rel="noopener">magiconch.me</a></em></p><a id="more"></a><p> </p><h1 id="architecture">Architecture</h1><p>There are currently 3 tabs in the webpage:</p><ul><li><strong><em>Stock: Streaming &amp; Fundamental</em></strong><ul><li>Single stock's candlestick, basic company &amp; financial information;</li><li>Realtime S&amp;P500 price during trading hours (<em>fake date</em> during non-trading hours)</li></ul></li><li><strong><em>Stock: Comparison</em></strong><ul><li>2 user-selected stocks' price, and their statstical summay &amp; correlation</li><li>5,10,30-day moving average of adjusted close price</li></ul></li><li><strong><em>Economy</em></strong><ul><li>Geomap of various economy data by state</li><li>Economy indicators's plot</li><li>Latest market news</li></ul></li></ul><p> </p><p>Here is the architecture of the platform.</p><p><img src="./kafka_stock.png" width="900"></p><p>Please check each tab's screenshot:</p><p><strong>Tab 1:</strong></p><p><img src="./tab1.gif" width="800"></p><p><strong>Tab 2:</strong></p><p><img src="./tab2.gif" width="800"></p><p><strong>Tab 3:</strong></p><p><img src="./tab3.gif" width="800"></p><h2 id="stock-streaming-fundamental">1 Stock: Streaming &amp; Fundamental</h2><h3 id="data-source">1.1 Data Source</h3><ul><li><a href="https://www.alphavantage.co/" target="_blank" rel="noopener">Alpha Vantage</a> : provide free APIs for realtime and historical data on stocks.<ul><li>Problems: rate limiting of 5 calls per minute, 500 calls per day for free account .</li></ul></li><li><a href="https://finance.yahoo.com/" target="_blank" rel="noopener">Yahoo Finance</a>: write a web crawler to get company's summary profile and fundamental information such as financial statements from Yahoo Finance.</li></ul><h3 id="etl">1.2 ETL</h3><h4 id="historical-data">1. Historical data</h4><p>In the first tab, both historical data and streaming data are presented. Before each trading day, I got every stocks' historical data from <strong><em>Alpha Vantage</em></strong> to update the newest daily price and volume.</p><p>The raw data is in <em>string</em> type, the transformation of the data type needs to be performed. I also standardized 'time' column to &quot;%Y-%m-%d %H:%M:%S&quot; because that's the datetime format Cassandra could recognize.</p><p>Each stock has three tables that stored its daily, 1-minute, and 18-second frequency's OHLCV in <strong><em>Apache Cassandra</em></strong>. Cassandra has its own query language <strong><em>Cassandra Query Language (CQL)</em></strong> which is quite similar to SQL.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE IF NOT EXISTS SYMBOL_historical ( </span><br><span class="line">       TIME timestamp,           </span><br><span class="line">       SYMBOL text,              </span><br><span class="line">       OPEN float,               </span><br><span class="line">       HIGH float,               </span><br><span class="line">       LOW float,                </span><br><span class="line">       CLOSE float,              </span><br><span class="line">       ADJUSTED_CLOSE float,     </span><br><span class="line">       VOLUME float,             </span><br><span class="line">       dividend_amount float,    </span><br><span class="line">       split_coefficient float,  </span><br><span class="line">       PRIMARY KEY (SYMBOL,TIME));</span><br></pre></td></tr></table></figure><h4 id="realtime-data">2. Realtime data</h4><p>I used Apache Kafka to handle streaming data, the stream plot would combine both historical as well as the streaming data for visualization. The <em>topic</em> for this streaming task is <em>'stock_streaming1'</em>.</p><p>This <em>topic</em> has two producers, which recevied fake data generated by random number during non-trading hours and realtime S&amp;P500 price and volume from <strong><em>Alpha Vantage</em></strong> during trading hours. It also has two consumers, which send data to Cassandra database and to a local json file.</p><p><img src="./stream1.png" width="600"></p><p><strong>NOTE:</strong></p><p>Since <strong><em>Alpha Vantage</em></strong> could only be called 5 times per minute, so I could either get one minute frequency data by calling <em>TIME_SERIES_INTRADAY</em> function provided by <strong><em>Alpha Vantage</em></strong> every minute , or to get data faster, call <em>GLOBAL_QUOTE</em> function every 18 seconds to get the latest price and volume information, while avoiding rate limiting error.</p><p>One other thing to note is <strong>timezone</strong>. I am in Pacific Time. To align with Eastern Time trading hours, I need to record the time I called <em>GLOBAL_QUOTE</em>, adjust to Eastern Time, and store accordingly to the database.</p><p>Interestingly, Cassandra stores datetime type data in UTC timezone. When querying data from Cassandra, we need to double check whether the time changed. Otherwise, the plot would not seamlessly combine historical data and streaming data.</p><h4 id="fundatmental-data">3. Fundatmental data</h4><p>I wrote a script to crawl the financial statements and summary profile given the company's symbol, and loaded each company's fundamental data to a json file.</p><p><img src="./fundamental.png" width="700"></p><p> </p><h2 id="stock-comparison">2 Stock: Comparison</h2><p>The second tab only uses historical daily frequency data, extracted from Cassandra database. It aims to compare two stocks. So I plot 2 selected stocks' adjusted close and volume, along with 3 technical indicators: 5MA, 10MA, 30MA; as well as the statistical summay and correlation of these 2 stocks.</p><p>You can also click on the legend entries to hide technical indicators.</p><p><img src="./click.png" width="700"></p><h2 id="economy">3 Economy</h2><h3 id="data-source-1">3.1 Data Source</h3><ul><li><a href="https://www.bea.gov/" target="_blank" rel="noopener">The U.S. Bureau of Economic Analysis</a>: provide API to access to BEA published economic statistics.</li><li><a href="https://fred.stlouisfed.org/" target="_blank" rel="noopener">The Federal Reserve Bank of St. Louis</a>: provide API that has been integrated into <em>pandas_datareader</em></li><li><a href="https://newsapi.org/" target="_blank" rel="noopener">NEWS API</a>: provide breaking news headlines by categories or keywords</li></ul><h3 id="etl-1">3.2 ETL</h3><h4 id="economy-data">1. Economy data</h4><p>Firstly I went through websites of <strong><em>BEA</em></strong> and <strong><em>FRED</em></strong>, and identified the key economy data to present, such as real GDP, CPI, unemployment rate, and etc. <em>BEA</em> provides economy data by state, <em>FRED</em> provides nationwide economy indicators. Then I decided to combine geomap using data from <em>BEA</em> to present the difference across the states, and time series plots using data from <em>FRED</em> to present the change of key economy indicators. The economy data are all stored in json files.</p><p>To plot geomap, I downloaded geographical boundaries of U.S. states <a href="http://econym.org.uk/gmap/states.xml" target="_blank" rel="noopener">here</a>, and fed into <strong>Bokeh</strong>.</p><h4 id="business-news">2. Business news</h4><p><strong><em>NEWS API</em></strong> is a really interesting and useful tool to get up-to-date news headlines given categories or keywords.</p><p><img src="./news.png" width="600"></p><p>Here I used <strong><em>Kafka</em></strong> to stream news data using <em>topic</em> <strong><em>NEWS</em></strong> and stored in <strong><em>Cassandra</em></strong>.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE IF NOT EXISTS NEWS ( </span><br><span class="line">       DATE date, </span><br><span class="line">       publishedAt timestamp,           </span><br><span class="line">       TITLE text,              </span><br><span class="line">       SOURCE text,               </span><br><span class="line">       description text,               </span><br><span class="line">       url text, PRIMARY KEY (DATE,publishedAt)) </span><br><span class="line">       WITH CLUSTERING ORDER BY (publishedAt ASC);&quot;)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;In this project, I developed a financial data processing and visualization platform using &lt;strong&gt;&lt;u&gt;Apache Kafka&lt;/u&gt;&lt;/strong&gt;, &lt;u&gt;&lt;strong&gt;Apache Cassandra&lt;/strong&gt;&lt;/u&gt;, and &lt;strong&gt;&lt;u&gt;Bokeh&lt;/u&gt;&lt;/strong&gt;. I used Kafka for realtime stock price and market news streaming, Cassandra for historical and realtime stock data warehousing, and Bokeh for visualization on web browsers. I also wrote a web crawler to scrape companys&#39; financial statements and basic information from Yahoo Finance, and played with various economy data APIs.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Please check the &lt;a href=&quot;https://github.com/nancyyanyu/kafka_stock&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub repo&lt;/a&gt; of this project, and &lt;strong&gt;most importantly, please check this platform&#39;s website and play with each plot&lt;/strong&gt; &lt;span class=&quot;math inline&quot;&gt;\(\rightarrow\)&lt;/span&gt; &lt;a href=&quot;http://magiconch.me/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;magiconch.me&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Project" scheme="https://nancyyanyu.github.io/categories/Project/"/>
    
    
      <category term="Bokeh" scheme="https://nancyyanyu.github.io/tags/Bokeh/"/>
    
      <category term="Kafka" scheme="https://nancyyanyu.github.io/tags/Kafka/"/>
    
      <category term="Cassandra" scheme="https://nancyyanyu.github.io/tags/Cassandra/"/>
    
  </entry>
  
  <entry>
    <title>July 2019 | 七月日志</title>
    <link href="https://nancyyanyu.github.io/posts/445d7012/"/>
    <id>https://nancyyanyu.github.io/posts/445d7012/</id>
    <published>2019-08-02T16:38:59.000Z</published>
    <updated>2019-08-02T19:03:41.286Z</updated>
    
    <content type="html"><![CDATA[<p> </p><a id="more"></a><p> </p><center><img src="./allen2.jpg" width="300"></center><p> </p><center><b>❤ 7月手帳 ❤</b></center><table><colgroup><col style="width: 6%"><col style="width: 81%"><col style="width: 12%"></colgroup><thead><tr class="header"><th style="text-align: left;">Date</th><th>Study</th><th>Workout</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">07/01</td><td>Job Hunting</td><td>/</td></tr><tr class="even"><td style="text-align: left;">07/02</td><td>Job Hunting</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">07/03</td><td>Job Hunting</td><td>/</td></tr><tr class="even"><td style="text-align: left;">07/04</td><td>Learn pyspark streaming</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">07/05</td><td>Learn pyspark streaming</td><td>/</td></tr><tr class="even"><td style="text-align: left;">07/06</td><td>(Travel in Portland~~)</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">07/07</td><td>(Travel in Portland~~)</td><td>Hiking</td></tr><tr class="even"><td style="text-align: left;">07/08</td><td>Finish udemy's course <a href="https://www.udemy.com/apache-spark-streaming-with-python-and-pyspark/" target="_blank" rel="noopener">Apache Spark Streaming with Python and PySpark</a></td><td>/</td></tr><tr class="odd"><td style="text-align: left;">07/09</td><td>Write pyspark project code and blog;</td><td>/</td></tr><tr class="even"><td style="text-align: left;">07/10</td><td>Write twitter pyspark streaming project</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">07/11</td><td>Coffee chat and write email all day</td><td>/</td></tr><tr class="even"><td style="text-align: left;">07/12</td><td>Work on new spotify project</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">07/13</td><td>(/<sub>/</sub>/<sub>nothing</sub>~~)</td><td>/</td></tr><tr class="even"><td style="text-align: left;">07/14</td><td>Coding for project; learn AWS DynamoDB, S3</td><td>Ping Pong</td></tr><tr class="odd"><td style="text-align: left;">07/15</td><td>Work on project; learn Apache Airflow, review Linear Regression</td><td>/</td></tr><tr class="even"><td style="text-align: left;">07/16</td><td>Write project report; learn Apache Airflow</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">07/17</td><td>Write project report</td><td>/</td></tr><tr class="even"><td style="text-align: left;">07/18</td><td>Learn Beautifulsoup, crawling and start new project</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">07/19</td><td>Write crawling code</td><td>Ping Pong</td></tr><tr class="even"><td style="text-align: left;">07/20</td><td>Crawling all day, deploy to EC2</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">07/21</td><td>Write feature extraction code; rewrite resume</td><td>/</td></tr><tr class="even"><td style="text-align: left;">07/22</td><td>Revise resume</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">07/23</td><td>Revise resume and review probability theory</td><td>/</td></tr><tr class="even"><td style="text-align: left;">07/24</td><td>Sending applications all day</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">07/25</td><td>Learn Kafka and Spark, start a new project (a full stack data engineering and analysis project with realtime data)</td><td>Ping Pong</td></tr><tr class="even"><td style="text-align: left;">07/26</td><td>Meeting, Research on new project</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">07/27</td><td>Learn Kafka, play with multiple realtime data API</td><td>/</td></tr><tr class="even"><td style="text-align: left;">07/28</td><td>Learn Cassandra and CQL</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">07/29</td><td>Study bokeh, write streaming visualization module</td><td>/</td></tr><tr class="even"><td style="text-align: left;">07/30</td><td>Crawl yahoo finance for realtime data</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">07/31</td><td>Study bokeh, write visualization module for project</td><td>Ping Pong</td></tr></tbody></table><p> </p><p> </p><center><b>❤ Plan ❤</b></center><table><colgroup><col style="width: 12%"><col style="width: 75%"><col style="width: 12%"></colgroup><thead><tr class="header"><th style="text-align: center;">Categories</th><th style="text-align: center;">Content</th><th style="text-align: center;">Progress</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">Project</td><td style="text-align: center;"><a href="https://nancyyanyu.github.io/posts/9fb5a802/">Realtime Twitter Data Analysis using Spark Streaming</a></td><td style="text-align: center;">■■■■■■■■■■</td></tr><tr class="even"><td style="text-align: center;">Project</td><td style="text-align: center;"><a href="https://nancyyanyu.github.io/posts/63adf3bb/">Data Analysis of K-POP: Playing with Spotify API</a></td><td style="text-align: center;">■■■■■■■■■■</td></tr><tr class="odd"><td style="text-align: center;">Project</td><td style="text-align: center;">Realtime financial market data visualization and analysis</td><td style="text-align: center;">■■■■■■□□□□</td></tr><tr class="even"><td style="text-align: center;">Project</td><td style="text-align: center;">Crawling project</td><td style="text-align: center;">■■■■■■■■■■</td></tr><tr class="odd"><td style="text-align: center;">Big Data</td><td style="text-align: center;">Udemy: <a href="https://www.udemy.com/apache-spark-streaming-with-python-and-pyspark/" target="_blank" rel="noopener">Apache Spark Streaming with Python and PySpark</a></td><td style="text-align: center;">■■■■■■■■■■</td></tr></tbody></table><p> </p><p> </p><center><b>❤ Some Spark Resources that I like to share~ ❤</b></center><ul><li>Youtube: <a href="https://www.youtube.com/watch?v=7ooZ4S7Ay6Y" target="_blank" rel="noopener">Advanced Apache Spark Training - Sameer Farooqui (Databricks)</a></li><li>Blog &amp; Github: <a href="%5Bhttps://github.com/NFLX-WIBD/WIBD-Workshops-2018/tree/master/Data%20Engineering%5D(https://github.com/NFLX-WIBD/WIBD-Workshops-2018/tree/master/Data%20Engineering)">Data Engineering Workshop 2018 from Netflix</a> &amp; <a href="https://medium.com/hasbrain/a-typical-data-engineering-project-sharing-from-netflix-data-engineering-team-cc27878fce55" target="_blank" rel="noopener">A Typical Data Engineering Project — Sharing From Netflix Data Engineering Team</a></li></ul><p> </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; &lt;/p&gt;
    
    </summary>
    
      <category term="Journal" scheme="https://nancyyanyu.github.io/categories/Journal/"/>
    
    
      <category term="Journal" scheme="https://nancyyanyu.github.io/tags/Journal/"/>
    
  </entry>
  
  <entry>
    <title>Data Analysis of K-POP: Playing with Spotify API</title>
    <link href="https://nancyyanyu.github.io/posts/63adf3bb/"/>
    <id>https://nancyyanyu.github.io/posts/63adf3bb/</id>
    <published>2019-07-16T16:08:09.000Z</published>
    <updated>2019-10-20T01:51:39.387Z</updated>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>K-pop has become a phenomenon in the U.S, as evidenced by bombing number of K-pop shows across the nation. In Spotify's K-pop genre, there are more than 500 K-pop artists. Among them, the wildly popular male group BTS is certainly worth mentioning. According to the data of Top 100 selling artists in the first half of 2019, out of 10 album copies sold out, there are 4 of BTS's albums <a href="https://www.allkpop.com/article/2019/07/bts-occupies-419-of-total-album-sales-of-top-100-kpop-artists" target="_blank" rel="noopener">(Ref)</a>.</p><p>BTS is certainly a reason for the rise of K-pop, but as more and more K-pop artists successfully hit the Billboard charts and took massive world tour, K-pop is definitely a hot topic to dig into.</p><p>This K-Pop sensation draws my attention to perform data analysis using data provided by <strong><em>Spotify API</em></strong> which could be used to answer some interesting questions like:</p><ul><li>Besides BTS, who are the best K-pop artists in the U.S. market?</li><li>How do their popularity change?</li><li>How did the market of K-pop music evolve in size and audio features?</li></ul><a id="more"></a><h1 id="setup">Setup</h1><h3 id="register-an-app"><strong>Register an app</strong>:</h3><p>Just like my last project which used Twitter API, the first step is to register an app on <a href="https://developer.spotify.com/" target="_blank" rel="noopener">Spotify for Developers</a>, then copy <em>Client ID</em> and <em>Client Secret</em>.</p><p><img src="./1.jpg" width="800"></p><p> </p><h3 id="install-spotify-web-api-python-library"><strong>Install Spotify Web API Python library:</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install spotipy</span><br></pre></td></tr></table></figure><p>Please check <strong><em>spotipy</em></strong>'s <a href="https://spotipy.readthedocs.io/en/latest/" target="_blank" rel="noopener">document</a> for more detail.</p><p> </p><h3 id="download-all-relevant-data-to-local-machine">Download all relevant data to local machine</h3><p>I firstly wrote a class <strong><em>MySpotify</em></strong> to grab all relevant data and stored the data under local machine in <em>json</em> format.</p><h4 id="relevant-data"><strong>Relevant data:</strong></h4><ul><li>All artists' information under 'k-pop' genre;</li><li>All albums' information of every k-pop artists</li><li>All tracks' information of all albums of every k-pop artists</li><li>Audio features provided by <em>Spotify</em> of all tracks</li></ul><h4 id="audio-features"><strong>Audio features:</strong></h4><p>There are 8 main audio features provided by <em>Spotify</em>. Here's the explanation from Spotify's <a href="https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/" target="_blank" rel="noopener">document</a>.</p><ul><li><strong><em>Danceability</em></strong> describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity.</li><li><strong>Energy</strong> is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity.</li><li><strong>Instrumentalness</strong> predicts whether a track contains no vocals.<br></li><li><strong>Liveness</strong> detects the presence of an audience in the recording.</li><li><strong>Loudness</strong>: the overall loudness of a track in decibels (dB).</li><li><strong>Speechiness</strong> detects the presence of spoken words in a track.</li><li><strong>Valence:</strong> A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track.</li><li><strong>Tempo:</strong> the overall estimated tempo of a track in beats per minute (BPM).</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpotify</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""This class was to download data from Spotify API.</span></span><br><span class="line"><span class="string">    The download process should be: </span></span><br><span class="line"><span class="string">        artist=artist_genre -&gt; album=album_artists(artist) -&gt; songs=songs_albums(album)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,genre=<span class="string">'k-pop'</span>)</span>:</span></span><br><span class="line">        self.authenticate()</span><br><span class="line">        self.genre=genre</span><br><span class="line">                </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">authenticate</span><span class="params">(self)</span>:</span></span><br><span class="line">        auth=open_json(<span class="string">'auth'</span>)</span><br><span class="line">        my_id = auth[<span class="string">'my_id'</span>]</span><br><span class="line">        secret_key = auth[<span class="string">'secret_key'</span>]</span><br><span class="line">        </span><br><span class="line">        client_credentials_manager = SpotifyClientCredentials(client_id = my_id, client_secret = secret_key)</span><br><span class="line">        self.sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)                </span><br><span class="line">        </span><br><span class="line">......</span><br></pre></td></tr></table></figure><p><a href="https://github.com/nancyyanyu/mini_projects/blob/master/spotify_project/spotify_app.py" target="_blank" rel="noopener">Full code</a></p><p> </p><h1 id="data-analysis">Data Analysis</h1><h3 id="who-are-the-most-popular-k-pop-artists">1. Who are the most popular K-pop artists?</h3><p>Before we jump to the answer, let me take a second to introduce two measures of prevelance in Spotify.</p><ul><li><strong>Followers:</strong> The number of people following the artist.</li><li><strong>Popularity:</strong> The popularity of the artist. The value will be between 0 and 100, with 100 being the most popular. The artist’s popularity is calculated from the popularity of all the artist’s tracks.</li></ul><p>There are cases when some artists have a large amount of followers, but relatively low popularity. That's possibly because they are not active in releasing new songs recently or simply retired from musical life.</p><p>To find the hottest K-pop artists, I extracted all artists' followers and popularity data, ranked them, and visualized top 40 artists in terms of the number of followers.</p><p><img src="./2.png" alt="Fig 1" width="800"></p><p>In <em>Fig 1</em>, for artists with <font color="orange">orange</font> bar higher than <font color="blue">blue</font> bar, some of them are hot, recently debuted K-pop groups, like <strong><em>(G)I-DLE, NCT 127, Stray Kids</em></strong>; most of them released new songs in recent month, e.g. <strong>Jay Park</strong> released his new EP &quot;Nothing Matters&quot; this month.</p><p>For artists with <font color="orange">orange</font> bar lower than <font color="blue">blue</font> bar, some of them are inactive for a long time, such as <strong>G-Dragon</strong> who is serving compulsory military duty; some of them are <em>temporary</em> group or solo which only released one or two albums, like <strong>EXO-CBX, J-hope</strong>.</p><p>Ranking could be sometimes misleading. As 4 out of 10 album copies sold out are BTS's albums, what if this K-pop phenomenon is actually BTS phenomenon? Let's check the numerical number of followers and popularity of these K-pop artists.</p><p><img src="./3.png" alt="Fig 2" width="800"></p><p><img src="./4.png" alt="Fig 3" width="800"></p><p>From <em>Fig 2 &amp; 3</em>, we can see that BTS has more than 1/3 followers than the second hottest group, and more than 1/2 followers than the third one. In addition, BTS has an popularity rate of 99, very close to 100, the highest possible popularity rate.</p><p>For comparison, I listed some top artists in the U.S. market:</p><table><thead><tr class="header"><th>Artists</th><th>Followers</th><th>Popularity</th></tr></thead><tbody><tr class="odd"><td>Ed Sheeran</td><td>47,295,649</td><td>100</td></tr><tr class="even"><td>Drake</td><td>38,673,834</td><td>99</td></tr><tr class="odd"><td>Taylor Swift</td><td>21,198,450</td><td>93</td></tr><tr class="even"><td>Ariana Grande</td><td>32,355,391</td><td>95</td></tr></tbody></table><p>Comparing the top K-pop artists and the top U.S. mainstream artists, except from BTS, other K-pop artists' could hardly be labelled as top stars in the U.S. Even BTS who have amazingly high popularity, they have significantly less followers.</p><p>In my point of view, before BTS rising to fame, K-pop is only a subculture in the U.S., and not usually depicted by western mainstream media. Today, whether K-pop has become a mainstream phenomenon is still open to debate. At least the far less followers of most of the K-pop artists is an evidence against this argument.</p><h3 id="how-does-popularity-change">2. How does popularity change?</h3><p>Since <em>Spotify</em> does not provide historical data of artists' popularity and followers, I write a class <strong><em>Popularity</em></strong> to grab everyday's data, upload to AWS DynamoDB.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Popularity</span><span class="params">(MySpotify)</span>:</span>    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,table_name=<span class="string">'KPOP_POPULARITY'</span>,genre=<span class="string">'k-pop'</span>)</span>:</span></span><br><span class="line">        super().__init__(genre)</span><br><span class="line">        self.table = dynamodb.Table(table_name)</span><br><span class="line">        self.track_table=dynamodb.Table(table_name+<span class="string">'_TRACK'</span>)</span><br><span class="line">        self.data_folder=<span class="string">'%s_artists_info'</span>%self.genre</span><br><span class="line">        self.today=str(dt.datetime.now().date())</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">put_data</span><span class="params">(self)</span>:</span></span><br><span class="line">        artists=open_json(<span class="string">"data/artists/"</span>+self.data_folder)            </span><br><span class="line">        info=&#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> artist <span class="keyword">in</span> artists:</span><br><span class="line">            info[artist[<span class="string">'name'</span>]]=&#123;&#125;</span><br><span class="line">            info[artist[<span class="string">'name'</span>]][<span class="string">'followers'</span>]=artist[<span class="string">'followers'</span>][<span class="string">'total'</span>]</span><br><span class="line">            info[artist[<span class="string">'name'</span>]][<span class="string">'popularity'</span>]=artist[<span class="string">'popularity'</span>]</span><br><span class="line">        </span><br><span class="line">        self.table.put_item(</span><br><span class="line">           Item=&#123;<span class="string">'time'</span>: self.today,</span><br><span class="line">               <span class="string">'info'</span>:info&#125;)</span><br><span class="line">        print(<span class="string">"Update artist data to AWS DynamoDB: &#123;&#125;"</span>.format(self.today))</span><br><span class="line">                </span><br><span class="line">......</span><br></pre></td></tr></table></figure><p><a href="https://github.com/nancyyanyu/mini_projects/blob/master/spotify_project/popularity.py" target="_blank" rel="noopener">Full code</a></p><p>The <strong><em>ETL process</em></strong> works like this:</p><center><img src="./6.png" width="540"></center><ol type="1"><li>Grab artists' data from <em>Spotify</em> API;</li><li>Organize and clean the data;</li><li>Save data to local machine in <em>json</em> format;</li><li>Save data to <strong>AWS DynamoDB</strong>.</li></ol><p>Here is a visualization of the change of BTS' followers &amp; popularity using data collected during writing this report.</p><p><img src="./5.png" alt="Fig 4" width="800"></p><h3 id="how-many-k-pop-albums-released-every-year">3. How many K-pop albums released every year?</h3><p>As an industry, K-pop has experienced significant increase in size over the past two decades. According to each music agent companies' marketing tactics, K-pop artitists will demonstrate certain pattern in terms of the timing of releasing new albums.</p><p>Here is a visualization of the number of albums released by year and by month. We can see a sharp increase from 2016 to 2018. Also artists seem to be more active during October and November.</p><p><img src="./7.png" width="700"></p><h3 id="audios-features-analysis">4. Audios' features analysis</h3><p>To analyze audio features, I wrote a class <strong><em>Analyze</em></strong> to wrap to ETL process and analysis functions.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Analyze</span><span class="params">(object)</span>:</span>       </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,genre=<span class="string">'k-pop'</span>)</span>:</span></span><br><span class="line">        self.features=[<span class="string">'acousticness'</span>,<span class="string">'liveness'</span>,          </span><br><span class="line">                  <span class="string">'instrumentalness'</span>,<span class="string">'speechiness'</span>, </span><br><span class="line">                   <span class="string">'danceability'</span>,<span class="string">'energy'</span>,<span class="string">'valence'</span>,<span class="string">'tempo'</span>]</span><br><span class="line">        self.poplr=Popularity()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">extract_data</span><span class="params">(self,artist)</span>:</span></span><br><span class="line">        df=pd.read_hdf(<span class="string">'./data/analysis/&#123;&#125;_tracks_analysis.h5'</span>.format(artist))</span><br><span class="line">        <span class="keyword">return</span> df</span><br><span class="line">......</span><br></pre></td></tr></table></figure><p><a href="https://github.com/nancyyanyu/mini_projects/blob/master/spotify_project/track_analysis.py" target="_blank" rel="noopener">Full code</a></p><p>The <strong><em>ETL process</em></strong> works like this:</p><center><img src="./10.png" width="540"></center><p>As last chapter says, I extracted 8 audio features of all tracks. I calculated the truncated average of each feature by year, and try to find some changes in the K-pop music itself.</p><p><img src="./8.png" width="800"></p><p>We can see features like <strong><em>acousticness</em></strong>, and <strong><em>instrumentalness</em></strong> have an upward trend; while <strong><em>energy</em></strong>, <strong><em>tempo</em></strong> and <strong><em>valence</em></strong> have un downward trend in recent years.</p><p>The evolution of K-pop music is not smooth. It seems it has experienced a radical shift around 2013 (2012-2014), as most of the features changed their disposition. After 2013 or 2014, the K-pop songs:</p><ul><li>Become more and more acoustic, and instrumental.</li><li>Contain more spoken words.</li><li>Convey less energy and positiveness. In other words, they sound less cheerful but more sad and depressed.</li><li>Become a bit slower in tempo.</li><li>Are still highly danceable, but not as danceable as before 2014.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;K-pop has become a phenomenon in the U.S, as evidenced by bombing number of K-pop shows across the nation. In Spotify&#39;s K-pop genre, there are more than 500 K-pop artists. Among them, the wildly popular male group BTS is certainly worth mentioning. According to the data of Top 100 selling artists in the first half of 2019, out of 10 album copies sold out, there are 4 of BTS&#39;s albums &lt;a href=&quot;https://www.allkpop.com/article/2019/07/bts-occupies-419-of-total-album-sales-of-top-100-kpop-artists&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;(Ref)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;BTS is certainly a reason for the rise of K-pop, but as more and more K-pop artists successfully hit the Billboard charts and took massive world tour, K-pop is definitely a hot topic to dig into.&lt;/p&gt;
&lt;p&gt;This K-Pop sensation draws my attention to perform data analysis using data provided by &lt;strong&gt;&lt;em&gt;Spotify API&lt;/em&gt;&lt;/strong&gt; which could be used to answer some interesting questions like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Besides BTS, who are the best K-pop artists in the U.S. market?&lt;/li&gt;
&lt;li&gt;How do their popularity change?&lt;/li&gt;
&lt;li&gt;How did the market of K-pop music evolve in size and audio features?&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Project" scheme="https://nancyyanyu.github.io/categories/Project/"/>
    
    
      <category term="AWS" scheme="https://nancyyanyu.github.io/tags/AWS/"/>
    
      <category term="Airflow" scheme="https://nancyyanyu.github.io/tags/Airflow/"/>
    
      <category term="Visualization" scheme="https://nancyyanyu.github.io/tags/Visualization/"/>
    
  </entry>
  
  <entry>
    <title>Realtime Twitter Data Analysis using Spark Streaming</title>
    <link href="https://nancyyanyu.github.io/posts/9fb5a802/"/>
    <id>https://nancyyanyu.github.io/posts/9fb5a802/</id>
    <published>2019-07-10T18:23:56.000Z</published>
    <updated>2019-10-20T01:51:32.784Z</updated>
    
    <content type="html"><![CDATA[<p>In this project, I built an application that extract streaming tweets from Twitter, transform the data, and visualize using Apache Sparking Streaming to gain the trending hashtags of a specific topic. In particular, I used a window size of 5 minutes to always get the latest 5 minutes result.</p><a id="more"></a><h1 id="apache-spark-streaming">Apache Spark Streaming</h1><p>Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like <code>map</code>, <code>reduce</code>, <code>join</code> and <code>window</code>.</p><p>I will skip the explaination of how to set up spark in local machine, and the details of Streaming API. Please see the <a href="https://spark.apache.org/docs/latest/api/python/pyspark.streaming.html" target="_blank" rel="noopener">document</a> of Spark.</p><h1 id="part-i.-create-twitter-streaming">Part I. Create Twitter Streaming</h1><h2 id="register-twitter-app">Register Twitter App</h2><p>Before using Twitter's API, I registered an app <a href="https://developer.twitter.com/en/apps" target="_blank" rel="noopener">here</a>, and got the <em>Consumer API keys</em>, <em>Access token &amp; access token secret</em>.</p><p><img src="./2.jpg" width="500"></p><p>These information should be saved, as Twitter needs them to authenticate a user.</p><h2 id="extract-tweets-from-twitter-streaming">Extract Tweets from Twitter Streaming</h2><p>I used Twitter API Python wrapper <a href="https://github.com/bear/python-twitter" target="_blank" rel="noopener">python-twitter</a> to get Tweets stream. Here's a snippet of the code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">twt_app</span><span class="params">(TCP_IP,TCP_PORT,keyword=KEY_WORD)</span>:</span></span><br><span class="line">    consumer_key=<span class="string">''</span></span><br><span class="line">    consumer_secret=<span class="string">''</span></span><br><span class="line">    access_token=<span class="string">''</span></span><br><span class="line">    access_token_secret=<span class="string">''</span></span><br><span class="line">    </span><br><span class="line">    api = twitter.Api(consumer_key=consumer_key,</span><br><span class="line">                      consumer_secret=consumer_secret,</span><br><span class="line">                      access_token_key=access_token,</span><br><span class="line">                      access_token_secret=access_token_secret,</span><br><span class="line">                      sleep_on_rate_limit=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    LANGUAGES = [<span class="string">'en'</span>]</span><br><span class="line">    </span><br><span class="line">    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)</span><br><span class="line">    s.bind((TCP_IP, TCP_PORT))</span><br><span class="line">    s.listen(<span class="number">10</span>)    </span><br><span class="line">    conn, addr = s.accept()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> api.GetStreamFilter(track=[keyword],languages=LANGUAGES):</span><br><span class="line">        conn.send( line[<span class="string">'text'</span>].encode(<span class="string">'utf-8'</span>) )</span><br><span class="line">        print(line[<span class="string">'text'</span>])</span><br><span class="line">        print()</span><br></pre></td></tr></table></figure><p>Basically, I get the streaming of tweets from Twitter API, extract each tweet's text content, and send them to Spark Streaming instance via TCP connection.</p><p>Let's try a topic as <strong>&quot;Trump&quot;</strong>! Here is the result recorded in console:</p><p><img src="./3.gif" width="750"></p><h1 id="part-ii.-set-up-streaming-application">Part II. Set Up Streaming Application</h1><p>Then I set up Spark Streaming App to process tweets text, gain hashtags in every tweet mentioned <strong>&quot;Trump&quot;</strong>, and barplot the top 20 hashtags on the times they appeared in most recent 5 minutes. Here's a snippet of the code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">spark</span><span class="params">(TCP_IP,TCP_PORT,KEY_WORD)</span>:</span></span><br><span class="line">    sc=SparkContext(appName=<span class="string">"TwitterStreamming"</span>)</span><br><span class="line">    sc.setLogLevel(<span class="string">"ERROR"</span>)</span><br><span class="line">    ssc=StreamingContext(sc,<span class="number">5</span>)</span><br><span class="line">    </span><br><span class="line">    socket_stream = ssc.socketTextStream(TCP_IP,TCP_PORT)</span><br><span class="line">    </span><br><span class="line">    lines=socket_stream.window(<span class="number">300</span>)</span><br><span class="line">    df=lines.flatMap(<span class="keyword">lambda</span> x:x.split(<span class="string">" "</span>))  \</span><br><span class="line">            .filter(<span class="keyword">lambda</span> x:x.startswith(<span class="string">"#"</span>))  \</span><br><span class="line">            .filter(<span class="keyword">lambda</span> x:x!=<span class="string">'#%s'</span>%KEY_WORD)  </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(rdd)</span>:</span></span><br><span class="line">        spark=SparkSession \</span><br><span class="line">                .builder \</span><br><span class="line">                .config(conf=rdd.context.getConf()) \</span><br><span class="line">                .getOrCreate()</span><br><span class="line">    </span><br><span class="line">        rowRdd = rdd.map(<span class="keyword">lambda</span> x: Row(word=x))</span><br><span class="line">        wordsDataFrame = spark.createDataFrame(rowRdd)</span><br><span class="line">    </span><br><span class="line">        wordsDataFrame.createOrReplaceTempView(<span class="string">"words"</span>)</span><br><span class="line">        wordCountsDataFrame = spark.sql(<span class="string">"select word, count(*) as total from words group by word order by 2 desc"</span>)       </span><br><span class="line">        pd_df=wordCountsDataFrame.toPandas()</span><br><span class="line">        </span><br><span class="line">        plt.figure( figsize = ( <span class="number">10</span>, <span class="number">8</span> ) )</span><br><span class="line">        sns.barplot( x=<span class="string">"total"</span>, y=<span class="string">"word"</span>, data=pd_df.head(<span class="number">20</span>))</span><br><span class="line">        plt.show()</span><br><span class="line">        </span><br><span class="line">    df.foreachRDD(process)</span><br><span class="line">    </span><br><span class="line">    ssc.start()</span><br><span class="line">    time.sleep(<span class="number">600</span>)</span><br><span class="line">    ssc.stop(stopSparkContext=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>Let's see what I got!</p><p><img src="./4.gif" width="750"></p><h1 id="summary">Summary</h1><p>Thanks to Twitter API and its python Wrapper, I was able to easily get tweets streaming filtered on specified topic. Real-time ETL process could be used to provide instantaneous recommendation, anomaly detection, and etc. There are various projects around Twitter Streaming to be explored. I, in this post, tried a very simple application to find the real-time hashtags trending around a topic.</p><p>As I filtered tweets on topic <strong>&quot;Trump&quot;</strong> , I got <em>#WGDP</em>, <em>#USWNT</em> to be two of the most trending hashtags during the time I ran the application. This application could catch big hot news, and it also serves as a great way to know about what people are talking about in a smaller topic.</p><p>Please check the full code on <a href="https://github.com/nancyyanyu/mini_projects/tree/master/twitter_project" target="_blank" rel="noopener">GitHub</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;In this project, I built an application that extract streaming tweets from Twitter, transform the data, and visualize using Apache Sparking Streaming to gain the trending hashtags of a specific topic. In particular, I used a window size of 5 minutes to always get the latest 5 minutes result.&lt;/p&gt;
    
    </summary>
    
      <category term="Project" scheme="https://nancyyanyu.github.io/categories/Project/"/>
    
    
      <category term="Spark" scheme="https://nancyyanyu.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark SQL &amp; DataFrame, SparkETL</title>
    <link href="https://nancyyanyu.github.io/posts/9a328503/"/>
    <id>https://nancyyanyu.github.io/posts/9a328503/</id>
    <published>2019-07-09T19:49:45.000Z</published>
    <updated>2019-07-09T18:04:06.548Z</updated>
    
    <content type="html"><![CDATA[<h1 id="sql-and-dataframe">SQL and DataFrame</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyspark</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.conf <span class="keyword">import</span> SparkConf</span><br></pre></td></tr></table></figure><p>A <strong><em>SparkSession</em></strong> can be used create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables, and read parquet files. To create a SparkSession, use the following builder pattern:</p><p><strong><em>getOrCreate</em></strong>: Gets an existing SparkSession or, if there is no existing one, creates a new one based on the options set in this builder</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conf=SparkConf().set(<span class="string">"spark.python.profile"</span>, <span class="string">"true"</span>)</span><br><span class="line">spark=SparkSession.builder.master(<span class="string">"local"</span>).appName(<span class="string">"wordcount"</span>).config(conf=SparkConf()).getOrCreate()</span><br></pre></td></tr></table></figure><p><strong><em>createDataFrame</em></strong>(data, schema=None, samplingRatio=None, verifySchema=True)</p><p>Creates a DataFrame from an RDD, a list or a pandas.DataFrame.</p><a id="more"></a><h2 id="creates-a-dataframe-from-a-list">Creates a DataFrame from a list</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">l=[(<span class="string">"alice"</span>,<span class="number">1</span>),(<span class="string">"bob"</span>,<span class="number">2</span>)]</span><br><span class="line">df=spark.createDataFrame(l,[<span class="string">'name'</span>,<span class="string">'age'</span>])</span><br><span class="line">df.collect()</span><br></pre></td></tr></table></figure><pre><code>[Row(name=&#39;alice&#39;, age=1), Row(name=&#39;bob&#39;, age=2)]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.toPandas()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }        .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe"><thead><tr style="text-align: right;"><th></th><th>name</th><th>age</th></tr></thead><tbody><tr><th>0</th><td>alice</td><td>1</td></tr><tr><th>1</th><td>bob</td><td>2</td></tr></tbody></table></div><h2 id="creates-a-dataframe-from-pandas-dataframe-and-use-sql-query">Creates a DataFrame from pandas DataFrame, and use sql query</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df2=spark.createDataFrame(pd.DataFrame(l,columns=[<span class="string">'name'</span>,<span class="string">'age'</span>]))</span><br><span class="line">df2.collect()</span><br></pre></td></tr></table></figure><p>[Row(name='alice', age=1), Row(name='bob', age=2)]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df2.select(<span class="string">"name"</span>).collect()</span><br></pre></td></tr></table></figure><p>[Row(name='alice'), Row(name='bob')]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df2.createOrReplaceTempView(<span class="string">'table1'</span>)</span><br><span class="line">spark.sql(<span class="string">"select name from table1"</span>).collect()</span><br></pre></td></tr></table></figure><p>[Row(name='alice'), Row(name='bob')]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.table(<span class="string">"table1"</span>).collect()</span><br></pre></td></tr></table></figure><p>[Row(name='alice', age=1), Row(name='bob', age=2)]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.stop()</span><br></pre></td></tr></table></figure><h2 id="sqlcontext">SQLContext</h2><p>The entry point for working with structured data (rows and columns) in Spark, in Spark 1.x.</p><p>As of Spark 2.0, this is replaced by SparkSession. However, we are keeping the class here for backward compatibility.</p><p>A SQLContext can be used create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables, and read parquet files.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sc=SparkContext()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Row,SQLContext</span><br><span class="line">sqlContext=SQLContext(sc)</span><br><span class="line">rdd=sc.parallelize(l)</span><br><span class="line">Person=Row(<span class="string">"name"</span>,<span class="string">"age"</span>)</span><br><span class="line">person=rdd.map(<span class="keyword">lambda</span> x: Person(*x))</span><br><span class="line">sqlContext.createDataFrame(person).collect()</span><br></pre></td></tr></table></figure><p>[Row(name='alice', age=1), Row(name='bob', age=2)]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sc.stop()</span><br></pre></td></tr></table></figure><h1 id="sparketl">SparkETL</h1><p>ETL is a type of data integration process referring to three distinct but interrelated steps (Extract, Transform and Load) and is used to synthesize data from multiple sources many times to build a Data Warehouse, Data Hub, or Data Lake.</p><p>Let's write an ETL job on pyspark!</p><p>Reference: (https://github.com/AlexIoannides/pyspark-example-project)</p><p>Before building ETL process, we write a function <em>start_spark</em> to start our sparkSession, update and get our configuration.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> __main__</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> environ,listdir,path</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkFiles</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.conf <span class="keyword">import</span> SparkConf</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_spark</span><span class="params">(app_name=<span class="string">"my_spark_app"</span>,master=<span class="string">"local[*]"</span>,files=[<span class="string">'etl_conf.json'</span>])</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    flag_repl=<span class="keyword">not</span>(hasattr(__main__,<span class="string">'__file__'</span>))</span><br><span class="line">    flag_debug=<span class="string">'DEBUG'</span> <span class="keyword">in</span> environ.keys()</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span>(flag_repl <span class="keyword">or</span> flag_debug):    </span><br><span class="line">        spark_builder=(SparkSession.builder.appName(app_name))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        spark_builder=SparkSession.builder.appName(app_name).master(master)</span><br><span class="line">    </span><br><span class="line">    spark_files=<span class="string">'.'</span>.join(list(files))</span><br><span class="line">    spark_builder.config(<span class="string">'spark.files'</span>,spark_files)</span><br><span class="line">    spark_builder.config(conf=SparkConf())    </span><br><span class="line">    spark_sess=spark_builder.getOrCreate()</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">#spark_logger=logger.Log4j(spark_sess)</span></span><br><span class="line">    spark_files_dir=SparkFiles.getRootDirectory()</span><br><span class="line">    </span><br><span class="line">    config_files=[x <span class="keyword">for</span> x <span class="keyword">in</span> listdir(spark_files_dir) <span class="keyword">if</span> x.endswith(<span class="string">'conf.json'</span>)]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> config_files:</span><br><span class="line">        path_to_config_file=path.join(spark_files_dir,config_files[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">with</span> open(path_to_config_file,<span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            config_dict=json.load(f)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        </span><br><span class="line">        config_dict=<span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> spark_sess,config_dict</span><br></pre></td></tr></table></figure><p>ETL Process contains 3 stages: Extract, Transform, Load. In Spark,</p><ul><li><strong>Extract</strong>: read Parquet format data in local machine</li><li><strong>Transform</strong>: use sparkSQL to manipulate data</li><li><strong>Load</strong>: write to csv</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> spark <span class="keyword">import</span> start_spark</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkFiles</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Row</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    spark,conf=start_spark()</span><br><span class="line">    steps_per_floor_=conf[<span class="string">'steps_per_floor'</span>]</span><br><span class="line">    df=extract(spark)</span><br><span class="line">    df_tf=transform(df,steps_per_floor_,spark)</span><br><span class="line">    load(df_tf)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract</span><span class="params">(spark)</span>:</span></span><br><span class="line">    df=spark.read.parquet(<span class="string">'tests/test_data/employees'</span>)</span><br><span class="line">    <span class="keyword">return</span> df</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(df,steps_per_floor_,spark)</span>:</span></span><br><span class="line">    df.createOrReplaceTempView(<span class="string">"table1"</span>)</span><br><span class="line">    df_transformed=spark.sql(<span class="string">"select id, concat(first_name,' ' , second_name) as name, floor* %s as steps_to_desk from table1"</span>%steps_per_floor_)</span><br><span class="line">    <span class="keyword">return</span> df_transformed</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load</span><span class="params">(df)</span>:</span></span><br><span class="line">    df.coalesce(<span class="number">1</span>).write.csv(<span class="string">'loaded_data'</span>, mode=<span class="string">'overwrite'</span>, header=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_test_data</span><span class="params">(spark,conf)</span>:</span>   </span><br><span class="line">    local_records=[</span><br><span class="line">            Row(id=<span class="number">1</span>, first_name=<span class="string">'nancy'</span>, second_name=<span class="string">"yan"</span>, floor=<span class="number">1</span>),</span><br><span class="line">            Row(id=<span class="number">2</span>, first_name=<span class="string">'Dan'</span>, second_name=<span class="string">'Sommerville'</span>, floor=<span class="number">1</span>),</span><br><span class="line">            Row(id=<span class="number">3</span>, first_name=<span class="string">'Alex'</span>, second_name=<span class="string">'Ioannides'</span>, floor=<span class="number">2</span>),</span><br><span class="line">            Row(id=<span class="number">4</span>, first_name=<span class="string">'Ken'</span>, second_name=<span class="string">'Lai'</span>, floor=<span class="number">2</span>),</span><br><span class="line">            Row(id=<span class="number">5</span>, first_name=<span class="string">'Stu'</span>, second_name=<span class="string">'White'</span>, floor=<span class="number">3</span>),</span><br><span class="line">            Row(id=<span class="number">6</span>, first_name=<span class="string">'Mark'</span>, second_name=<span class="string">'Sweeting'</span>, floor=<span class="number">3</span>),</span><br><span class="line">            Row(id=<span class="number">7</span>, first_name=<span class="string">'Phil'</span>, second_name=<span class="string">'Bird'</span>, floor=<span class="number">4</span>),</span><br><span class="line">            Row(id=<span class="number">8</span>, first_name=<span class="string">'Kim'</span>, second_name=<span class="string">'Suter'</span>, floor=<span class="number">4</span>)</span><br><span class="line">        ]</span><br><span class="line">    </span><br><span class="line">    df=spark.createDataFrame(local_records)</span><br><span class="line">    df_tf=transform(df,conf[<span class="string">'steps_per_floor'</span>],spark)</span><br><span class="line">    df_tf.coalesce(<span class="number">1</span>).write.parquet(<span class="string">'tests/test_data/employees_report'</span>,mode=<span class="string">'overwrite'</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;sql-and-dataframe&quot;&gt;SQL and DataFrame&lt;/h1&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; pyspark&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; pyspark &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; SparkContext&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; pyspark.sql &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; SparkSession&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; pyspark.conf &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; SparkConf&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;A &lt;strong&gt;&lt;em&gt;SparkSession&lt;/em&gt;&lt;/strong&gt; can be used create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables, and read parquet files. To create a SparkSession, use the following builder pattern:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;getOrCreate&lt;/em&gt;&lt;/strong&gt;: Gets an existing SparkSession or, if there is no existing one, creates a new one based on the options set in this builder&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;conf=SparkConf().set(&lt;span class=&quot;string&quot;&gt;&quot;spark.python.profile&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;true&quot;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;spark=SparkSession.builder.master(&lt;span class=&quot;string&quot;&gt;&quot;local&quot;&lt;/span&gt;).appName(&lt;span class=&quot;string&quot;&gt;&quot;wordcount&quot;&lt;/span&gt;).config(conf=SparkConf()).getOrCreate()&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;createDataFrame&lt;/em&gt;&lt;/strong&gt;(data, schema=None, samplingRatio=None, verifySchema=True)&lt;/p&gt;
&lt;p&gt;Creates a DataFrame from an RDD, a list or a pandas.DataFrame.&lt;/p&gt;
    
    </summary>
    
      <category term="Big Data" scheme="https://nancyyanyu.github.io/categories/Big-Data/"/>
    
    
      <category term="Spark" scheme="https://nancyyanyu.github.io/tags/Spark/"/>
    
      <category term="ETL" scheme="https://nancyyanyu.github.io/tags/ETL/"/>
    
      <category term="SQL" scheme="https://nancyyanyu.github.io/tags/SQL/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop MapReduce: Tuning Distributed Storage Platform with File Types</title>
    <link href="https://nancyyanyu.github.io/posts/c490e7f0/"/>
    <id>https://nancyyanyu.github.io/posts/c490e7f0/</id>
    <published>2019-07-09T18:08:35.268Z</published>
    <updated>2019-07-10T23:03:55.904Z</updated>
    
    <content type="html"><![CDATA[<h1 id="data-modeling-and-file-formats">Data modeling and file formats</h1><p>There is a mismatch between terms used to define business tasks and terms used to describe what HDFS is.</p><p>Data modeling and data management are concerned with these issues.</p><a id="more"></a><p><img src="./week_50.png" width="300"></p><h2 id="data-modeling">Data modeling</h2><ul><li><p><strong>Data model</strong> – a way you think about your data elements, what they are, what domain they come from, how different elements relate to each other, what they are composed of</p></li><li>abstract model</li><li>explicitly defines the structure of data</li><li>Makes some things easier to express than others</li><li><p>Will use a relational model</p></li><li><p><strong>Relational data model</strong></p></li><li>a data set is an ordered set called table of tuples called rows.</li><li>Where every tuple is composed of simple values such as numbers or strings.</li><li><p>A position of a value within a tuple is a column. And column defines value semantics. <img src="./week_51.png" width="300"></p></li><li><p><strong>Graph data model</strong></p></li><li>A graph consist of vertices and edges.</li><li>vertices: represent entities. Movies, actors, directors, titles, and so on.</li><li><p>edges: represent relations between entities.</p></li></ul><p><img src="./week_52.png" width="300"></p><ul><li><p><strong>Unstructured data</strong></p></li><li>Technically, all data is structured at least as a byte sequence</li><li>Usually, means “not structured enough for a task</li><li><p>Denote <em>complexity of bringing data to useful form for a particular application</em>.</p></li></ul><p>e.g. Videos are structured as a sequence of frames, where each frame is an ordered set of pixels, where every pixel is just a triple of RGB color intensities.</p><p>However, this structure is useless if you're willing to count people in the video. The hard job is to do the image recognition and bring the appropriate structure to the data so that solving accounting problem would become easier.</p><h2 id="data-management">Data Management</h2><blockquote><p>How to store and how to organize your data physically. In Hadoop, this is a matter of file format or storage format</p></blockquote><p>File format: - Defines (physical) data layout - Different choices of data layout lead to different tradeoffs in application complexity, and thus, affect performance and correctness.</p><p><strong>Primary function</strong>: - <strong>serialization</strong>: a process of converting an object into a sequence of bytes which can be persisted to a disk or database or can be sent through streams. - <strong>deserialization</strong>: creating object from sequence of bytes</p><p><strong>Differ in:</strong> - space efficiency: different formats use different coding schemes which directly affect consumed disk space. - encoding &amp; decoding speed - supported data types - splittable/monolithic structure: this property allows you to extract a subset of data without reading the entire file. We typically expect data to be splittable. This follows from our data model. - extensibility</p><h1 id="text-formats">Text Formats</h1><p>line delimited text files:</p><ul><li><p>pros: human-readable</p></li><li><p>cons: you need to parse it, convert it from the textual form into programmatic data structures.</p></li></ul><h2 id="csv">CSV</h2><blockquote><p>comma-separated values</p></blockquote><p>Criteria: - Space efficiency: BAD - Extensibility: BAD -It is not that easy to remove or reorder fields in these formats and the code is likely to make assumptions about the field indexes. - Splittable: You should not include any headers in your data as it hinders splittability and mergability of your data. - Data types: ONLY STRINGS - Speed: generation and parsing are very efficient</p><h2 id="json">JSON</h2><blockquote><p>JavaScript Object Notation: defines a representation for the primitive values and their combination in the form of lists and maps.</p></blockquote><p>Criteria: - Space efficiency: WORSE THAN CSV -It includes field names in serialized form. As you can see, the strings ticker, date, and others are repeated in every row of the data set. - Extensibility: You can easily add and remove fields from your data items and JSON will remain valid and parsable. - Splittable: - SPLITTABLE IF 1 DOCUMENT PER LINE - Data types: JSON allows you to store strings, numbers, Booleans, maps, lists in the native way. - Speed: GOOD ENOUGH</p><h1 id="binary-formats">Binary formats</h1><ul><li>Text formats are designed with human readability in mind</li><li>Binary formats are designed for machines and they trade readability for efficiency and correctness.</li></ul><h2 id="sequence-file">Sequence File</h2><ul><li>First binary format implemented in Hadoop</li><li>Stores sequence of key-value pairs, where key and value are of arbitrary type with the user defined serialization and deserialization code</li><li>Java-specific serialization/deserialization</li><li>Primary use case: storing the intermediate data in MapReduce computations</li></ul><h3 id="data-layout">Data Layout</h3><p>SequenceFile starts with the header which includes format version, class names for key and value types, flags syndicating compression, metadata, and a sync marker.</p><p><img src="./week_60.png" width="400"></p><p>Uncompressed case for every record:</p><ul><li>Fixed size header with a record key length and value length, followed by the serialized key and the serialized value.</li><li>To decode data, you can read the file linearly and use length to read the key and the value.</li></ul><p>Block compressed case:</p><ul><li>Key-value pairs are grouped in blocks and every block starts with a number of pairs followed by the key lengths and the compressed keys. Then by value lengths and finally by compressed values.</li></ul><p>Record compressed case: - Every value is compressed individually while block compressed case, a set of keys or values are compressed together resulting in better compression</p><h3 id="critera">Critera</h3><ul><li>Space efficiency: MODERATE TO GOOD -the on-disk format closely matches the in-memory format to enable fast encoding and decoding.</li><li>Extensibility: No</li><li>Splittable: splittable via sync markers -Sync markers are unique with the high probability, so you can seek to an arbitrary point in the file and scan for the next occurrence of the sync marker to get the next record.</li><li>Data types: Any type implement in the appropriate interfaces could be used with a format.</li><li>Speed:GOOD</li></ul><h2 id="avro">AVRO</h2><blockquote><p>Avro's design goal was to create an efficient and flexible format which could be used with different programming languages.</p></blockquote><ul><li>Both format &amp; support library</li><li>Stores objects defined by the schema</li><li>specifies field names, types, aliases</li><li>defines serialization/deserialization code</li><li>allows some schema updates</li><li>Interoperability with many languages</li></ul><p><strong>schema</strong>: a description of the fields in data items and their types.</p><ul><li>defines data encoding for every item</li><li>When storing data, the schema is included in the file thus allowing future readers to decode it correctly</li><li>If the read schema does not match the data schema, Avro tries to resolve inconsistencies thus enabling smooth schema migrations</li></ul><h3 id="data-layout-1">Data Layout</h3><p>Every Avro file starts with a header followed by a sequence of blocks containing the number of encoded objects, their sizes, and the actual payload.</p><p><strong>Sync markers</strong> are used to delimit consequent blocks of records.</p><p>What is different in Avro is that the serialization code is defined by the schema and not by the user-provided code. <img src="./week_61.png" width="300"></p><h3 id="critera-1">Critera</h3><ul><li>Space efficiency: MODERATE TO GOOD -The encoding format mostly follows the in-memory format. Space savings could be obtained by using compression.</li><li>Extensibility: field addition, or removal, or renaming, are handled transparently by the framework.</li><li>Splittable: Achieved using the same sync marker technique as in sequence files</li><li>Data types: same types as JSON, plus a few more complex types, like enumerations records.</li><li>Speed: GOOD WITH CODE GENERATION -Avro can generate serialization and deserialization code from a schema. In this case, its performance closely matches sequence files.</li></ul><p>Avro is a popular format now, holding the balance between efficiency and flexibility.</p><h2 id="rcfile">RCFile</h2><h3 id="columnar">Columnar</h3><p>The <strong>execution time</strong> for analytical applications is mostly I/O bound---you could gain more by optimizing input and output, while optimizing the computation has a diminishing effect on performance.</p><p><strong>How to save input and output operations?</strong> - not reading the data necessary for the processing - using superior compression schemes.</p><p>RCFile and Parquet: columnar(relational data model) data formats that exploit outlined optimizations.</p><p><strong>Pros of columnar:</strong> 1. Columnar stores are highly efficient in terms of CPU usage.databases were storing data row by row, linearly. They would completely serialize one row before continuing to the other ---&gt; if you need to read the values from just one particular column, you still need to read the whole table.</p><ol start="2" type="1"><li>Columnar stores transpose data layout and store all the values column by column, enabling two key optimizations:</li></ol><ul><li>you can efficiently scan only the necessary data.</li><li>you can achieve better compression ratios, because column-wise, data is more regular and more repetitive, and hence, more compressible.</li></ul><p><strong>Cons of columnar:</strong> 1. row assembly -To reconstruct the full row, you need to perform lookups from all the columns, which is likely to cause more input and output operations. However, by accessing the subset of columns, you can reduce the number of input and output operations.</p><h3 id="data-layout-2">Data Layout</h3><p>RC: Record Columnar</p><ul><li>First columnar format* in Hadoop()</li><li>one of the most popular storage formats for data warehouses.</li><li>Horizontal/vertical partitioning to layout data</li><li>rows are split into row groups; within each row group, values are encoded column by column.</li><li>transpose values within a row group &gt; The scheme, assuming that the row repeats with a single block managed by a single machine, ensures that the row assembly is a local operation, and hence, does not incur any network accesses.</li></ul><p><img src="./week_70.png" width="300"></p><p>Every row group contains three factions: sync marker, metadata, and column data.</p><ul><li>Metadata:</li><li>the number of rows</li><li>the number of columns</li><li>the total number of bytes</li><li>bytes per column</li><li>bytes per value.</li></ul><p>This information is used by a decoder to read the consequent column data.</p><p><strong>Compression:</strong></p><ul><li><p>Metadata is compressed with the run-length encoding to save on the repeated integers;</p></li><li><p>Column data is compressed with a general-purpose codec such as ZIP.</p></li></ul><p>To produce a block of data, you need to buffer a row group within the main memory and transpose it, and then precompute metadata.</p><h3 id="critera-2">Critera</h3><ul><li>Space efficiency:</li><li>RCFiles save a lot of space by exploiting the columnar layout</li><li>data itself is compressed on the block level, increasing space savings.</li><li>Speed: MODERATE TO GOOD,LESS I/O</li><li>ZIP is not the fastest codec in the world.</li><li>Speed is gained by reducing input and output operations, by not reading columns that are not used in further computation.</li><li>Splittable: SPLITTABLE</li><li>sync markers are used to make a splittable format.</li><li>Data types: BYTE STRINGS</li><li>RCFiles are untyped. And values are treated as bytes.</li><li>The reason for that is because RCFiles are mostly used in conjunction with Hive. And Hive deals with all the serialization and deserialization. So there is no need to offload this functionality to the format.</li><li>Extensibility: NO</li><li>Encoded records have a fixed structure. So you need to deal with schema migration by yourself. Once again, this is mostly because Hive rewrites data on schema change.</li></ul><h2 id="parquet">Parquet</h2><ul><li>The most sophisticated columnar format in Hadoop</li><li>Collaborative effort by Twitter &amp; Cloudera</li><li>Supports nested and repeated data</li><li>Exploits many columnar optimizations (such as predicate pruning,per column codecs)</li><li>Optimizes write path</li></ul><h3 id="data-layout-3">Data Layout</h3><p><img src="./week_71.png" width="300"></p><h2 id="conclusion">Conclusion</h2><ul><li>Binary formats are efficient in coding data</li><li>SequenceFile is a reasonable choice for Java users</li><li>Avro is a good alternative for many use cases</li><li>RCFile/ORC/Parquet are best for “wide” tables and analytical workloads</li></ul><h1 id="compression">Compression</h1><ul><li>Block-level compression</li><li>used in SequenceFiles, RCFiles, Parquet</li><li>RCFiles: sync markers and metadata could be used to devise splitting for the dataset without decompressing the entire file.</li><li>applied within a block of data</li><li><p>pros: compression does not hinder the ability to navigate through the file quickly.</p></li><li>File-level compression</li><li>a file was compressed before being written.</li><li>e.g.zip applied to all files in Unix systems.</li><li>applied to the file as a whole</li><li>hinders an ability to navigate through file</li><li>pros: achieves better compression ratios</li><li><p>cons: breaks the file structure and makes the files lesss splitable.</p></li></ul><h2 id="codecs">Codecs</h2><p>In Hadoop stack, there are few popular codecs that you can use with your data: Gzip, Bzip2, LZO, Snappy.</p><p><img src="./week_80.png" width="400"></p><ul><li>Bzip2 is the slowest yet the most efficient</li><li>Snappy is the fastest, yet not so efficient</li></ul><h2 id="when-to-use-compression">When to use compression?</h2><ol type="1"><li><strong>CPU-bound</strong>: Assume your program can process data at a rate 10 MB per second.</li></ol><ul><li>Providing data at higher rates, has no effect on completion time because there is no capacity to perform more work in a unit of time. (Your program spends more time computing rather than doing I/O operations.)</li><li>CPU is your bottleneck</li><li>Results: Adding compression would put more pressure on CPU and increase the completion time.</li></ul><p><img src="./week_81.png" width="400"></p><ol start="2" type="1"><li>I/O bound: your program can process data at a rate 1000 MB per second, while HDFS can stream data only at rate 100 MB per second</li></ol><ul><li><p>your program spends more time waiting for input and output, rather than doing actual computation. <img src="./week_82.png" width="400"></p></li><li><p>Adding compression here would allow HDFS to stream the compressed data at rate 100 MB per second, which transforms to 500 MB per second of uncompressed data, assuming the compression ratio of five <img src="./week_83.png" width="400"></p></li></ul><h2 id="summary">Summary</h2><ol type="1"><li>Raise awareness about application bottlenecks</li></ol><ul><li>CPU-bound : cannot benefit from the compression</li><li>I/O-bound : can benefit from the compression &gt;trade spare CPU time that was wasted in I/O wait for an extra work for compression which resulted in better utilization of both CPU and I/O.</li></ul><ol start="2" type="1"><li>Codec performance vary depending on data, many options available</li></ol><h1 id="conclusion-1">Conclusion</h1><ul><li>Many applications assume relational data model</li><li>File format defines encoding of your data</li><li>text formats are readable, allow quick prototyping, but inefficient</li><li>binary formats are efficient, but more complicated to use</li><li>File formats vary in terms of space efficiency, encoding &amp; decoding speed, support for data types, extensibility</li><li>When I/O bound, can benefit from compression</li><li>When CPU bound, compression may increase completion time</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;data-modeling-and-file-formats&quot;&gt;Data modeling and file formats&lt;/h1&gt;
&lt;p&gt;There is a mismatch between terms used to define business tasks and terms used to describe what HDFS is.&lt;/p&gt;
&lt;p&gt;Data modeling and data management are concerned with these issues.&lt;/p&gt;
    
    </summary>
    
      <category term="Big Data" scheme="https://nancyyanyu.github.io/categories/Big-Data/"/>
    
    
      <category term="Hadoop" scheme="https://nancyyanyu.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>June 2019 | 六月日志</title>
    <link href="https://nancyyanyu.github.io/posts/4935c427/"/>
    <id>https://nancyyanyu.github.io/posts/4935c427/</id>
    <published>2019-07-09T16:18:24.000Z</published>
    <updated>2019-08-02T19:03:11.019Z</updated>
    
    <content type="html"><![CDATA[<p> </p><a id="more"></a><center><img src="./allen.png" width="400"></center><p> </p><center><b>❤ 6月手帳 ❤</b></center><table><colgroup><col style="width: 5%"><col style="width: 68%"><col style="width: 25%"></colgroup><thead><tr class="header"><th style="text-align: left;">Date</th><th>Study</th><th>Workout</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">06/12</td><td>Finished reading ISLR and writing notes; set up blogs on Hexo, githubPage, and NexT; published notes</td><td>/</td></tr><tr class="even"><td style="text-align: left;">06/13</td><td>Practiced tensorflow; implemented CNN, MLP</td><td>Yoga</td></tr><tr class="odd"><td style="text-align: left;">06/14</td><td>Practiced tensorflow; recap RNN, LSTM; tried implementation on high &amp; low level API; wrote data grabbing module</td><td>/</td></tr><tr class="even"><td style="text-align: left;">06/15</td><td>Answer ML questions</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">06/16</td><td>(Travelling all day)</td><td>/</td></tr><tr class="even"><td style="text-align: left;">06/17</td><td>Answer ML questions</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">06/18</td><td>Answer ML &amp; DL questions</td><td>/</td></tr><tr class="even"><td style="text-align: left;">06/19</td><td>~~nothing accomplished</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">06/20</td><td>Study <a href="https://www.linkedin.com/learning/tableau-10-essential-training" target="_blank" rel="noopener">Tableau</a> , write code on phishing problems using SVM, LR, RF, NN</td><td>/</td></tr><tr class="even"><td style="text-align: left;">06/21</td><td>Study <a href="https://www.linkedin.com/learning/tableau-10-essential-training" target="_blank" rel="noopener">Tableau</a> , <a href="https://mode.com/resources/sql-tutorial" target="_blank" rel="noopener">Mode: Basic SQL</a></td><td>Running, Glutei bridge</td></tr><tr class="odd"><td style="text-align: left;">06/22</td><td><a href="https://mode.com/resources/sql-tutorial" target="_blank" rel="noopener">Mode: Intermediate SQL</a></td><td>Yogo</td></tr><tr class="even"><td style="text-align: left;">06/23</td><td><a href="https://mode.com/resources/sql-tutorial" target="_blank" rel="noopener">Mode: Intermediate &amp; Advanced SQL</a></td><td>/</td></tr><tr class="odd"><td style="text-align: left;">06/24</td><td>Leetcode SQL</td><td>/</td></tr><tr class="even"><td style="text-align: left;">06/25</td><td>Leetcode SQL Premium</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">06/26</td><td>Learn Spark</td><td>/</td></tr><tr class="even"><td style="text-align: left;">06/27</td><td>Learn Spark, pyspark. Setup spark,jupyter on AWS EC2</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">06/28</td><td>Learn pyspark, write project code and work on Tableau</td><td>/</td></tr><tr class="even"><td style="text-align: left;">06/29</td><td>Write project report and work on Tableau; learn Tableau</td><td>Hiking</td></tr><tr class="odd"><td style="text-align: left;">06/30</td><td>Write project report and work on Tableau</td><td>Running</td></tr></tbody></table><p> </p><p> </p><center><b>❤ Plan ❤</b></center><table><thead><tr class="header"><th style="text-align: center;">Categories</th><th style="text-align: center;">Content</th><th style="text-align: center;">Progress</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">Machine Learning</td><td style="text-align: center;">Intro to Statistical Learning</td><td style="text-align: center;">■■■■■■■■■■</td></tr><tr class="even"><td style="text-align: center;">Machine Learning</td><td style="text-align: center;"><a href="https://github.com/Sroy20/machine-learning-interview-questions" target="_blank" rel="noopener">Answer ML Interview Questions</a></td><td style="text-align: center;">■■■■■■■■■■</td></tr><tr class="odd"><td style="text-align: center;">Machine Learning</td><td style="text-align: center;"><a href="%5Bhttp://cs229.stanford.edu/syllabus.html%5D(http://cs229.stanford.edu/syllabus.html)">CS229: Machine Learning</a></td><td style="text-align: center;">■□□□□□□□□□</td></tr><tr class="even"><td style="text-align: center;">Deep Learning</td><td style="text-align: center;">Practice Tensorflow via Implementing DL Models</td><td style="text-align: center;">■■■□□□□□□□</td></tr><tr class="odd"><td style="text-align: center;">Deep Learning</td><td style="text-align: center;"><a href="https://github.com/Sroy20/machine-learning-interview-questions" target="_blank" rel="noopener">Answer DL Interview Questions</a></td><td style="text-align: center;">■■■■□□□□□□</td></tr><tr class="even"><td style="text-align: center;">A/B Testing</td><td style="text-align: center;"><a href="https://classroom.udacity.com/courses/ud257" target="_blank" rel="noopener">Udacity</a></td><td style="text-align: center;">■■■□□□□□□□</td></tr><tr class="odd"><td style="text-align: center;">A/B Testing</td><td style="text-align: center;">Studying Bayesian A/B Testing</td><td style="text-align: center;">■□□□□□□□□□</td></tr><tr class="even"><td style="text-align: center;">Product Sense</td><td style="text-align: center;"><a href="https://www.youtube.com/channel/UC6hlQ0x6kPbAGjYkoz53cvA" target="_blank" rel="noopener">Product School</a> Good Machine Learning PM by Google</td><td style="text-align: center;">■■■■■■■■■■</td></tr><tr class="odd"><td style="text-align: center;">Coding</td><td style="text-align: center;"><a href="https://www.teamblind.com/article/New-Year-Gift---Curated-List-of-Top-75-LeetCode-Questions-to-Save-Your-Time-OaM1orEU" target="_blank" rel="noopener">Top 75 LeetCode Questions</a></td><td style="text-align: center;">■■■□□□□□□□</td></tr><tr class="even"><td style="text-align: center;">SQL</td><td style="text-align: center;">Leetcode SQL easy/medium/hard/premium</td><td style="text-align: center;">■■■■■■■■■■</td></tr><tr class="odd"><td style="text-align: center;">SQL</td><td style="text-align: center;">Mode Analytics SQL Practice</td><td style="text-align: center;">■■■■■■■■■■</td></tr><tr class="even"><td style="text-align: center;">SQL</td><td style="text-align: center;">Hackerrank SQL</td><td style="text-align: center;">■■■■■■■□□□</td></tr><tr class="odd"><td style="text-align: center;">Tableau</td><td style="text-align: center;">LinkedIn Course <a href="https://www.linkedin.com/learning/tableau-10-essential-training" target="_blank" rel="noopener">Tableau 10 Essential Training</a></td><td style="text-align: center;">■■■■□□□□□□</td></tr><tr class="even"><td style="text-align: center;">Big Data</td><td style="text-align: center;"><a href="https://www.coursera.org/learn/big-data-essentials/home/welcome" target="_blank" rel="noopener">Big Data Essentials: HDFS, MapReduce and Spark RDD</a></td><td style="text-align: center;">■■■■■■▥□□□</td></tr><tr class="odd"><td style="text-align: center;">Big Data</td><td style="text-align: center;"><a href="https://www.udemy.com/apache-spark-streaming-with-python-and-pyspark/" target="_blank" rel="noopener">Apache Spark Streaming with Python and PySpark</a></td><td style="text-align: center;">■■□□□□□□□□</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; &lt;/p&gt;
    
    </summary>
    
      <category term="Journal" scheme="https://nancyyanyu.github.io/categories/Journal/"/>
    
    
      <category term="Journal" scheme="https://nancyyanyu.github.io/tags/Journal/"/>
    
  </entry>
  
</feed>
