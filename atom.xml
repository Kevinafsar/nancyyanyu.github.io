<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Nancy&#39;s Notes</title>
  
  <subtitle>Code changes world!</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://nancyyanyu.github.io/"/>
  <updated>2019-10-19T22:56:41.782Z</updated>
  <id>https://nancyyanyu.github.io/</id>
  
  <author>
    <name>Nancy Yan</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Linear Regression Part II: Potential Problems</title>
    <link href="https://nancyyanyu.github.io/posts/4df00c7b/"/>
    <id>https://nancyyanyu.github.io/posts/4df00c7b/</id>
    <published>2019-10-19T22:56:41.781Z</published>
    <updated>2019-10-19T22:56:41.782Z</updated>
    
    <content type="html"><![CDATA[<h1 id="qualitative-predictors">Qualitative Predictors</h1><h2 id="predictors-with-only-two-levels">Predictors with Only Two Levels</h2><p>Suppose that we wish to investigate differences in credit card balance between males and females, ignoring the other variables for the moment. If a qualitative predictor (also known as a <strong>factor</strong>) only has two <strong>levels</strong>, or possible values, then incorporating it into a regression model is very simple. We simply create an indicator or <strong>dummy variable</strong> that takes on two possible numerical values.</p><p><img src="./8.png" width="300"> and use this variable as a predictor in the regression equation. This results in the model</p><p><img src="./9.png" width="600"></p><a id="more"></a><p>Now β0 can be interpreted as the average credit card balance among males, β0 + β1 as the average credit card balance among females</p><h2 id="qualitative-predictors-with-more-than-two-levels">Qualitative Predictors with More than Two Levels</h2><p>When a qualitative predictor has more than two levels, we can create additional dummy variables. For example, for the ethnicity variable we create two dummy variables. The first could be</p><p><img src="./10.png" width="300"> and the second could be <img src="./11.png" width="300"></p><p>Then both of these variables can be used in the regression equation, in order to obtain the model</p><p><img src="./12.png" width="600"></p><p><strong>Baseline</strong></p><ul><li>There will always be <strong>one fewer</strong> dummy variable than the number of levels. The level with no dummy variable—African American in this example—is known as the baseline.</li></ul><p><img src="./13.png" width="600"></p><p>The p-values associated with the coefficient estimates for the two dummy variables are very large, suggesting no statistical evidence of a real difference in credit card balance between the ethnicities</p><blockquote><p>The coefficients and their p-values do depend on the choice of dummy variable coding</p></blockquote><p>Rather than rely on the individual coefficients, we can use an <strong>F-test</strong> to test H0 : β1 = β2 = 0; this does not depend on the coding.</p><p>This F-test has a p-value of 0.96, indicating that we cannot reject the null hypothesis that there is no relationship between balance and ethnicity.</p><h1 id="extensions-of-the-linear-model">Extensions of the Linear Model</h1><p>Two of the most important assumptions state that the relationship between the predictors and response are <strong>additive</strong> and <strong>linear</strong>. - <strong>Additive</strong>: the effect of changes in a predictor <span class="math inline">\(X_j\)</span> on the response <span class="math inline">\(Y\)</span> is independent of the values of the other predictors - <strong>Linear</strong>: the change in the response <span class="math inline">\(Y\)</span> due to a one-unit change in <span class="math inline">\(X_j\)</span> is constant, regardless of the value of <span class="math inline">\(X_j\)</span></p><h2 id="removing-the-additive-assumption">Removing the Additive Assumption</h2><p>Consider the standard linear regression model with two variables, <span class="math display">\[\begin{align}Y = β_0 + β_1X_1 + β_2X_2 + \epsilon\end{align}\]</span></p><p>One way of extending this model to allow for interaction effects is to include a third predictor, called an <strong>interaction term</strong>:</p><p><span class="math display">\[\begin{align}Y = β_0 + β_1X_1 + β_2X_2 +  β_3X_1X_2 + \epsilon \end{align}\]</span></p><p><strong>How does inclusion of this interaction term relax the additive assumption?</strong></p><p>The model above could be written as: <span class="math display">\[\begin{align}Y &amp;= β_0 + (β_1+β_3X_2)X_1 + β_2X_2 + \epsilon  \\&amp;= β_0 + \tilde{β}_1X_1 + β_2X_2 + \epsilon\end{align}\]</span></p><p>Since <span class="math inline">\(\tilde{β}_1\)</span> changes with <span class="math inline">\(X_2\)</span>, the effect of <span class="math inline">\(X_1\)</span> on <span class="math inline">\(Y\)</span> is no longer constant: adjusting <span class="math inline">\(X_2\)</span> will change the impact of <span class="math inline">\(X_1\)</span> on <span class="math inline">\(Y\)</span>.</p><p><img src="./14.png" width="600"></p><ul><li>Sometimes the case that an interaction term has a very small p-value, but the associated main effects (in this case, TV and radio) do not.</li><li>The <strong>hierarchical principle</strong> states that if we include an interaction in a model, we should also include the <strong>main effects</strong>, even if the p-values associated with their coefficients are not significant. (If the interaction between X1 and X2 seems important, we should include both X1 and X2 in the model even if their coefficient estimates have large p-values)</li></ul><p><strong>Concept of interactions applies on qualitative variables</strong> <img src="./15.png" width="600"></p><p>Adding an interaction variable, model now becomes: <img src="./17.png" width="600"> <img src="./16.png" width="600"></p><h2 id="non-linear-relationships">Non-linear Relationships</h2><p>Extending the linear model to accommodate non-linear relationships is known as <strong>polynomial regression</strong>, since we have included <strong>polynomial functions</strong> of the predictors in the regression model</p><h1 id="potential-problems">Potential Problems</h1><h2 id="non-linearity-of-the-data">Non-linearity of the Data</h2><p><strong>Assumption</strong>: The linear regression model assumes that there is a straight-line relationship between the predictors and the response.</p><p><strong>Residual plots</strong>: graphical tool for identifying non-linearity - Given a simple linear regression model, we can plot the residuals,<span class="math inline">\(e_i = y_i-\hat{y_i}\)</span> versus the predictor <span class="math inline">\(x_i\)</span>, or <span class="math inline">\(\hat{y_i}\)</span> when there are multiple predictors</p><p><img src="./18.png" width="600"></p><ul><li>Ideally, the residual plot will show no discernible pattern.</li><li>If the residual plot indicates non-linear associations in the data, then a simple approach is to use <strong>non-linear transformations</strong> of the predictors, such as <span class="math inline">\(\log{X},\sqrt{X}, X^2\)</span>, in the regression model.</li></ul><h2 id="correlation-of-error-terms">Correlation of Error Terms</h2><p><strong>Assumption</strong>: The error terms, <span class="math inline">\(\epsilon_1,\epsilon_2,...,\epsilon_n\)</span>, are uncorrelated. - If the errors are uncorrelated, then the fact that i is positive provides little or no information about the sign of i+1. - If the error terms are correlated, we may have an unwarranted sense of confidence in our model. - <strong>estimated standard errors</strong> will underestimate the true standard errors. - <strong>confidence and prediction intervals</strong> will be narrower than they should be. For example, a 95% confidence interval may in reality have a much lower probability than 0.95 of containing the true value of the parameter. - <strong>p-values</strong> will be lower than they should be - Lead to erroneously conclude that a parameter is statistically significant.</p><p><strong>Why might correlations among the error terms occur?</strong> - Such correlations frequently occur in the context of <strong>time series</strong> data - In many cases, observations that are obtained at adjacent time points will have <strong>positively correlated errors</strong>. - Plot the residuals from our model as a function of time to identify this correlation. - <strong>Tracking</strong>: If the error terms are positively correlated, then we may see <strong>tracking</strong> in the residuals—that is, adjacent residuals may have similar values. <img src="./19.png" width="600"></p><h2 id="non-constant-variance-of-error-terms">Non-constant Variance of Error Terms</h2><p><strong>Assumption</strong>: the error terms have a constant variance, <span class="math inline">\(Var(\epsilon_i) = σ^2\)</span>. - The standard errors, confidence intervals, and hypothesis tests associated with the linear model rely upon this assumption.</p><p>The variances of the error terms are non-constant. - For instance, the variances of the error terms may increase with the value of the response. - One can identify non-constant variances in the errors, or <strong>heteroscedasticity</strong>异方差性,from the presence of a funnel shape in residual plot. - <strong>Solution</strong>: transform the response Y using a concave function such as <span class="math inline">\(\log{Y}\)</span> or <span class="math inline">\(\sqrt{Y}\)</span> . Such a transformation results in a greater amount of shrinkage of the larger responses, leading to a reduction in <strong>heteroscedasticity</strong>.</p><p><img src="./20.png" width="600"></p><h2 id="outliers">Outliers</h2><p><strong>Outlier</strong>: is a point for which <span class="math inline">\(y_i\)</span> is far from the value predicted by the model. Outliers can arise for a variety of reasons, such as incorrect recording of an observation during data collection.</p><p><strong>Problems of Outlier</strong>: - Effect on the least squares fit, - Effect on interpretation of the fit - For instance, in this example, the RSE is 1.09 when the outlier is included in the regression, but it is only 0.77 when the outlier is removed.</p><p><strong>Residual Plots</strong> can be used to identify outliers</p><p><img src="./21.png" width="600"></p><ul><li>Difficult to decide how large a residual needs to be</li></ul><p><strong>Studentized residuals</strong>: computed by dividing each residual <span class="math inline">\(e_i\)</span> by its estimated standard error. - Observations whose studentized residuals are greater than 3 in abso- residual lute value are possible outliers.</p><h2 id="high-leverage-points">High Leverage Points</h2><p><strong>High Leverage</strong>:Observations with high leverage have an unusual value for xi - removing the high leverage observation has a much more substantial impact on the least squares line than removing the outlier. <img src="./23.png" width="600"></p><p><strong>Leverage Statistic</strong>: quantify an observation’s leverage</p><p>For a simple linear regression</p><p><span class="math display">\[\begin{align}h_i=\frac{1}{n}+\frac{(x_i-\bar{x})^2}{\sum_{i^{&#39;}=1}^n (x_{i^{&#39;}}-\bar{x})^2}\end{align}\]</span></p><ul><li><span class="math inline">\(h_i\)</span> increases with the distance of <span class="math inline">\(x_i\)</span> from <span class="math inline">\(\bar{x}\)</span>.</li><li><span class="math inline">\(h_i\)</span> is always between 1/n and 1, and the <strong>average leverage</strong> for all the observations is always equal to <span class="math inline">\((p+1)/n\)</span>.</li><li><strong>High leverage</strong>: a leverage statistic that greatly exceeds <span class="math inline">\((p+1)/n\)</span>, high leverage.</li></ul><p><img src="./22.png" width="600"></p><p>The right-hand panel of Figure 3.13 provides a plot of the studentized residuals versus <span class="math inline">\(h_i\)</span> for the data in the left-hand panel of Figure 3.13. Observation 41 stands out as having a very high leverage statistic as well as a high studentized residual. In other words, it is an outlier as well as a high leverage observation.</p><h2 id="collinearity">Collinearity</h2><p>Collinearity: situation in which two or more predictor variables are closely related to one another.</p><p><strong>Problems of Collinearity</strong></p><ul><li>Difficult to separate out the individual effects of collinear variables on the response</li><li>Uncertainty in the coefficient estimates.</li><li>Causes the standard error for <span class="math inline">\(\hat{β_j}\)</span> to grow</li><li>Recall that the t-statistic for each predictor is calculated by dividing <span class="math inline">\(\hat{β_j}\)</span> by its standard error. Consequently, collinearity results in a decline in the t-statistic. As a result, in the presence of collinearity, we may fail to reject H0 : βj = 0. This means that the <strong>power</strong> of the hypothesis test—the probability of correctly detecting a non-zero coefficient—is reduced by collinearity.</li></ul><p><img src="./24.png" width="600"></p><p><strong>Detection of Collinearity</strong></p><ul><li><strong>Correlation matrix</strong> of the predictors.</li><li>An element of this matrix that is large in absolute value indicates a pair of highly correlated variables.</li><li><strong>Situation Multicollinearity</strong>: it is possible for collinearity to exist between three or more variables even if no pair of variables has a particularly high correlation</li><li><strong>Variance Inflation Factor (VIF)</strong></li><li>The ratio of the variance of <span class="math inline">\(\hat{β_j}\)</span> when fitting the full model divided by the variance of <span class="math inline">\(\hat{β_j}\)</span> if fit on its own. The smallest possible value for VIF is 1, which indicates the complete absence of collinearity.</li><li>A VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity.</li></ul><p>The VIF for each variable,</p><p>where <span class="math inline">\(R^2_{X_j|X_{-j}}\)</span> is the <span class="math inline">\(R^2\)</span> from a regression of <span class="math inline">\(X_j\)</span> onto all of the other predictors. If <span class="math inline">\(R^2_{X_j|X_{-j}}\)</span> is close to one, then collinearity is present, and so the VIF will be large.</p><p><strong>Solution of Collinearity</strong></p><ul><li>Drop one of the problematic variables from the regression.</li><li>Combine the collinear variables together into a single predicto</li><li>E.g.: take the average of standardized versions of limit and rating in order to create a new variable that measures credit worthiness</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;qualitative-predictors&quot;&gt;Qualitative Predictors&lt;/h1&gt;
&lt;h2 id=&quot;predictors-with-only-two-levels&quot;&gt;Predictors with Only Two Levels&lt;/h2&gt;
&lt;p&gt;Suppose that we wish to investigate differences in credit card balance between males and females, ignoring the other variables for the moment. If a qualitative predictor (also known as a &lt;strong&gt;factor&lt;/strong&gt;) only has two &lt;strong&gt;levels&lt;/strong&gt;, or possible values, then incorporating it into a regression model is very simple. We simply create an indicator or &lt;strong&gt;dummy variable&lt;/strong&gt; that takes on two possible numerical values.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;./8.png&quot; width=&quot;300&quot;&gt; and use this variable as a predictor in the regression equation. This results in the model&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;./9.png&quot; width=&quot;600&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Linear Regression" scheme="https://nancyyanyu.github.io/tags/Linear-Regression/"/>
    
      <category term="Regression" scheme="https://nancyyanyu.github.io/tags/Regression/"/>
    
  </entry>
  
  <entry>
    <title>Linear Regression Part I: Linear Regression Models</title>
    <link href="https://nancyyanyu.github.io/posts/9b5af8e1/"/>
    <id>https://nancyyanyu.github.io/posts/9b5af8e1/</id>
    <published>2019-10-19T22:56:00.145Z</published>
    <updated>2019-10-19T22:56:00.145Z</updated>
    
    <content type="html"><![CDATA[<h1 id="simple-linear-regression-models">Simple Linear Regression Models</h1><h2 id="linear-regression-model">Linear Regression Model</h2><ul><li><p>Form of the linear regression model: <em><span class="math inline">\(f(X)=\beta_{0}+\sum_{j=1}^{p}X_{j}\beta_{j}\)</span></em>.</p></li><li><p>Training data: (<span class="math inline">\(x_1\)</span>,<span class="math inline">\(y_1\)</span>) ... (<span class="math inline">\(x_N\)</span>,<span class="math inline">\(y_N\)</span>). Each <span class="math inline">\(x_{i} =(x_{i1},x_{i2},...,x_{ip})^{T}\)</span> is a vector of feature measurements for the <span class="math inline">\(i\)</span>-th case.</p></li><li><p>Goal: estimate the parameters <span class="math inline">\(β\)</span></p></li><li><p>Estimation method: <strong>Least Squares</strong>, we pick the coeﬃcients <span class="math inline">\(β =(β_0,β_1,...,β_p)^{T}\)</span> to minimize the <strong>residual sum of squares</strong></p></li></ul><p><strong>Assumptions:</strong></p><ul><li>Observations <span class="math inline">\(y_i\)</span> are uncorrelated and have constant variance <span class="math inline">\(\sigma^2\)</span>;</li><li><span class="math inline">\(x_i\)</span> are ﬁxed (non random)</li><li>The regression function E(Y |X) is linear, or the linear model is a reasonable approximation.</li></ul><a id="more"></a><h2 id="residual-sum-of-squares">Residual Sum of Squares</h2><p><strong>Residual</strong>: <span class="math inline">\(e_i = y_i−\hat{y_i}\)</span> represents the ith residual—this is the difference between residual the ith observed response value and the ith response value that is predicted by our linear model. <span class="math display">\[\begin{align}RSS(\beta)&amp;=e_1^2+e_2^2+e_3^2+...e_n^2 \\&amp;=\sum_{i=1}^{N}(y_{i}-f(x_{i}))^2 \\&amp;=\sum_{i=1}^{N}(y_{i}-\beta_{0}-\sum_{j=1}^{p}X_{ij}\beta_{j})^2\end{align}\]</span></p><h4 id="solution">Solution</h4><p>Denote by <span class="math inline">\(X\)</span> the $N × (p + 1) $matrix with each row an input vector (with a 1 in the ﬁrst position), and similarly let <span class="math inline">\(y\)</span> be the <span class="math inline">\(N\)</span>-vector of outputs in the training set.</p><p><span class="math display">\[\begin{align} \min RSS(\beta)= (y-\mathbf{X}\beta)^T(y-\mathbf{X}\beta) \end{align}\]</span> A quadratic function in the <span class="math inline">\(p + 1\)</span> parameters</p><p>Taking derivatives:</p><p><span class="math display">\[\begin{align} \frac{\partial RSS}{\partial \beta}=-2\mathbf{X}^T(y-\mathbf{X}\beta) \end{align}\]</span></p><p><span class="math display">\[\begin{align} \frac{\partial^2 RSS}{\partial \beta \partial \beta^T}=2\mathbf{X}^T\mathbf{X}  \end{align}\]</span></p><p>Assuming (for the moment) that <span class="math inline">\(\mathbf{X}\)</span> has full column rank, and hence <span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> is positive deﬁnite, we set the ﬁrst derivative to zero: <span class="math inline">\(\mathbf{X}^T(y-\mathbf{X}\beta)=0\)</span></p><p><span class="math display">\[\begin{align}\Rightarrow \hat{\beta_1}&amp;=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^Ty \\&amp;=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2} \\\hat{\beta_0}&amp;=\bar{y}-\hat{\beta_1}\bar{x} \end{align}\]</span> where <span class="math inline">\(\bar{y}=\sum_{i=1}^ny_i/n\)</span>, <span class="math inline">\(\bar{x}=\sum_{i=1}^nx_i/n\)</span> are the <strong>sample means</strong>.</p><p>Fitted values at the training inputs: <span class="math inline">\(\hat{y}=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^Ty\)</span></p><p>Hat matrix: <span class="math inline">\(H=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\)</span></p><h2 id="assessing-the-accuracy-of-the-coefficient-estimates">Assessing the Accuracy of the Coefficient Estimates</h2><p>Assume that the true relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> takes the form <span class="math inline">\(Y = f(X) + \epsilon\)</span> for some unknown function <span class="math inline">\(f\)</span>, where <span class="math inline">\(\epsilon\)</span> is a mean-zero random error term.</p><p><strong>Least squares line</strong>: <span class="math display">\[\begin{align}\hat{y_i} = \hat{β_0} + \hat{β_1}x_i\end{align}\]</span> <strong>Population regression line</strong>: <span class="math display">\[\begin{align}Y=\beta_0+\beta_1X+\epsilon\end{align}\]</span> The <strong>error term</strong> is a catch-all for what we miss with this simple model: the true relationship is probably not linear, there may be other variables that cause variation in <span class="math inline">\(Y\)</span> , and there may be measurement error. We typically assume that the error term is independent of <span class="math inline">\(X\)</span>.</p><p><img src="./1_v2.png" width="600"></p><h3 id="population-v.s.-sample">Population V.S. Sample</h3><p>The true relationship is generally not known for real data, but the least squares line can always be computed using the coefficient estimates.</p><p><strong>Why there are two different lines describe the relationship between the predictor and the response?</strong></p><ul><li>The concept of these two lines is a natural extension of the standard statistical approach of using information from a sample to estimate characteristics of a large population.</li><li>The <strong>sample mean</strong> <span class="math inline">\(\bar{x}=\sum_{i=1}^nx_i/n\)</span> and the <strong>population mean</strong> <span class="math inline">\(\mu\)</span> are different, but in general the sample mean <span class="math inline">\(\bar{x}\)</span> will provide a good estimate of the population mean <span class="math inline">\(\hat{\mu}\)</span>.</li></ul><p><strong>Unbiased</strong></p><ul><li>If we use the sample mean <span class="math inline">\(\hat{\mu}\)</span> to estimate μ, this estimate is <strong>unbiased</strong>, in the sense that on average, we expect <span class="math inline">\(\hat{\mu}\)</span> to equal <span class="math inline">\(μ\)</span>.</li><li>An unbiased estimator does not systematically over- or under-estimate the true parameter.</li></ul><h3 id="standard-error">Standard Error</h3><p><strong>How accurate is the sample mean <span class="math inline">\(\hat{\mu}\)</span> as an estimate of μ?</strong></p><ul><li><strong>Standard error of <span class="math inline">\(\hat{\mu}\)</span>(SE(<span class="math inline">\(\hat{\mu}\)</span>)</strong>): average amount that this estimate <span class="math inline">\(\hat{\mu}\)</span> differs from the actual value of μ. <span class="math display">\[\begin{align}Var(\hat{\mu})=SE(\hat{\mu})^2=\frac{\sigma^2}{n}\end{align}\]</span> where <span class="math inline">\(σ\)</span> is the standard deviation of each of the realizations <span class="math inline">\(y_i\)</span> of <span class="math inline">\(Y\)</span> provided that the n observations are <strong>uncorrelated</strong>.</li></ul><p><strong>Standard Deviation V.S. Standard Error</strong></p><ul><li>The <strong>standard deviation (SD)</strong> measures the amount of variability, or dispersion, for a subject set of data from the mean</li><li>The <strong>standard error of the mean (SEM)</strong> measures how far the sample mean of the data is likely to be from the true population mean.</li></ul><p><img src="./2_v2.png" width="300"></p><p><strong>How close <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span> are to the true values <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>?</strong></p><ul><li><p><span class="math display">\[\begin{align}SE(\hat{\beta_0})^2&amp;=\sigma^2\left[\frac{1}{n}+\frac{\bar{x}^2}{\sum_{i=1}^n(x_i-\bar{x})^2}  \right]  \\SE(\hat{\beta_1})^2&amp;=\frac{\sigma^2}{\sum_{i=1}^n(x_i-\bar{x})^2} \end{align}\]</span></p><p>where <span class="math inline">\(\sigma^2 = Var(\epsilon)\)</span></p></li><li><p>For these formulas to be strictly valid, we need to assume that the errors <span class="math inline">\(\epsilon_i\)</span> for each observation are uncorrelated with common variance <span class="math inline">\(σ^2\)</span>.</p></li></ul><p><strong>Estimate <span class="math inline">\(\sigma^2\)</span></strong></p><ul><li><p><strong>residual standard error(RSE)</strong>: <span class="math inline">\(\sigma^2\)</span> is not known, but can be estimated from the data. This estimate is known as the <strong>residual standard error</strong></p></li><li><p><span class="math display">\[\begin{align}RSE=\sqrt{RSS/(n-2)}\end{align}\]</span></p></li></ul><h2 id="sampling-properties-of-beta">Sampling Properties of <span class="math inline">\(\beta\)</span></h2><p>The <u>variance–covariance</u> matrix of the least squares parameter estimates: <span class="math display">\[\begin{align} Var(\hat{\beta})=(\mathbf{X}^T\mathbf{X})^{-1}\sigma^2 \end{align}\]</span> <strong>Unbiased estimate of <span class="math inline">\(\sigma^2\)</span>:</strong> <span class="math display">\[\begin{align} \hat{\sigma}^2=\frac{1}{N-p-1}\sum^{N}_{i=1}(y_i-\hat{y_i})^2 \end{align}\]</span> Assume the deviations of <span class="math inline">\(\mathbf{Y}\)</span> around its expectation are <u>additive and Gaussian</u>: <span class="math display">\[\begin{align} Y=E(Y|X_1,...,X_p)+\epsilon=\beta_0+\sum_{j=1}^{p}X_j\beta_j+\epsilon \end{align}\]</span> where <span class="math inline">\(\epsilon \sim N(0,\sigma^2)\)</span></p><p>Thus, <span class="math inline">\(\beta\)</span> follows <u>multivariate normal distribution</u> with mean vector and variance–covariance matrix: <span class="math display">\[\begin{align}\hat{\beta} \sim N(\beta,(\mathbf{X}^T\mathbf{X})^{-1}\sigma^2 ) \end{align}\]</span> Also, a chi-squared distribution with <span class="math inline">\(N −p−1\)</span> degrees of freedom: <span class="math display">\[\begin{align} (N-p-1)\hat{\sigma}^2 \sim \sigma^2 \chi_{N-p-1}^{2} \end{align}\]</span> (<span class="math inline">\(\hat{\beta}\)</span> and <span class="math inline">\(\hat{\sigma^2}\)</span> are indep.)</p><p>We use these distributional properties to form tests of hypothesis and conﬁdence intervals for the parameters <span class="math inline">\(\beta_j\)</span>:</p><p><strong>Confidence Intervals</strong></p><ul><li><p><strong>A 95% confidence confidence interval</strong>: is defined as a range of values such that with 95% interval probability, the range will contain the true unknown value of the parameter.</p></li><li><p>For linear regression, the 95% confidence interval for <span class="math inline">\(β_1\)</span> approximately takes the form <span class="math display">\[\begin{align}&amp;\hat{\beta_1} \pm 2 \cdot SE(\hat{\beta_1})     \\&amp;SE(\hat{\beta_1}) =\frac{\sigma^2}{\sum_{i=1}^n(x_i-\bar{x})^2} \end{align}\]</span> (which relies on the assumption that the errors are Gaussian. Also, the factor of 2 in front of the <span class="math inline">\(SE(\hat{\beta_1})\)</span> term will vary slightly depending on the number of observations n in the linear regression. To be precise, rather than the number 2, it should contain the 97.5% quantile of a t-distribution with n−2 degrees of freedom.)</p><p>(which relies on the assumption that the errors are Gaussian. Also, the factor of 2 in front of the <span class="math inline">\(SE(\hat{\beta_1})\)</span> term will vary slightly depending on the number of observations n in the linear regression. To be precise, rather than the number 2, it should contain the 97.5% quantile of a t-distribution with n−2 degrees of freedom.)</p></li></ul><p><strong>In ESL book:</strong></p><p><span class="math inline">\(1-2\alpha\)</span> conﬁdence interval for <span class="math inline">\(\beta_j\)</span>: <span class="math display">\[\begin{align} (\hat{\beta_j}-z^{(1-\alpha)}\upsilon^{0.5}_j \hat{\sigma},\hat{\beta_j}+z^{(1-\alpha)}\upsilon^{0.5}_j \hat{\sigma}) \end{align}\]</span> where <span class="math inline">\(z^{(1-\alpha)}\)</span> is the 1 − α percentile of the normal distribution. <span class="math inline">\(\alpha\)</span> could be 0.025, 0.5, etc.</p><p>where <span class="math inline">\(z^{(1-\alpha)}\)</span> is the 1 − α percentile of the normal distribution. <span class="math inline">\(\alpha\)</span> could be 0.025, 0.5, etc.</p><p>In a similar fashion we can obtain an approximate confidence set for the entire parameter vector <span class="math inline">\(\beta\)</span>, namely <span class="math display">\[\begin{equation}C_\beta = \left\{ \beta \big| (\hat\beta-\beta)^T\mathbf{X}^T\mathbf{X}(\hat\beta-\beta) \le \hat\sigma^2{\chi^2_{p+1}}^{(1-\alpha)}\right\},\end{equation}\]</span> This condifence set for <span class="math inline">\(\beta\)</span> generates a corresponding confidence set for the true function <span class="math inline">\(f(x) = x^T\beta\)</span>: <span class="math inline">\(\left\{ x^T\beta \big| \beta \in C_\beta \right\}\)</span></p><h2 id="inference">Inference</h2><h3 id="hypothesis-tests">Hypothesis Tests</h3><p>The most common hypothesis test involves testing the <strong>null test hypothesis</strong> of</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">H_0: There is no relationship between X and Y or β1=0</span><br></pre></td></tr></table></figure><p>versus the <strong>alternative hypothesis</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">H_a : There is some relationship between X and Y or β1≠0</span><br></pre></td></tr></table></figure><p>To test the null hypothesis, we need to determine whether <span class="math inline">\(\hat{\beta_1}\)</span>, our estimate for <span class="math inline">\(\beta_1\)</span>, is sufficiently far from zero that we can be confident that <span class="math inline">\(\beta_1\)</span> is non-zero <span class="math inline">\(\Rightarrow\)</span> it depends on <span class="math inline">\(SE( \hat{\beta_1}\)</span>)</p><ul><li>If <span class="math inline">\(SE( \hat{\beta_1}\)</span>) is small, then even relatively small values of <span class="math inline">\(\hat{\beta_1}\)</span> may provide strong evidence that <span class="math inline">\(\beta_1 \neq 0\)</span>, and hence that there is a relationship between X and Y</li></ul><h4 id="t-statistic"><strong>T-statistic</strong>:</h4><p>To test the hypothesis that a particular coeﬃcient <span class="math inline">\(\beta_j= 0\)</span>, we form the standardized coeﬃcient or Z-score <span class="math display">\[\begin{align}t=\frac{\hat{\beta_1}-0}{SE(\hat{\beta_1})}  \\ or \quad z_j=\frac{\hat{\beta_j}-0}{\hat{\sigma}\sqrt{\upsilon_j}}\end{align}\]</span> where <span class="math inline">\(\upsilon_j\)</span> is the j-th diagonal element of <span class="math inline">\((\mathbf{X}^T\mathbf{X})^{-1}\)</span></p><p>which measures the number of standard deviations that <span class="math inline">\(\hat{\beta_1}\)</span> is away from 0.If there really is no relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> , then we expect it will have a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n−2\)</span> degrees of freedom.</p><p>Under the null hypothesis that <span class="math inline">\(\beta_j= 0\)</span>, <span class="math inline">\(z_j\)</span> is distributed as <span class="math inline">\(t_{N-p-1}\)</span>, and hence a large (absolute) value of <span class="math inline">\(z_j\)</span> will lead to rejection of this null hypothesis. If <span class="math inline">\(\hat{\sigma}\)</span> is replaced by a known value <span class="math inline">\(σ\)</span>, then <span class="math inline">\(z_j\)</span> would have a standard normal distribution.</p><h4 id="p-value"><strong>p-value</strong></h4><ul><li>The probability of observing any value <span class="math inline">\(≥ t\)</span> or <span class="math inline">\(≤ -t\)</span>, assuming <span class="math inline">\(β_1 = 0\)</span>.</li></ul><p><img src="./3_v2.png" width="300"> (Here |t|=2.17, p-value=0.015.The area in red is 0.015 + 0.015 = 0.030, 3%. If we had chosen a significance level of 5%, this would mean that we had achieved statistical significance. We would reject the null hypothesis in favor of the alternative hypothesis.)</p><ul><li><strong>Interpretation</strong>:a small p-value indicates<ul><li>It is unlikely to observe such a substantial association between the predictor and the response due to LUCK, in the absence of any real association between the predictor and the response.</li><li>There is an association between the predictor and the response.</li><li>We reject the null hypothesis—that is, we declare a relationship to exist between X and Y</li></ul></li></ul><h4 id="f-statistic"><strong>F-statistic</strong>:</h4><p>To test if a categorical variable with <span class="math inline">\(k\)</span> levels can be excluded from a model, we need to test whether the coeﬃcients of the dummy variables used to represent the levels can all be set to zero. Here we use the <span class="math inline">\(F\)</span> statistic：</p><p><span class="math display">\[\begin{align} F=\frac{(RSS_0-RSS_1)/(p_1-p_0)}{RSS_1/(N-p_1-1)} \end{align}\]</span></p><ul><li><span class="math inline">\(RSS_1\)</span> is the residual sum-of-squares for the least squares ﬁt of the bigger model with <span class="math inline">\(p_1+1\)</span> parameters;</li><li><span class="math inline">\(RSS_0\)</span> the same for the nested smaller model with <span class="math inline">\(p_0 +1\)</span> parameters, having <span class="math inline">\(p_1 −p_0\)</span> parameters constrained to be zero.</li></ul><p>The <span class="math inline">\(F\)</span> statistic measures the change in residual sum-of-squares per additional parameter in the bigger model, and it is normalized by an estimate of <span class="math inline">\(\sigma^2\)</span>.</p><p>Under the Gaussian assumptions, and the null hypothesis that the smaller model is correct, the <span class="math inline">\(F\)</span> statistic will have a <span class="math inline">\(F_{p_1-p_0,N-p_1-1}\)</span> distribution.</p><h2 id="the-gaussmarkov-theorem">The Gauss–Markov Theorem</h2><p>We focus on estimation of any linear combination of the parameters <span class="math inline">\(\theta=\alpha^T\beta\)</span>, for example, predictions <span class="math inline">\(f(x_0)=x^T_0\beta\)</span> are of this form.The least squares estimate of <span class="math inline">\(\alpha^T\beta\)</span> is: <span class="math display">\[\begin{equation}\hat{\theta}=\alpha^T\hat{\beta}=\alpha^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\end{equation}\]</span> Considering <span class="math inline">\(\mathbf{X}\)</span> to be ﬁxed, this is a linear function <span class="math inline">\(\mathbf{c}^T_0\mathbf{y}\)</span> of the response vector <span class="math inline">\(\mathbf{y}\)</span>.If we assume that the linear model is correct, <span class="math inline">\(\alpha^T\hat{\beta}\)</span> is unbiased.</p><p>The Gauss–Markov theorem states that if we have any other linear estimator <span class="math inline">\(\tilde{\theta}=\mathbf{c}^T\mathbf{y}\)</span> that is unbiased for <span class="math inline">\(\alpha^T\beta\)</span>, that is, <span class="math inline">\(E(\mathbf{c}^T\mathbf{y})= \alpha^T\beta\)</span>, then: <span class="math display">\[\begin{equation}Var(\alpha^T\hat{\beta})\leq Var(\mathbf{c}^T\mathbf{y})\end{equation}\]</span></p><h2 id="assessing-the-accuracy-of-the-model">Assessing the Accuracy of the Model</h2><p>The quality of a linear regression fit is typically assessed using two related quantities: <strong>the residual standard error (RSE)</strong> and the <strong>R2</strong> statistic.</p><h3 id="residual-standard-error">Residual Standard Error</h3><p>The <span class="math inline">\(RSE\)</span> is <u>an estimate of the standard deviation of <span class="math inline">\(\epsilon\)</span>: t</u>he average amount that the response will deviate from the true regression line <span class="math display">\[\begin{align}RSS&amp;=\sum_{i=1}^n(y_i-\hat{y})^2  \\RSE&amp;=\sqrt{\frac{1}{n-2}RSS}=\sqrt{\frac{1}{n-2}\sum_{i=1}^n(y_i-\hat{y})^2}\end{align}\]</span> <img src="./4_v2.png" width="600"></p><p>In the case of the advertising data, we see from the linear regression output in Table 3.2 that the RSE is 3.26. In other words, actual sales in each market deviate from the true regression line by approximately 3,260 units, on average.</p><p>The mean value of sales over all markets is approximately 14,000 units, and so the percentage error is 3,260/14,000 = 23%.</p><p><u>The RSE is considered a measure of the <strong>lack of fit</strong> of the model <span class="math inline">\(Y = β_0 + β_1X + \epsilon\)</span> to the data</u>.</p><h3 id="r2-statistic">R2 Statistic</h3><p>The <span class="math inline">\(RSE\)</span> is measured in the units of <span class="math inline">\(Y\)</span> , it is not always clear what constitutes a good <span class="math inline">\(RSE\)</span>.</p><p>The <span class="math inline">\(R^2\)</span> statistic takes the form of a <strong>proportion</strong>—<u>the proportion of variance explained</u>—and so it always takes on a value between 0 and 1, and isi ndependent of the scale of <span class="math inline">\(Y\)</span> . <span class="math display">\[\begin{align}R^2 = (TSS − RSS)/TSS= 1− RSS/TSS =  1-\frac{\sum(y_i-\hat{y})^2}{\sum(y_i-\bar{y})^2}\end{align}\]</span> <strong>TSS(total sum of squares)</strong>: <span class="math inline">\(\sum(y_i-\bar{y})^2\)</span> - the amount of variability inherent in the response before the regression is performed</p><p><strong>RSS</strong>: <span class="math inline">\(\sum_{i=1}^n(y_i-\hat{y})^2\)</span> - the amount of variabilityt hat is left unexplained after performing the regression</p><p><strong>(TSS−RSS)</strong>: measures the amount of variability in the response that is explained (or removed) by performing the regression, and <strong><span class="math inline">\(R^2\)</span> measures the proportion of variability in <span class="math inline">\(Y\)</span> that can be explained using <span class="math inline">\(X\)</span>.</strong></p><p><strong>Interpretation</strong>：</p><ul><li><u>close to 1</u> : a large proportion of the variability in the response has been explained by the regression.</li><li><u>close to 0</u> : the regression did not explain much of the variability in the response<ul><li>The linear model is wrong</li><li>The inherent error <span class="math inline">\(σ^2\)</span> is high, or both.</li></ul></li></ul><h2 id="squared-correlation-v.s.-r2-statistic">Squared Correlation V.S. R2 Statistic</h2><p><strong>Correlation</strong>: <span class="math display">\[\begin{align}Cor(X,Y)=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^n(x_i-\bar{x})^2}\sqrt{\sum_{i=1}^n(y_i-\bar{y})^2}}\end{align}\]</span> is also a measure of the linear relationship between X and Y.</p><blockquote><p>In the simple linear regression setting, <span class="math inline">\(R^2 = [Cor]^2\)</span>. In other words, the squared correlation and the R2 statistic are identical</p></blockquote><h2 id="note-of-p-value-v.s.-r-square">Note of P-Value v.s. R-Square</h2><p>Referring answer fromn <a href="https://www.researchgate.net/profile/Faye_Anderson3" target="_blank" rel="noopener">Faye Anderson</a>:</p><p>There is no established association/relationship between p-value and R-square. This all depends on the data (i.e.; contextual).</p><p>R-square value tells you how much variation is explained by your model. So 0.1 R-square means that your model explains 10% of variation within the data. The greater R-square the better the model. Whereas p-value tells you about the F statistic hypothesis testing of the &quot;fit of the intercept-only model and your model are equal&quot;. So if the p-value is less than the significance level (usually 0.05) then your model fits the data well.</p><p><u>Thus you have four scenarios:</u></p><p><strong>1) low R-square and low p-value (p-value &lt;= 0.05) :</strong> means that your model doesn't explain much of variation of the data but it is significant (better than not having a model)</p><p><strong>2) low R-square and high p-value (p-value &gt; 0.05) :</strong> means that your model doesn't explain much of variation of the data and it is not significant (worst scenario)</p><p><strong>3) high R-square and low p-value :</strong> means your model explains a lot of variation within the data and is significant (best scenario)</p><p><strong>4) high R-square and high p-value :</strong> means that your model explains a lot of variation within the data but is not significant (model is worthless)</p><h2 id="variance-bias">Variance &amp; Bias</h2><p>Consider the mean squared error of an estimator <span class="math inline">\(\tilde{\theta}\)</span> in estimating <span class="math inline">\(\theta\)</span>:</p><p><span class="math display">\[\begin{align}MSE(\tilde{\theta})&amp;= E(\tilde{\theta}-\theta)^2 \\&amp;= E(\tilde{\theta^2}+\theta^2-2\theta\tilde{\theta}) \\&amp;= E(\tilde{\theta^2})-E^2(\tilde{\theta})+E^2(\tilde{\theta})+E(\theta^2-2\theta\tilde{\theta})\\&amp;= Var(\tilde{\theta})+[E(\tilde{\theta})-\theta]^2\end{align}\]</span> The ﬁrst term is the <strong>variance</strong>, while the second term is the <strong>squared bias</strong>.</p><p>The <strong>Gauss-Markov theorem</strong> implies that the <em>least squares estimator</em> has the smallest mean squared error of all linear estimators with no bias. However, there may well exist a biased estimator with smaller mean squared error. <font color="red">Such an estimator would trade a little bias for a larger reduction in variance.</font></p><p>From a more pragmatic point of view, most models are distortions of the truth, and hence are biased; picking the right model amounts to creating the right balance between bias and variance.</p><p>Mean squared error is intimately related to prediction accuracy. Consider the prediction of the new response at input <span class="math inline">\(x_0\)</span>, <span class="math display">\[\begin{equation}y_0=f(x_0)+\epsilon_0\end{equation}\]</span></p><h2 id="prediction-error-mse">Prediction error &amp; MSE</h2><p>The expected prediction error of an estimate <span class="math inline">\(\tilde{f}(x_0)=x_0^T\tilde{\beta}\)</span>:</p><p><span class="math display">\[\begin{align}E(y_0-\tilde{f}(x_0))^2 &amp;=E(f(x_0)+\epsilon_0-x_0^T\tilde{\beta})^2 \\&amp;=E(\epsilon_0^2)+E(f(x_0)-x_0^T\tilde{\beta})^2-2E(\epsilon_0(f(x_0)-x_0^T\tilde{\beta})) \\&amp;=\sigma^2+E(f(x_0)-x_0^T\tilde{\beta})^2 \\&amp;=\sigma^2+MSE(\tilde{f}(x_0))\end{align}\]</span> Therefore, expected prediction error and mean squared error diﬀer only by the constant <span class="math inline">\(\sigma^2\)</span>, representing the variance of the new observation <span class="math inline">\(y_0\)</span>.</p><h1 id="multiple-linear-regression">Multiple Linear Regression</h1><p>Multiple linear regression model takes the form: <span class="math display">\[\begin{align}Y=\beta_0+\beta_1X_1+,,,+\beta_pX_p+\epsilon\end{align}\]</span></p><h2 id="estimating-the-regression-coefficients">Estimating the Regression Coefficients</h2><p>We choose <span class="math inline">\(β_0, β_1, . . . , β_p\)</span> to minimize the sum of squared residuals <span class="math display">\[\begin{align}RSS&amp;=\sum_{i=1}^n(y_i-\hat{y}_i)^2 \\&amp;=\sum_{i=1}^n(y_i-\hat{\beta_0}-\hat{\beta_1}x_{i1}-,,,-\hat{\beta_p}x_{ip})^2\end{align}\]</span> <img src="./5_v3.png" width="600"></p><p><strong>Does it make sense for the multiple regression to suggest no relationship between <em>sales</em> and <em>newspaper</em> while the simple linear regression implies the opposite?</strong></p><ul><li>Notice that the correlation between radio and newspaper is 0.35.</li><li>In markets where we spend more on radio our sales will tend to be higher, and as our correlation matrix shows, we also tend to spend more on newspaper advertising in those same markets.</li><li>Hence, in a simple linear regression which only examines sales versus newspaper, we will observe that higher values of newspaper tend to be associated with higher values of sales, even though newspaper advertising does not actually affect sales.</li></ul><h2 id="some-important-questions">Some Important Questions</h2><h3 id="is-there-a-relationship-between-the-response-and-predictors">1. Is There a Relationship Between the Response and Predictors?</h3><h4 id="hypothesis-test"><strong>Hypothesis Test</strong></h4><p>We use a hypothesis test to answer this question.</p><p>We test the <strong>null hypothesis</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">H_0 : β1 = β2 = · · · = βp = 0</span><br></pre></td></tr></table></figure><p>versus the <strong>alternative</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">H_a : at least one βj is non-zero</span><br></pre></td></tr></table></figure><p>This hypothesis test is performed by computing the <strong>F-statistic</strong>, <span class="math display">\[\begin{align}F=\frac{(TSS-RSS)/p}{RSS/(n-p-1)}\end{align}\]</span> where <span class="math inline">\(TSS =\sum(y_i − \bar{y})^2\)</span> and <span class="math inline">\(RSS =\sum(y_i−\hat{y}_i)^2\)</span>.</p><p>If the linear model assumptions are correct, one can show that <span class="math display">\[\begin{align}E[RSS/(n-p-1)]=\sigma^2\end{align}\]</span> and that, provided <span class="math inline">\(H_0\)</span> is true, <span class="math display">\[\begin{align}E[(TSS-RSS)/p]=\sigma^2\end{align}\]</span></p><ul><li>When there is no relationship between the response and predictors, one would expect the F-statistic to take on a value close to 1.</li><li>On the other hand, if <span class="math inline">\(H_a\)</span> is true, then <span class="math inline">\(E[(TSS-RSS)/p]&gt;\sigma^2\)</span>, so we expect <span class="math inline">\(F\)</span> to be greater than 1.</li></ul><p><img src="./6_v3.png" width="600"></p><h3 id="how-large-does-the-f-statistic-need-to-be-before-we-can-reject-h0-and-conclude-that-there-is-a-relationship"><strong>2. How large does the F-statistic need to be before we can reject H0 and conclude that there is a relationship?</strong></h3><ul><li>When n is large, an F-statistic that is just a little larger than 1 might still provide evidence against H_0.</li><li>In contrast, a larger F-statistic is needed to reject H_0 if n is small.</li><li>For the advertising data, the <strong>p-value</strong> associated with the F-statistic in Table 3.6 is essentially zero, so we have extremely strong evidence that at least one of the media is associated with increased sales.</li></ul><p><strong>To test that a particular subset of q of the coefficients are zero</strong></p><p>This corresponds to a null hypothesis</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">H_0 : β(p-q+1) = β(p-q+2) = · · · = βp = 0</span><br></pre></td></tr></table></figure><p>In this case we fit a second model that uses all the variables <strong>except those last q</strong>. Suppose that the residual sum of squares for that model is <span class="math inline">\(RSS_0\)</span>. Then the appropriate F-statistic is <span class="math display">\[\begin{align}F=\frac{(RSS_0-RSS)/q}{RSS/(n-p-1)}\end{align}\]</span></p><h4 id="f-statistics-v.s.-t-statistics"><strong>F-statistics v.s. t-statistics</strong></h4><ul><li><strong>Equivalency</strong>: In Table 3.4, for each individual predictor a t-statistic and a p-value were reported. These provide information about whether each individual predictor is related to the response, after adjusting for the other predictors. It turns out that each of these are exactly equivalent to the F-test that omits that single variable from the model, leaving all the others in—i.e. q=1 in the model. So it reports the <strong>partial effect</strong> of adding that variable to the model.</li></ul><blockquote><p>The square of each <em>t-statistic</em> is the corresponding <em>F-statistic</em>.</p></blockquote><ul><li><strong>p is large</strong>: If any one of the p-values for the individual variables is very small, then <strong><em>at least one of the predictors is related to the response</em></strong>. However, this logic is flawed, especially when the number of predictors p is large.<ul><li>If we use the individual t-statistics and associated p-values to decide whether there is any association between the variables and the response, high chance we will incorrectly conclude there is a relationship.</li><li>However, the F-statistic does not suffer from this problem because it adjusts for the number of predictors.</li></ul></li><li><strong>p &gt; n</strong>: more coefficients βj to estimate than observations from which to estimate them.<ul><li>cannot even fit the multiple linear regression model using least squares,</li></ul></li></ul><h3 id="do-all-the-predictors-help-to-explain-y-or-is-only-a-subset-of-the-predictors-useful">3. Do all the predictors help to explain Y , or is only a subset of the predictors useful?</h3><h4 id="variable-selection"><strong>Variable Selection</strong></h4><ul><li>Various statistics can be used to judge the quality of a model:<ul><li><strong>Mallow’s Cp, Akaike informa-Mallow’s Cp tion criterion (AIC)</strong></li><li><strong>Bayesian information criterion (BIC)</strong></li><li><strong>adjusted R2</strong></li></ul></li><li>There are three classical approaches to select models:<ul><li><strong>Forward selection</strong></li><li><strong>Backward selection</strong></li><li><strong>Mixed selection</strong></li></ul></li></ul><h3 id="how-well-does-the-model-fit-the-data">4. How well does the model fit the data?</h3><p>Two of the most common numerical measures of model fit are the <strong>RSE</strong> and <strong><span class="math inline">\(R^2\)</span></strong></p><h4 id="r2-statistics">R2 Statistics</h4><p>An <span class="math inline">\(R^2\)</span> value close to 1 indicates that the model explains a large portion of the variance in the response variable. <span class="math display">\[\begin{align}R^2 = (TSS − RSS)/TSS= 1− RSS/TSS\end{align}\]</span> Recall that in simple regression, <span class="math inline">\(R^2\)</span> is the square of the correlation of the response and the variable. In multiple linear regression, it turns out that it equals <span class="math inline">\(Cor(Y, \hat{Y} )^2\)</span>, the square of the correlation between the response and the fitted linear model; in fact one property of the fitted linear model is that it maximizes this correlation among all possible linear models.</p><p><strong><span class="math inline">\(R^2\)</span> will always increase when more variables are added to the model, even if those variables are only weakly associated with the response.</strong></p><ul><li>This is due to the fact that adding another variable tothe least squares equations must allow us to fit the training data (though not necessarily the testing data) more accurately.</li><li>The fact that adding <em>newspaper</em> advertising to the model containing only TV and radio advertising leads to just a tiny increase in R2 provides additional evidence that newspaper can be dropped from the model.</li></ul><h4 id="rse">RSE</h4><p><strong>RSE</strong> is defined as <span class="math display">\[\begin{align}RSE=\sqrt{\frac{RSS}{n-p-1}}\end{align}\]</span> Models with more variables can have higher RSE if the decrease in RSS is small relative to the increase in p.</p><h4 id="graphical-summaries">Graphical summaries</h4><p><img src="./7_v3.png" width="600"> It suggests a <strong>synergy</strong> or <strong>interaction</strong> effect between the advertising media, whereby combining the media together results in a bigger boost to sales than using any single medium</p><h3 id="given-a-set-of-predictor-values-what-response-value-should-we-predict-and-how-accurate-is-our-prediction">5. Given a set of predictor values, what response value should we predict, and how accurate is our prediction?</h3><h4 id="uncertainty-associated-with-prediction"><strong>Uncertainty associated with prediction</strong></h4><ol type="1"><li>The coefficient estimates <span class="math inline">\(\hat{\beta_0},\hat{\beta_1},...,\hat{\beta_p}\)</span> are estimates for <span class="math inline">\(β_0, β_1, . . . , β_p\)</span>. That is, the <strong>least squares plane</strong> <span class="math display">\[\begin{align}\hat{Y}=\hat{\beta_0}+\hat{\beta_1}X_1+,...+\hat{\beta_p}X_p\end{align}\]</span> is only an estimate for the <strong>true population regression plane</strong> <span class="math display">\[\begin{align}f(X)=\beta_0+\beta_1X_1+,...+\beta_pX_p\end{align}\]</span></li></ol><ul><li>The inaccuracy in the coefficient estimates is related to the <strong>reducible error</strong>.</li><li>We can compute a <strong>confidence interval</strong> in order to determine how close <span class="math inline">\(\hat{Y}\)</span> will be to f(X).</li></ul><ol start="2" type="1"><li>In practice assuming a linear model for <span class="math inline">\(f(X)\)</span> is almost always an approximation of reality, so there is an additional source of potentially <strong>reducible error</strong> which we call <strong>model bias</strong>.</li><li>Even if we knew <span class="math inline">\(f(X)\)</span>—true values for <span class="math inline">\(β_0, β_1, . . . , β_p\)</span>—the response value cannot be predicted perfectly because of the random error <span class="math inline">\(\epsilon\)</span> --<strong>irreducible error</strong>.</li></ol><ul><li>How much will Y vary from <span class="math inline">\(\hat{Y}\)</span>? -- <strong>prediction intervals</strong></li></ul><h4 id="prediction-intervals"><strong>Prediction intervals</strong></h4><p><strong>Prediction intervals</strong> are always wider than <strong>confidence intervals</strong></p><ul><li>Because they incorporate both <em>the error in the estimate for f(X) (the reducible error) and the uncertainty as to how much an individual point will differ from the population regression plane (the irreducible error).</em></li></ul><p>E.g.</p><ul><li><strong>confidence interval</strong> : quantify the uncertainty surrounding the average sales over a large number of cities.</li><li><strong>prediction interval</strong> : quantify the uncertainty surrounding sales for a particular city.</li></ul><h2 id="multiple-regression-from-simple-univariate-regression">Multiple Regression from Simple Univariate Regression</h2><p>Gram-Schmidt正交化</p><h3 id="simple-univariate-regression">Simple Univariate Regression</h3><p>Suppose ﬁrst that we have a univariate model with no intercept: <span class="math display">\[\begin{align}\mathbf{Y}=\mathbf{X}\beta+\epsilon\end{align}\]</span> The least squares estimate and residuals are: <span class="math display">\[\begin{align}\hat{\beta}&amp;=\frac{\sum_1^Nx_iy_i}{\sum_1^Nx_i^2} \\r_i&amp;=y_i-x_i\hat{\beta}\end{align}\]</span> The least squares estimate and residuals are: <span class="math display">\[\begin{align}\hat{\beta}&amp;=\frac{\sum_1^Nx_iy_i}{\sum_1^Nx_i^2} \\r_i&amp;=y_i-x_i\hat{\beta}\end{align}\]</span> Let <span class="math inline">\(\mathbf{y}=(y_1,...,y_N)^T\)</span>,<span class="math inline">\(\mathbf{x}=(x_1,...,x_N)^T\)</span></p><p>Define the <strong>inner product</strong> between <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>: <span class="math display">\[\begin{align}&lt;\mathbf{x},\mathbf{y}&gt;=\sum_1^Nx_iy_i=\mathbf{x}^T\mathbf{y}\end{align}\]</span> Then, <span class="math display">\[\begin{align}\hat{\beta}&amp;=\frac{&lt;\mathbf{x},\mathbf{y}&gt;}{&lt;\mathbf{x},\mathbf{x}&gt;} \\\mathbf{r}&amp;=\mathbf{y}-\mathbf{x}\hat{\beta}\end{align}\]</span> Suppose the inputs <span class="math inline">\(x_1, x_2,..., x_p\)</span> (the columns of the data matrix <span class="math inline">\(\mathbf{X}\)</span>) are orthogonal; that is <span class="math inline">\(&lt;\mathbf{x_k},\mathbf{x_j}&gt;=0\)</span>. Then the multiple least squares estimates <span class="math inline">\(\hat{\beta_j}\)</span> are equal to <span class="math inline">\(\frac{&lt;\mathbf{x_j},\mathbf{y}&gt;}{&lt;\mathbf{x_j},\mathbf{x_j}&gt;}\)</span>—the univariate estimates. In other words, <font color="red">when the inputs are orthogonal, they have no eﬀect on each other’s parameter estimates in the model.</font></p><h3 id="orthogonalization">Orthogonalization</h3><p><span class="math display">\[\begin{align}\hat{\beta}_1&amp;=\frac{&lt;\mathbf{x}-\bar{x}\mathbf{1},\mathbf{y}&gt;}{&lt;\mathbf{x}-\bar{x}\mathbf{1},\mathbf{x}-\bar{x}\mathbf{1}&gt;} \\\end{align}\]</span></p><ul><li><span class="math inline">\(\bar{x}=\sum_ix_i/N\)</span>;</li><li><span class="math inline">\(\mathbf{1}\)</span>, the vector of N ones;</li></ul><p><strong>Steps:</strong> 1. regress <span class="math inline">\(\mathbf{x}\)</span> on <span class="math inline">\(\mathbf{1}\)</span> to produce the residual <span class="math inline">\(\mathbf{z}=\mathbf{x}-\bar{x}\mathbf{1}\)</span>; 2. regress <span class="math inline">\(\mathbf{y}\)</span> on the residual <span class="math inline">\(\mathbf{z}\)</span> to give the coeﬃcient <span class="math inline">\(\hat{\beta}_1\)</span></p><p>Regress <span class="math inline">\(\mathbf{a}\)</span> on <span class="math inline">\(\mathbf{b}\)</span> (<span class="math inline">\(\mathbf{b}\)</span> is adjusted for <span class="math inline">\(\mathbf{a}\)</span>),(or <span class="math inline">\(\mathbf{b}\)</span> is <strong>“orthogonalized”</strong> with respect to <span class="math inline">\(\mathbf{a}\)</span>); a simple univariate regression of <span class="math inline">\(\mathbf{b}\)</span> on a with no intercept, producing coeﬃcient <span class="math inline">\(\hat{\lambda}=\frac{&lt;\mathbf{a},\mathbf{b}&gt;}{&lt;\mathbf{a},\mathbf{a}&gt;}\)</span> and residual vector $ - $.</p><p>The orthogonalization does not change the subspace spanned by <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, it simply produces an <strong>orthogonal basis</strong> for representing it.</p><h3 id="gramschmidt-procedure-for-multiple-regression">Gram–Schmidt procedure for multiple regression</h3><p><strong>ALGORITHM 3.1 Regression by Successive Orthogonalization</strong></p><ol type="1"><li>Initialize <span class="math inline">\(\mathbf{z_0}=\mathbf{x_0}=\mathbf{1}\)</span>.</li><li>For j=1,2,...,1,,...,p,<br> Regress <span class="math inline">\(\mathbf{x_j}\)</span> on <span class="math inline">\(\mathbf{z_0},\mathbf{z_1},...,\mathbf{z_{j-1}}\)</span> to produce coeﬃcients <span class="math inline">\(\hat{\lambda}_{l,j}=\frac{&lt;\mathbf{z_l},\mathbf{x_j}&gt;}{&lt;\mathbf{z_l},\mathbf{z_l}&gt;}\)</span>, l=0,1,...,j-1, and residual vector <span class="math inline">\(\mathbf{z_j}=\mathbf{x_j}-\sum_{k=0}^{j-1}\hat{\lambda_{kj}}\mathbf{z_k}\)</span></li><li>Regress <span class="math inline">\(\mathbf{y}\)</span> on the residual <span class="math inline">\(\mathbf{z_p}\)</span> to give the estimate <span class="math inline">\(\hat{\beta_p}=\frac{&lt;\mathbf{z_p},\mathbf{y}&gt;}{&lt;\mathbf{z_p},\mathbf{z_p}&gt;}\)</span>.</li></ol><p><strong>Note:</strong></p><ul><li>Each of the <span class="math inline">\(\mathbf{x}_j\)</span> is a linear combination of the <span class="math inline">\(\mathbf{z}_k\)</span>, <span class="math inline">\(k ≤ j\)</span>.</li><li>Since the <span class="math inline">\(\mathbf{z}_j\)</span> are all orthogonal, they form a basis for the column space of <span class="math inline">\(\mathbf{X}\)</span>, and hence the least squares projection onto this subspace is <span class="math inline">\(\mathbf{\hat{y}}\)</span>.</li><li>By rearranging the <span class="math inline">\(x_j\)</span> , any one of them could be in the last position, and a similar results holds.</li><li>The multiple regression coeﬃcient <span class="math inline">\(\mathbf{x}_j\)</span> represents the additional contribution of <span class="math inline">\(\mathbf{x}_j\)</span> on <span class="math inline">\(\mathbf{y}\)</span>, after <span class="math inline">\(\mathbf{x}_j\)</span> has been adjusted for <span class="math inline">\(x_0, x_1,..., x_{j−1},x_{j+1},..., x_p\)</span>.</li></ul><h3 id="precision-of-coefficient-estimation">Precision of Coefficient Estimation</h3><p>If <span class="math inline">\(\mathbf{x}_p\)</span> is highly correlated with some of the other <span class="math inline">\(\mathbf{x}_k\)</span>’s, the residual vector <span class="math inline">\(\mathbf{z}_p\)</span> will be close to zero, and the coeﬃcient <span class="math inline">\(\mathbf{x}_j\)</span> will be very unstable.</p><p>From <span class="math inline">\(\hat{\beta_p}=\frac{&lt;\mathbf{z_p},\mathbf{y}&gt;}{&lt;\mathbf{z_p},\mathbf{z_p}&gt;}\)</span>, we also obtain an alternate formula for the variance estimates:</p><p><span class="math display">\[\begin{align}Var(\hat{\beta}_p)=\frac{\sigma^2}{&lt;\mathbf{z_p},\mathbf{z_p}&gt;}=\frac{\sigma^2}{||\mathbf{z_p}||^2} \end{align}\]</span></p><p>The precision with which we can estimate <span class="math inline">\(\hat{\beta_p}\)</span> depends on the length of the residual vector <span class="math inline">\(\mathbf{z_p}\)</span>; this represents how much of <span class="math inline">\(\mathbf{x_p}\)</span> is unexplained by the other <span class="math inline">\(\mathbf{x_k}\)</span>’s</p><h3 id="qr-decomposition">QR decomposition</h3><p>We can represent step 2 of Algorithm 3.1 in matrix form:</p><p><span class="math display">\[\begin{align}\mathbf{X}=\mathbf{Z}\mathbf{Γ}\end{align}\]</span></p><ul><li><span class="math inline">\(\mathbf{Z}\)</span> has as columns the <span class="math inline">\(\mathbf{z_j}\)</span> (in order)</li><li><span class="math inline">\(\mathbf{Γ}\)</span> is the upper triangular matrix with entries <span class="math inline">\(\hat{\lambda}_{kj}\)</span></li></ul><p>Introducing the diagonal matrix D with jth diagonal entry <span class="math inline">\(D_{jj} = ||\mathbf{z_j}||\)</span>, we get</p><p><strong>QR decomposition of X</strong>:</p><p><span class="math display">\[\begin{align}\mathbf{X}=\mathbf{Z}\mathbf{D}^{-1}\mathbf{D}\mathbf{Γ}=\mathbf{Q}\mathbf{R}\end{align}\]</span></p><ul><li><span class="math inline">\(\mathbf{Q}\)</span> is an <span class="math inline">\(N ×(p+1)\)</span> orthogonal matrix, <span class="math inline">\(Q^TQ=I\)</span>;</li><li><span class="math inline">\(\mathbf{R}\)</span> is a $(p +1) × (p + 1) $Vupper triangular matrix.</li></ul><p><strong>Least squares solution:</strong> <span class="math display">\[\begin{align}\hat{\beta}&amp;=R^{-1}Q^T\mathbf{y} \\\mathbf{\hat{y}}&amp;=QQ^T\mathbf{y}\end{align}\]</span></p><h2 id="multiple-outputs">3.2.4 Multiple Outputs</h2><p>Suppose we have multiple outputs Y1,Y2,...,YK that we wish to predict from our inputs X0,X1,X2,...,Xp. We assume a linear model for each output:</p><p><span class="math display">\[\begin{align}Y_k&amp;=\beta_{0k}+\sum_{j=1}^pX_j\beta_{jk}+\epsilon_k \\&amp;=f_k(X)+\epsilon_k\end{align}\]</span></p><p>With N training cases we can write the model in matrix notation:</p><ul><li><p><span class="math display">\[\begin{align}Y=XB+E\end{align}\]</span></p></li><li><p>Y: N×K response matrix</p></li><li><p>X: N×(p+1) input matrix</p></li><li><p>B: (p+1)× K matrix of parameters</p></li><li><p>E: N×K matrix of errors</p></li></ul><p>A straightforward generalization of the univariate loss function:</p><p><span class="math display">\[\begin{align}RSS(B)&amp;=\sum_{k=1}^K\sum_{i=1}^N(y_{ik}-f_k(x_i))^2 \\&amp;=tr[(Y-XB)^T(Y-XB)]\end{align}\]</span> The least squares estimates have exactly the same form as before: <span class="math display">\[\begin{align}\hat{B}=(X^TX)^{-1}X^Ty\end{align}\]</span> If the errors <span class="math inline">\(\epsilon =(\epsilon_1,...,\epsilon_K)\)</span> in are correlated, suppose <span class="math inline">\(Cov(\epsilon)= Σ\)</span>, then the multivariate weighted criterion: <span class="math display">\[\begin{align}RSS(B;Σ)&amp;=\sum_{i=1}^N(y_{ik}-f_k(x_i))^TΣ^{-1}(y_{ik}-f_k(x_i)) \end{align}\]</span></p><h1 id="comparison-of-linear-regression-with-k-nearest-neighbors">Comparison of Linear Regression with K-Nearest Neighbors</h1><h2 id="parametric-v.s.-non-parametric">Parametric v.s. Non-parametric</h2><p>Linear regression is an example of a parametric approach because it assumes a linear functional form for f(X).</p><p><strong>Parametric methods</strong></p><ul><li><strong>Advantages</strong>:</li><li>Easy to fit, because one need estimate only a small number of coefficients.</li><li>Simple interpretations, and tests of statistical significance can be easily performed</li><li><strong>Disadvantage</strong>:</li><li>Strong assumptions about the form of f(X). If the specified functional form is far from the truth, and prediction accuracy is our goal, then the parametric method will perform poorly.</li></ul><p><strong>Non-parametric methods</strong></p><ul><li>Do not explicitly assume a parametric form for f(X), and thereby provide an alternative and more flexible approach for performing regression.</li><li>K-nearest neighbors regression (KNN regression)</li></ul><h2 id="knn-regression">KNN Regression</h2><p>Given a value for <span class="math inline">\(K\)</span> and a prediction point <span class="math inline">\(x_0\)</span>, KNN regression first identifies the <span class="math inline">\(K\)</span> training observations that are closest to <span class="math inline">\(x_0\)</span>, represented by <span class="math inline">\(N_0\)</span>. It then estimates <span class="math inline">\(f(x_0)\)</span> using the average of all the training responses in <span class="math inline">\(N_0\)</span>. <span class="math display">\[\begin{align}\hat{f}(x_0)=\frac{1}{K}\sum_{x_i\in N_0}y_i\end{align}\]</span></p><ul><li>The optimal value for K will depend on the <strong>bias-variance trade-off</strong>.</li><li>A small value for K provides the most flexible fit, which will have low bias but high variance. This variance is due to the fact that the prediction in a given region is entirely dependent on just one observation.</li><li>A larger values of K provide a smoother and less variable fit; the prediction in a region is an average of several points, and so changing one observation has a smaller effect. However, the smoothing may cause bias by masking some of the structure in f(X)</li></ul><p><strong>The parametric approach will outperform the nonparametric approach if the parametric form that has been selected is close to the true form of f.</strong></p><ul><li>A non-parametric approach incurs a cost in variance that is not offset by a reduction in bias</li><li>KNN performs slightly worse than linear regression when the relationship is linear, but much better than linear regression for non-linear situations. <img src="./26.png" width="600"></li></ul><p><strong>The increase in dimension has only caused a small deterioration in the linear regression test set MSE, but it has caused more than a ten-fold increase in the MSE for KNN.</strong></p><ul><li>This decrease in performance as the dimension increases is a common problem for KNN, and results from the fact that in higher dimensions there is effectively a reduction in sample size.<span class="math inline">\(\Rightarrow\)</span> <strong>curse of dimensionality</strong></li><li>As a general rule, parametric methods will tend to outperform non-parametric approaches when there is a small number of observations per predictor. <img src="./25.png" width="600"></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;simple-linear-regression-models&quot;&gt;Simple Linear Regression Models&lt;/h1&gt;
&lt;h2 id=&quot;linear-regression-model&quot;&gt;Linear Regression Model&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Form of the linear regression model: &lt;em&gt;&lt;span class=&quot;math inline&quot;&gt;\(f(X)=\beta_{0}+\sum_{j=1}^{p}X_{j}\beta_{j}\)&lt;/span&gt;&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Training data: (&lt;span class=&quot;math inline&quot;&gt;\(x_1\)&lt;/span&gt;,&lt;span class=&quot;math inline&quot;&gt;\(y_1\)&lt;/span&gt;) ... (&lt;span class=&quot;math inline&quot;&gt;\(x_N\)&lt;/span&gt;,&lt;span class=&quot;math inline&quot;&gt;\(y_N\)&lt;/span&gt;). Each &lt;span class=&quot;math inline&quot;&gt;\(x_{i} =(x_{i1},x_{i2},...,x_{ip})^{T}\)&lt;/span&gt; is a vector of feature measurements for the &lt;span class=&quot;math inline&quot;&gt;\(i\)&lt;/span&gt;-th case.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Goal: estimate the parameters &lt;span class=&quot;math inline&quot;&gt;\(β\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Estimation method: &lt;strong&gt;Least Squares&lt;/strong&gt;, we pick the coeﬃcients &lt;span class=&quot;math inline&quot;&gt;\(β =(β_0,β_1,...,β_p)^{T}\)&lt;/span&gt; to minimize the &lt;strong&gt;residual sum of squares&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Assumptions:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Observations &lt;span class=&quot;math inline&quot;&gt;\(y_i\)&lt;/span&gt; are uncorrelated and have constant variance &lt;span class=&quot;math inline&quot;&gt;\(\sigma^2\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(x_i\)&lt;/span&gt; are ﬁxed (non random)&lt;/li&gt;
&lt;li&gt;The regression function E(Y |X) is linear, or the linear model is a reasonable approximation.&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Linear Regression" scheme="https://nancyyanyu.github.io/tags/Linear-Regression/"/>
    
      <category term="Regression" scheme="https://nancyyanyu.github.io/tags/Regression/"/>
    
  </entry>
  
</feed>
