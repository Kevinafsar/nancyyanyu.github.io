<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang>
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">

<script>
    (function(){
        if(''){
            if (prompt('Show me your password') !== ''){
                alert('Blah, wrong.');
                history.back();
            }
        }
    })();
</script>


<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">






  
  
  <link rel="stylesheet" href="/lib/Han/dist/han.min.css?v=3.3">













  
  
  <link rel="stylesheet" href="/lib/fancybox/source/jquery.fancybox.css">







  

<link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">

<link rel="stylesheet" href="/css/main.css?v=7.1.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.2">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.1.2">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.1.2">


  <link rel="mask-icon" href="/images/logo.svg?v=7.1.2" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.1.2',
    sidebar: {"position":"right","display":"hide","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: true,
    fastclick: true,
    lazyload: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="Support Vector Machine 1. SVM v.s. Logistic Regression SVM Optimization problem: \[ \max_{\beta_0,...\beta_p,\epsilon_1,..,\epsilon_n} M \\ s.t.  \sum_{j=1}^p \beta_j^2=1,  \quad (9.13) \\  y_i(\beta_">
<meta name="keywords" content="Interview">
<meta property="og:type" content="article">
<meta property="og:title" content="Machine Learning Questions Part III: SVM">
<meta property="og:url" content="https://nancyyanyu.github.io/posts/c8f688ba/index.html">
<meta property="og:site_name" content="Nancy&#39;s Notes">
<meta property="og:description" content="Support Vector Machine 1. SVM v.s. Logistic Regression SVM Optimization problem: \[ \max_{\beta_0,...\beta_p,\epsilon_1,..,\epsilon_n} M \\ s.t.  \sum_{j=1}^p \beta_j^2=1,  \quad (9.13) \\  y_i(\beta_">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://nancyyanyu.github.io/posts/c8f688ba/1.png">
<meta property="og:image" content="https://nancyyanyu.github.io/posts/c8f688ba/2.png">
<meta property="og:image" content="https://nancyyanyu.github.io/posts/c8f688ba/4.png">
<meta property="og:image" content="https://nancyyanyu.github.io/posts/c8f688ba/5.png">
<meta property="og:updated_time" content="2019-06-18T21:46:08.638Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Machine Learning Questions Part III: SVM">
<meta name="twitter:description" content="Support Vector Machine 1. SVM v.s. Logistic Regression SVM Optimization problem: \[ \max_{\beta_0,...\beta_p,\epsilon_1,..,\epsilon_n} M \\ s.t.  \sum_{j=1}^p \beta_j^2=1,  \quad (9.13) \\  y_i(\beta_">
<meta name="twitter:image" content="https://nancyyanyu.github.io/posts/c8f688ba/1.png">



  <link rel="alternate" href="/atom.xml" title="Nancy's Notes" type="application/atom+xml">



  
  
  <link rel="canonical" href="https://nancyyanyu.github.io/posts/c8f688ba/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Machine Learning Questions Part III: SVM | Nancy's Notes</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Nancy's Notes</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">Code changes world!</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-resumé">

    
    
    
      
    

    

    <a href="/resume/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>Resumé</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-projects">

    
    
    
      
    

    

    <a href="/project" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br>Projects</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-ai">

    
    
    
      
    

    

    <a href="/categories/Machine-Learning" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>AI</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-big-data">

    
    
    
      
    

    

    <a href="/categories/Big-Data" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Big Data</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-journal">

    
    
    
      
    

    

    <a href="/categories/Journal/" rel="section"><i class="menu-item-icon fa fa-fw fa-coffee"></i> <br>Journal</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-手帳">

    
    
    
      
    

    

    <a href="/techou/" rel="section"><i class="menu-item-icon fa fa-fw fa-heart"></i> <br>手帳</a>

  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://nancyyanyu.github.io/posts/c8f688ba/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Nancy Yan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Nancy's Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Machine Learning Questions Part III: SVM

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-06-17 13:07:59" itemprop="dateCreated datePublished" datetime="2019-06-17T13:07:59-05:00">2019-06-17</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-06-18 16:46:08" itemprop="dateModified" datetime="2019-06-18T16:46:08-05:00">2019-06-18</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">11k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">10 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        <h2 id="support-vector-machine">Support Vector Machine</h2>
<h3 id="svm-v.s.-logistic-regression">1. SVM v.s. Logistic Regression</h3>
<p><strong>SVM Optimization problem</strong>: <span class="math display">\[
\max_{\beta_0,...\beta_p,\epsilon_1,..,\epsilon_n} M \\
s.t.  \sum_{j=1}^p \beta_j^2=1,  \quad (9.13) \\
 y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2},...+\beta_px_{ip})&gt;M(1-\epsilon_i) \quad \forall i=1,..,n.  \quad (9.14) \\
 \epsilon_i\geq0,\sum_{i=1}^p\epsilon_i \leq C, \quad (9.15)
\]</span> Rewrite the criterion (9.12)–(9.15) for fitting the support vector classifier <span class="math inline">\(f(X) = β_0 + β_1X_1 + . . . + β_pX_p\)</span> as <span class="math display">\[
\min_{\beta_0,...,\beta_p}\left\{ \sum_{i=1}^n\max[0,1-y_if(x_i)]+\lambda\sum_{j=1}^p\beta_j^2 \right\}
\]</span></p>
<ul>
<li>λ is small: few violations to the margin ; high-variance, low-bias; <span class="math inline">\(\Leftrightarrow\)</span> small <span class="math inline">\(C\)</span>;</li>
</ul>
<a id="more"></a>
<p><strong>“Loss + Penalty” form</strong>: <span class="math display">\[
\min_{\beta_0,...,\beta_p}\left\{ L(\mathbf{X},\mathbf{y},\beta)+\lambda P(\beta) \right\}
\]</span></p>
<ul>
<li><span class="math inline">\(L(\mathbf{X},\mathbf{y},\beta)\)</span> : loss function</li>
<li><span class="math inline">\(P(\beta)\)</span>: penalty function</li>
</ul>
<p><strong>Ridge regression and the lasso</strong>: <span class="math display">\[
L(\mathbf{X},\mathbf{y},\beta)=\sum_{i=1}^n \left( y_i-\beta_0-\sum_{j=1}^p x_{ij}\beta_j \right)^2 \\
P(\beta) = \sum_{j=1}^p \beta_j^2 \quad ridge \, regression \\
P(\beta) = \sum_{j=1}^p |\beta_j| \quad lasso
\]</span> <strong>SVM</strong>: <strong><em>hindge loss</em></strong> <span class="math display">\[
L(\mathbf{X},\mathbf{y},\beta)=\sum_{i=1}^n \max[0,1-y_i(\beta_0+\beta_1x_{i1}+,,,+\beta_px_{ip})]
\]</span> <strong>Optimization problems of linear SVM and (regularized) LR</strong>: <span class="math display">\[
\min_\beta \lambda||\beta||^2+\sum_{i=1}^n \max[0,1-y_i(\beta_0+\beta_1x_{i1}+,,,+\beta_px_{ip})] \\
\min_\beta \lambda||\beta||^2+\sum_{i=1}^n \log(1+\exp(1-y_i(\beta_0+\beta_1x_{i1}+,,,+\beta_px_{ip})))
\]</span> That is, they only differ in the loss function — <strong>SVM minimizes hinge loss while logistic regression minimizes logistic loss.</strong></p>
<ul>
<li>Logistic loss diverges faster than hinge loss. So, in general, it will be more sensitive to outliers.</li>
<li>Logistic loss does not go to zero even if the point is classified sufficiently confidently. This might lead to minor degradation in accuracy.</li>
</ul>
<p><strong>Main Difference</strong>:</p>
<ul>
<li>SVM try to maximize the margin between the closest support vectors while LR the posterior class probability. Thus, SVM find a solution which is as fare as possible for the two categories while LR has not this property.</li>
</ul>
<p><img src="./1.png" width="500"></p>
<ul>
<li>LR is more sensitive to outliers than SVM because the cost function of LR diverges faster than those of SVM. So putting an outlier on above picture would give below picture:</li>
</ul>
<p><img src="./2.png" width="500"></p>
<ul>
<li>Logistic Regression produces probabilistic values while SVM produces 1 or 0. So in a few words LR makes not absolute prediction and it does not assume data is enough to give a final decision. This maybe be good property when what we want is an estimation or we do not have high confidence into data.
<ul>
<li>In order to get discrete values <strong>1 or 0</strong> for the LR we can say that when a function value is greater than a threshold we classify as 1 and when a function value is smaller than the threshold we classify as 0.</li>
</ul></li>
</ul>
<p><strong>When to use which one?</strong></p>
<p><img src="./4.png" width="500"></p>
<h3 id="what-is-a-large-margin-classifier">2. What is a large margin classifier?</h3>
<p><strong><em>Margin</em></strong>: the smallest (perpendicular) distance from each training observation to a given separating hyperplane <span class="math inline">\(\Rightarrow\)</span> the minimal distance from the observations to the hyperplane.</p>
<p><strong><em>Maximal margin hyperplane</em></strong>: the separating hyperplane that is farthest from the training observations.</p>
<ul>
<li>The maximal margin hyperplane is the separating hyperplane for which the <em>margin</em> is <strong>largest</strong></li>
<li>Overfitting when <span class="math inline">\(p\)</span> is large.</li>
</ul>
<p><strong><em>Maximal margin classifier</em></strong>: classify a test observation based on which side of the maximal margin hyperplane it lies.</p>
<p>The <strong>maximal margin hyperplane</strong> is the solution to the optimization problem <span class="math display">\[
\max_{\beta_0,...\beta_p} M \\
s.t.  \sum_{j=1}^p \beta_j^2=1,  \quad (9.10) \\
 y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2},...+\beta_px_{ip})&gt;M \quad \forall i=1,..,n.  \quad (9.11)
\]</span></p>
<ul>
<li>The constraint in (9.11) in fact requires that each observation be on the correct side of the hyperplane, with some cushion, provided that <strong>margin</strong> <span class="math inline">\(M\)</span> is positive.)</li>
<li>The constraint in (9.10) makes sure the perpendicular distance from the i-th observation to the hyperplane is given by</li>
</ul>
<p><span class="math display">\[
y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2},...+\beta_px_{ip})
\]</span></p>
<h3 id="why-svm-is-an-example-of-a-large-margin-classifier">3. Why SVM is an example of a large margin classifier?</h3>
<p><strong><em>Support Vector Classifier (Soft Margin Classifier)</em></strong>: Rather than seeking the largest possible margin that every observation is not only on the correct side of the hyperplane but also on the correct side of the margin, we instead allow some observationsto be on the incorrect side of the margin, or even the incorrect side of the hyperplane.</p>
<p>​ <strong>Optimization problem</strong>: <span class="math display">\[
\max_{\beta_0,...\beta_p,\epsilon_1,..,\epsilon_n} M \\
s.t.  \sum_{j=1}^p \beta_j^2=1,  \quad (9.13) \\
 y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2},...+\beta_px_{ip})&gt;M(1-\epsilon_i) \quad \forall i=1,..,n.  \quad (9.14) \\
 \epsilon_i\geq0,\sum_{i=1}^p\epsilon_i \leq C, \quad (9.15)
\]</span></p>
<ul>
<li><strong><em>Slack variables</em></strong>: <span class="math inline">\(\epsilon_1,..,\epsilon_n\)</span> - allow individual observations to be on the wrong side of the margin or the hyperplane
<ul>
<li><span class="math inline">\(\epsilon_i=0\)</span>: the i-th observation is on the correct side of the <em>margin</em></li>
<li><span class="math inline">\(\epsilon_i &gt;0\)</span>: the i-th observation is on the wrong side of the <em>margin</em> <span class="math inline">\(\Rightarrow\)</span> i-th observation <strong><em>violated</em></strong> the margin.</li>
<li><span class="math inline">\(\epsilon_i &gt;1\)</span>: the i-th observation is on the wrong side of the <em>hyperplane</em></li>
</ul></li>
<li>Classify the test observation based on the sign of <span class="math inline">\(f(x^∗) = \beta_0+\beta_1x_{1}^*+\beta_2x_{2}^*,...+\beta_px_{p}^*\)</span>.</li>
</ul>
<p>The <strong><em>support vector machine (SVM)</em></strong> is an extension of the support vector classifier that results from enlarging the feature space using <strong>kernels</strong>.</p>
<h3 id="svm-being-a-large-margin-classifier-is-it-influenced-by-outliers">4. SVM being a large margin classifier, is it influenced by outliers?</h3>
<p>Yes, if C is large, otherwise not</p>
<h3 id="what-is-the-role-of-c-in-svm">5. What is the role of C in SVM?</h3>
<p><strong><em>Tuning parameter C</em></strong>: <span class="math inline">\(C\)</span> bounds the sum of the <span class="math inline">\(\epsilon_i\)</span>'s, and so it determines the number and severity of the violationsto the margin(and to the hyperplane) that we will tolerate.</p>
<ul>
<li><strong><em>budget</em></strong> for the amount that the margin can be violated by the <span class="math inline">\(n\)</span> observations.</li>
<li>Generally chosen via <em>cross-validation</em>.</li>
<li><span class="math inline">\(C\)</span> controls the <strong>bias-variance trade-off</strong> of the support vector classifier.
<ul>
<li>C is small: highly fit to the data, fewer support vectors <span class="math inline">\(\Rightarrow\)</span> low bias , high variance;</li>
<li>C is large: margin wider, many support vectors <span class="math inline">\(\Rightarrow\)</span> high bias , low variance;</li>
</ul></li>
</ul>
<h3 id="what-is-a-kernel-in-svm-why-do-we-use-kernels-in-svm">6. What is a kernel in SVM? Why do we use kernels in SVM?</h3>
<p><strong>Kernel</strong>: Kernel is a function that quantifies the similarity of two observations.</p>
<ul>
<li><strong><em>Linear kernel</em></strong>: <span class="math inline">\(K(x_i,x_{i^{&#39;}})=\sum_{j=1}^px_{ij}x_{i^{&#39;}j}\)</span>
<ul>
<li>Linear kernel essentially quantifies the similarity of a pair of observations using <strong>Pearson</strong> (standard) correlation.</li>
</ul></li>
<li><strong><em>Polynomial kernel</em></strong> of degree d: <span class="math inline">\(K(x_i,x_{i^{&#39;}})=(1+\sum_{j=1}^px_{ij}x_{i^{&#39;}j})^d\)</span>
<ul>
<li>fitting a support vector classifier in a higher-dimensional space involving polynomials of degree <span class="math inline">\(d\)</span>.</li>
</ul></li>
<li><strong><em>Radial kernel</em></strong>: <span class="math inline">\(K(x_i,x_{i^{&#39;}})=\exp(-\gamma \sum_{j=1}^p(x_{ij}-x_{i^{&#39;}j})^2)\)</span>
<ul>
<li>Radial kernel has very <em>local</em> behavior: only nearby training observations have an effect on the class label of a test observation
<ul>
<li>If a given test observation <span class="math inline">\(x^∗ = (x^∗_1 . . .x^∗_p)^T\)</span> is far from a training observation <span class="math inline">\(x_i\)</span> in terms of <strong><em>Euclidean distance</em></strong>; <span class="math inline">\(\Rightarrow\)</span> $ _{j=1}<sup>p(x_{ij}-x_{i</sup>{'}j})^2 $ will be large <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(K(x_i,x_{i^{&#39;}})=\exp(-\gamma \sum_{j=1}^p(x_{ij}-x_{i^{&#39;}j})^2)\)</span> will be very tiny. <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(x_i\)</span> will play virtually no role in <span class="math inline">\(f(x^∗)\)</span>.</li>
</ul></li>
</ul></li>
</ul>
<p><strong><em>Support Vector Machine</em></strong>: When the support vector classifieris combined with a non-linear kernel, the resulting classifier is known as a support vector machine. <span class="math display">\[
f(x)=\beta_0+\sum_{i \in S}^n \alpha_i K(x,x_i)
\]</span> In machine learning, a <strong>“kernel”</strong> is usually used to refer to the kernel trick, a method of <strong><em>using a linear classifier to solve a non-linear problem</em></strong>. The kernel function is what is applied on each data instance to map the original non-linear observations into a higher-dimensional space in which they become <em>separable</em>.</p>
<p><strong>Advantage of Kernel over enlarging the feature space using functions of the original features: </strong></p>
<ul>
<li><strong><em>Computational</em></strong>: one need only compute <span class="math inline">\(K(x_i,x_{i^{&#39;}})\)</span> for all <span class="math inline">\(\left(\begin{array}{c}n\\ 2\end{array}\right)\)</span> distinct pairs <span class="math inline">\(i, i^{&#39;}\)</span>. This can bedone without explicitly working in the <em>enlarged feature space.</em>
<ul>
<li><strong>Curse of dimensionality</strong>: for some kernels, such as the radial kernel, the feature space is implicit and infinite-dimensional.</li>
</ul></li>
</ul>
<h3 id="can-we-apply-the-kernel-trick-to-logistic-regression-why-is-it-not-used-in-practice-then">7. Can we apply the kernel trick to logistic regression? Why is it not used in practice then?</h3>
<p>While converting the primal SVM formulation into its dual form (which gives us the kernel version of the SVM), we notice that one of the equations we get is, <span class="math display">\[
\beta=\sum_{i=1}^n \alpha_iy_ix_i
\]</span> This, in the kernelized version where the kernel <span class="math inline">\(k\)</span> has an implicit representation for points given by x <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(\phi(x)\)</span>, <span class="math display">\[
\beta=\sum_{i=1}^n \alpha_iy_i\phi(x_i)
\]</span> Surprisingly, one can also get this form from the representer theorem [2]. This suggests something general about the classifiers we learn.</p>
<p>Now, let us get back to logistic regression, which is modeled as, <span class="math display">\[
p(y=1|x)=\frac{1}{1+\exp(-\beta^Tx)}
\]</span> First of all, let us map the x to the space of implicit representation of the kernel. So, our model in the implicit representation space would look like, <span class="math display">\[
p(y=1|x)=\frac{1}{1+\exp(-\beta^T\phi(x))}
\]</span> Next, let us use the form of <span class="math inline">\(\beta\)</span>, we observed from SVM and the representer theorem. This will give us, <span class="math display">\[
p(y=1|x)=\frac{1}{1+\exp(-\sum_{i=1}^n\alpha_iy_i\phi(x_i)^T\phi(x))} \\
=\frac{1}{1+\exp(-\sum_{i=1}^n\alpha_iy_iK(x_i,x))}
\]</span> This gives us the kernelized logistic regression model.</p>
<p><strong>Why is it not used in practice then?</strong>: logistic regression with kernels is merely an <strong>SVM</strong> without maximum margins</p>
<h3 id="how-does-the-svm-guassian-kernel-parameter-lambda-affect-the-biasvariance-trade-off">8. How does the SVM Guassian kernel parameter <span class="math inline">\(\lambda\)</span> affect the bias/variance trade off?</h3>
<p><strong>Larger RBF kernel bandwidths (i.e. smaller 𝛾γ) produce smoother decision boundaries because they produce smoother feature space mappings</strong>. Smoother mappings produce simpler decision boundaries:</p>
<p><img src="./5.png" width="700"> <span class="math display">\[
K(x_i,x_j)=\exp(-\lambda ||x_i-x_j||^2)
\]</span></p>
<ul>
<li><strong>Small <span class="math inline">\(\lambda\)</span>:</strong> a Gaussian with a <em>large variance</em> so the influence of <span class="math inline">\(x_j\)</span> is more, i.e. if <span class="math inline">\(x_j\)</span> is a support vector, the class of this support vector will have influence on deciding the class of the vector <span class="math inline">\(x_i\)</span> even if the distance between them is large <span class="math inline">\(\rightarrow\)</span> <strong>Larger RBF kernel bandwidths</strong> <span class="math inline">\(\rightarrow\)</span> smoother decision boundaries $  $ <strong>high bias , low variance</strong></li>
<li><strong>Large <span class="math inline">\(\lambda\)</span>:</strong> variance is small <span class="math inline">\(\rightarrow\)</span> support vector does not have wide-spread influence. Technically speaking <span class="math inline">\(\rightarrow\)</span> <strong>low bias , high variance</strong>.</li>
</ul>
<p><strong>Ref</strong>:</p>
<p><a href="https://github.com/Sroy20/machine-learning-interview-questions" target="_blank" rel="noopener">machine-learning-interview-questions</a></p>
<p><a href="https://towardsdatascience.com/support-vector-machine-vs-logistic-regression-94cc2975433f" target="_blank" rel="noopener">Support Vector Machine vs Logistic Regression</a></p>
<p><a href="https://towardsdatascience.com/kernel-function-6f1d2be6091" target="_blank" rel="noopener">Kernel Functions</a></p>
<p><a href="https://www.quora.com/What-are-kernels-in-machine-learning-and-SVM-and-why-do-we-need-them" target="_blank" rel="noopener">What are kernels in machine learning and SVM and why do we need them?</a></p>
<p><a href="https://www.quora.com/How-can-one-use-kernels-utilizing-the-kernel-trick-in-logistic-regression" target="_blank" rel="noopener">How can one use kernels (utilizing the kernel trick) in logistic regression?</a></p>
<p><a href="https://www.quora.com/What-are-C-and-gamma-with-regards-to-a-support-vector-machine" target="_blank" rel="noopener">What are C and gamma with regards to a support vector machine?</a></p>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      
        
          
        
        <div class="post-tags">
          
            <a href="/tags/Interview/" rel="tag"># Interview</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/posts/a2f8a358/" rel="next" title="Machine Learning Questions Part II: COD, Reg, Model Evaluation, Dimensionality Reduction">
                <i class="fa fa-chevron-left"></i> Machine Learning Questions Part II: COD, Reg, Model Evaluation, Dimensionality Reduction
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/posts/25b6d1fa/" rel="prev" title="Machine Learning Questions - Part IV: Clustering & Bayesian">
                Machine Learning Questions - Part IV: Clustering & Bayesian <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div class="comments" id="comments">
      
        <div id="gitment-container"></div>
      
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Nancy Yan</p>
              <div class="site-description motion-element" itemprop="description"></div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives">
                
                    <span class="site-state-item-count">50</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">4</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">39</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                  
                    
                  
                  <a href="https://github.com/nancyyanyu" title="GitHub &rarr; https://github.com/nancyyanyu" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i></a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                  
                    
                  
                  <a href="mailto:yy2799@columbia.edu" title="E-Mail &rarr; mailto:yy2799@columbia.edu" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i></a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                  
                    
                  
                  <a href="https://www.linkedin.com/in/nancy-yanyu-yan" title="LinkedIn &rarr; https://www.linkedin.com/in/nancy-yanyu-yan" rel="noopener" target="_blank"><i class="fa fa-fw fa-linkedin"></i></a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                  
                    
                  
                  <a href="https://instagram.com/nancy_yanyu_yan" title="Instagram &rarr; https://instagram.com/nancy_yanyu_yan" rel="noopener" target="_blank"><i class="fa fa-fw fa-instagram"></i></a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#support-vector-machine"><span class="nav-number">1.</span> <span class="nav-text">Support Vector Machine</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#svm-v.s.-logistic-regression"><span class="nav-number">1.1.</span> <span class="nav-text">1. SVM v.s. Logistic Regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#what-is-a-large-margin-classifier"><span class="nav-number">1.2.</span> <span class="nav-text">2. What is a large margin classifier?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#why-svm-is-an-example-of-a-large-margin-classifier"><span class="nav-number">1.3.</span> <span class="nav-text">3. Why SVM is an example of a large margin classifier?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#svm-being-a-large-margin-classifier-is-it-influenced-by-outliers"><span class="nav-number">1.4.</span> <span class="nav-text">4. SVM being a large margin classifier, is it influenced by outliers?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#what-is-the-role-of-c-in-svm"><span class="nav-number">1.5.</span> <span class="nav-text">5. What is the role of C in SVM?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#what-is-a-kernel-in-svm-why-do-we-use-kernels-in-svm"><span class="nav-number">1.6.</span> <span class="nav-text">6. What is a kernel in SVM? Why do we use kernels in SVM?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#can-we-apply-the-kernel-trick-to-logistic-regression-why-is-it-not-used-in-practice-then"><span class="nav-number">1.7.</span> <span class="nav-text">7. Can we apply the kernel trick to logistic regression? Why is it not used in practice then?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#how-does-the-svm-guassian-kernel-parameter-lambda-affect-the-biasvariance-trade-off"><span class="nav-number">1.8.</span> <span class="nav-text">8. How does the SVM Guassian kernel parameter \(\lambda\) affect the bias/variance trade off?</span></a></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Nancy Yan</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
    <span title="Symbols count total">358k</span>
  

  
</div>









        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>





  



  






  



  
    
    
      
    
  
  <script color="0,0,255" opacity="0.5" zindex="-1" count="99" src="//cdn.bootcss.com/canvas-nest.js/1.0.1/canvas-nest.min.js"></script>



  
  



  
  



  
  



  
  
  <script id="ribbon" size="300" alpha="0.6" zindex="-1" src="/lib/canvas-ribbon/canvas-ribbon.js"></script>





  
  <script src="//cdn.jsdelivr.net/jquery/2.1.3/jquery.min.js"></script>

  
  <script src="//cdn.jsdelivr.net/fastclick/1.0.6/fastclick.min.js"></script>

  
  <script src="//cdn.jsdelivr.net/npm/jquery-lazyload@1/jquery.lazyload.min.js"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script src="/lib/fancybox/source/jquery.fancybox.pack.js"></script>

  
  <script src="/lib/three/three.min.js"></script>

  
  <script src="/lib/three/three-waves.min.js"></script>

  
  <script src="/lib/three/canvas_lines.min.js"></script>

  
  <script src="/lib/three/canvas_sphere.min.js"></script>


  


  <script src="/js/utils.js?v=7.1.2"></script>

  <script src="/js/motion.js?v=7.1.2"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.1.2"></script>




  
  <script src="/js/scrollspy.js?v=7.1.2"></script>
<script src="/js/post-details.js?v=7.1.2"></script>



  


  <script src="/js/next-boot.js?v=7.1.2"></script>


  

  

  

  


  
    
<!-- LOCAL: You can save these files to your site and update links -->

  
  <script src="https://www.wenjunjiang.win/js/gitment.js"></script>

<link rel="stylesheet" href="https://www.wenjunjiang.win/css/gitment.css">
<!-- END LOCAL -->


<script>
  function renderGitment() {
    var gitment = new Gitment({
      id: window.location.pathname,
      owner: 'nancyyanyu',
      repo: 'nancyyanyu.github.io',
      
      oauth: {
      
      
        client_secret: '75adc257166813deff478053f3f05133285d6cf0',
      
        client_id: '90ddd3d00d8930cb0d84'
      }
    });
    gitment.render('gitment-container');
  }

  
    renderGitment();
  
</script>

  




  




  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

    
  


  

  

  

  

  

  

  

  

  

  

  

</body>
</html>

<script type="text/javascript" src="/js/src/dynamic_bg.js"></script>

