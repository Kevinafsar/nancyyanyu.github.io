<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang>
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">

<script>
    (function(){
        if(''){
            if (prompt('Show me your password') !== ''){
                alert('Blah, wrong.');
                history.back();
            }
        }
    })();
</script>


<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">






  
  
  <link rel="stylesheet" href="/lib/Han/dist/han.min.css?v=3.3">













  
  
  <link rel="stylesheet" href="/lib/fancybox/source/jquery.fancybox.css">







  

<link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">

<link rel="stylesheet" href="/css/main.css?v=7.1.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.2">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.1.2">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.1.2">


  <link rel="mask-icon" href="/images/logo.svg?v=7.1.2" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.1.2',
    sidebar: {"position":"right","display":"hide","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: true,
    fastclick: true,
    lazyload: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta property="og:type" content="website">
<meta property="og:title" content="Nancy&#39;s Notes">
<meta property="og:url" content="https://nancyyanyu.github.io/index.html">
<meta property="og:site_name" content="Nancy&#39;s Notes">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Nancy&#39;s Notes">



  <link rel="alternate" href="/atom.xml" title="Nancy's Notes" type="application/atom+xml">



  
  
  <link rel="canonical" href="https://nancyyanyu.github.io/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Nancy's Notes</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-right 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Nancy's Notes</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">Code changes world!</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home menu-item-active">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-resumé">

    
    
    
      
    

    

    <a href="/resume/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>Resumé</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-machine-learning">

    
    
    
      
    

    

    <a href="/categories/Machine-Learning" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Machine Learning</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-journal">

    
    
    
      
    

    

    <a href="/categories/Journal/" rel="section"><i class="menu-item-icon fa fa-fw fa-coffee"></i> <br>Journal</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-手帳">

    
    
    
      
    

    

    <a href="/techou/" rel="section"><i class="menu-item-icon fa fa-fw fa-heart"></i> <br>手帳</a>

  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://nancyyanyu.github.io/undefined/2a71b2a0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Nancy Yan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Nancy's Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/undefined/2a71b2a0/" class="post-title-link" itemprop="url">Hadoop MapReduce Streaming Application in Python</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-06-27 18:15:02 / Modified: 18:18:28" itemprop="dateCreated datePublished" datetime="2019-06-27T18:15:02-05:00">2019-06-27</time>
            

            
              

              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Big-Data/" itemprop="url" rel="index"><span itemprop="name">Big Data</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">8.3k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">8 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          
            <h1 id="unreliable-components">Unreliable Components</h1>
<p>3 different unreliable components in distributed systemsnodes: nodes, links and clock. Distributed computational systems: - built from unreliable components - Cluster nodes can break any time because of power supply, disk damages, overheated CPUs, and so on</p>
<p>3 different unreliable components in distributed systemsnodes: nodes, links and clock.</p>
<h2 id="nodes">Nodes</h2>
<p><strong>3 types of node failures:</strong></p>
<ul>
<li><strong>Fail-Stop</strong>: if machines get out of service during a computation then you have to have an external impact to bring system back to a working state. &gt; A system administrator should either</li>
<li>fix the node and reboot the whole system or part of it. Or <img src="./images/week2_0.png" width="200"></li>
<li>retire the broken machine and reconfigure the distributed system</li>
</ul>
<p><img src="./images/week2_1.png" width="200"></p>
<ul>
<li><strong>Fail-Recovery</strong>: during computations, notes can arbitrarily crash and return back to servers.</li>
<li>doesn't influence correctness and success of computations</li>
<li>no external impact necessary to reconfiguring the system at such events. &gt; if a hard drive was damaged, then a system administrator can physically change the hard drive.After reconnection, this node will be automatically picked up by a distributed system. And it will even be able to participate in current computations.</li>
</ul>
<p><img src="./images/week2_2.png" width="200"></p>
<ul>
<li><strong>Byzantine</strong>: A distributed system is robust Byzantine failures if it can correctly work despite some of the nodes behaving out of protocol. <img src="./images/week2_3.png" width="200"></li>
</ul>
<blockquote>
<p>If you are developing a financial system, then you are likely required to deal with these types of failures to protect your customers and your business.</p>
</blockquote>
<h2 id="links">Links</h2>
<p><strong>3 types of links:</strong></p>
<ul>
<li><p>perfect: all the sent messages must be delivered and received without any modification into the same portal. <img src="./images/week2_4.png" width="200"></p></li>
<li><p>fail-loss: some part of the messages can be lost but the probability of message loss does not depend on contents of a message.</p></li>
</ul>
<blockquote>
<p>the well-known TCP/IP protocol tries to solve this problem by re-transmitting messages if they were not received.</p>
</blockquote>
<p><img src="./images/week2_5.png" width="350"> - byzantine: some messages can be filtered according to some rule, some messages can be modified, and some messages could be created out of nowhere</p>
<h2 id="clocks">Clocks</h2>
<p><strong>clock synchronization problem: </strong></p>
<ul>
<li>clock skew: the time can be different on different machines</li>
<li>clock drift: there can be a different clock rate <img src="./images/week2_6.png" width="300"></li>
</ul>
<p><strong>Logical clocks</strong> help to track happened before events and therefore, order events to build reliable protocols.</p>
<h2 id="asynchronous-systems">[A]synchronous systems</h2>
<p>Systems can be divided into synchronous and asynchronous.</p>
<p><strong>synchronous</strong>: - every network packet should be delivered within a limited time bound. - Clock drift is limited in size - each CPU instruction is also limited in time.</p>
<h3 id="examples-of-different-distributed-systems">Examples of different distributed systems</h3>
<ol type="1">
<li>fail-stop, perfect link, and synchronous model</li>
</ol>
<blockquote>
<p>A parallel computational model and widely adopted by supercomputers where many processors connected by a local high speed computer bus.</p>
</blockquote>
<ol start="2" type="1">
<li><p>fail-recovery, fair-loss link, and asynchronous model (this course focus)</p></li>
<li><p>byzantine-failure, byzantine link, and asynchronous model</p></li>
</ol>
<blockquote>
<p>computational components spread across the globe of unreliable and untrusted network connections. The common representative of this model is grid computing.</p>
</blockquote>
<h1 id="mapreduce">MapReduce</h1>
<p>There are two phases during computation, map and reduce:</p>
<p><strong>map</strong>:apply the same function to each element of your collection. <img src="./images/week2_8.png" width="200"></p>
<p><strong>reduce</strong>:Reduce operator causes a sequence of elements by applying the following procedure iteratively. - As soon as you have more than one element in the sequence, then you get the first two and combine them to one element by applying the provided function. - Reduce function computes the value from left to right. &gt;Be careful about reducing functions that are not associative.</p>
<p><img src="./images/week2_9.png" width="200"></p>
<p><strong>MapReduce</strong>:the class of problems that you can solve with arbitrary map and reduce functions is quite big. <img src="./images/week2_10.png" width="200"></p>
<h1 id="distributed-shell">Distributed Shell</h1>
<ul>
<li>run a distributed <strong>grep</strong> as a MapReduce job. <img src="./images/week2_12.png" width="270"></li>
<li>Map will be equivalent to grep</li>
<li>Reduce will be None.</li>
</ul>
<p>In MapReduce applications, you don't always need map or reduce function.</p>
<ul>
<li><p>run a distributed <strong>head</strong> as a MapReduce job. <img src="./images/week2_13.png" width="270"> You can just retrieve the necessary data with HDFS client. To get these data with MapReduce job, get actual information such as, block index and size in lines on map phase to complete the task correctly --&gt; <strong>head and tweaks</strong>.</p></li>
<li>run a distributed <strong>wc</strong> as a MapReduce job. <img src="./images/week2_14.png" width="270"></li>
<li>The output from the map will be a tuple of the size 3: number of lines, words, and bytes.</li>
<li><p>Sum the items by components, so the reduced function will be an extended add operator.</p></li>
</ul>
<h2 id="wordcound-example">WordCound Example</h2>
<p><em>find the most popular words in the Wikipedia with MapReduce</em></p>
<ol type="1">
<li>count how many times each word appear in a data set &gt; wikipedia.dump | tr ' ' '' | sort | uniq -c</li>
</ol>
<p>Map--&gt;Shuffle&amp;Sort--&gt;Reduce - Map: the text is split in two words - tr - Shuffle&amp;Sort: words are distributed to a reduce phase in a way that reduce functions can be executed independently on different machines - sort - Reduce: uniq</p>
<p><img src="./images/mr2.png" width="370"> <strong>external sorting</strong>:If the data is sorted, and can be read as a stream, then uniq-c will be working correctly. To make data sorted, you only need to have enough disk space. The algorithm for this is called, external sorting.</p>
<p>All input and output of map and reduce functions should be a key value pair.</p>
<p><strong>MapReduce Formal Model</strong>:</p>
<p>map: (key, value) → (key, value)</p>
<p>reduce: (key, value) → (key, value)</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ cat -n wikipedia.dump | tr <span class="string">' '</span> <span class="string">'\n'</span>| sort | uniq -c</span><br></pre></td></tr></table></figure>
<ul>
<li>read data and get pairs with a line number, and line content.</li>
</ul>
<blockquote>
<p>cat -n wikipedia.dump: [(line_no, line), …]</p>
</blockquote>
<ul>
<li><p>on a map phase,you ignore line numbers and split lines into words. &gt; tr ' ' '': (-, line) —&gt; [ (word, 1), … ]</p></li>
<li><p>You can add value one to each output it worked. So, it means that you have seen this word once by reading a line from left to right.</p></li>
<li><p>a shuffle and sort phase where you spread the words by the hashes.So, you can process them on independent reducers. &gt; sort: Shuffle &amp; Sort</p></li>
<li><p>count how many figures of 1 you have for each word, and sum them up to get an answer. &gt; uniq -c: (word, [1, …]) —&gt; (word, count)</p></li>
</ul>
<p><img src="./images/mr4.png" width="370"></p>
<p><strong>3 types of key value pairs</strong>: - Key value pairs for the input data <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat -n wikipedia.dump: [(line_no, line), …]</span><br><span class="line"><span class="built_in">read</span>: [(k_in, v_in), …]</span><br></pre></td></tr></table></figure></p>
<ul>
<li><p>key value pairs for the intermediate data <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tr ‘ ‘ ‘\n’: (-, line) —&gt; [ (word, 1), … ]</span><br><span class="line">map: (k_in, v_in) —&gt; [(k_interm, v_interm), …]</span><br></pre></td></tr></table></figure></p></li>
<li><p>key value pairs for the output data <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Shuffle &amp; Sort: sort and group by k_interm</span><br><span class="line">uniq -c: (word, [1, …]) —&gt; (word, count)</span><br><span class="line">reduce: (k_interm, [(v_interm, …)] ) —&gt; [(k_out, v_out), …]</span><br></pre></td></tr></table></figure></p></li>
</ul>
<p><img src="./images/mr6.png" width="370"></p>
<h2 id="fault-tolerence">Fault Tolerence</h2>
<p>In a distributed file system, you store information with duplication in order to overcome node failures. MapReduce framework should also provide robustness against node failures during the job execution.</p>
<p>In a MapReduce job, you will have: 1. Master program: control the execution <img src="./images/ft1.png" width="370"></p>
<ol start="2" type="1">
<li><p>Master program will launch mappers to process input blocks or splits of data. <img src="./images/ft2.png" width="370"></p></li>
<li><p>To overcome the issues of correction execution mappers, there is no harm in re-executing mapper against the same data because you expect map function to be deterministic. As soon as you work on top of HDFS, you have a replica of this data on other nodes. So, you could assign another worker to a execute mapper against these data, and application master will do all this magic for you. <img src="./images/ft3.png" width="370"></p></li>
<li><p>If a worker running reducer dies, you can shuffle and sort data for this particular reducer to another worker. <img src="./images/ft4.png" width="370"></p></li>
<li><p>Shuffled and sorted data are stored on local disks instead of the distributed file system <img src="./images/ft5.png" width="370"></p></li>
</ol>
<blockquote>
<p><em>You only need to provide deterministic map and reduce function</em></p>
</blockquote>
<h2 id="hadoop-mapreduce-framework">Hadoop MapReduce framework</h2>
<p><strong>Job</strong> One MapReduce application is a job.</p>
<p><strong>Task</strong> a task can be either mapper or reducer.</p>
<h3 id="first-version">First version</h3>
<p><img src="./images/ft7.png" width="370"></p>
<ul>
<li>JobTracker: one global JobTracker to direct execution of MapReduce jobs.</li>
<li>located on one high-cost and high-performance node with HDFS namenode.</li>
<li>a single point failure</li>
<li>TaskTrackers</li>
<li>located once per every node where you store data, or where datanode daemon is working</li>
<li>spawns workers from mapper or reducer</li>
</ul>
<h3 id="yarn">YARN</h3>
<blockquote>
<p>Yet Another Resource Negotiation</p>
</blockquote>
<p><img src="./images/ft8.png" width="370"></p>
<ul>
<li>TaskTracker is subtituted by NodeManagers who can provide a layer of CPU and RAM containers.</li>
<li>ResourceManager overseas NodeManagers, and client request resources for execution</li>
<li>MapReduce applications can work on top of this resource layer.</li>
<li>There is no concept such as a global JobTracker because application master can start on any node.</li>
<li>All of these enable Hadoop to share resources dynamically between MapReduce and other parallel processing frameworks.</li>
</ul>
<p><img src="./images/mrf.png" width="370"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://nancyyanyu.github.io/undefined/2a71b2a0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Nancy Yan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Nancy's Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/undefined/2a71b2a0/" class="post-title-link" itemprop="url">Hadoop MapReduce Streaming Application in Python</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-06-27 18:13:45 / Modified: 18:19:06" itemprop="dateCreated datePublished" datetime="2019-06-27T18:13:45-05:00">2019-06-27</time>
            

            
              

              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Big-Data/" itemprop="url" rel="index"><span itemprop="name">Big Data</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">10k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">9 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          <blockquote>
<p>This is course note of <a href="https://www.coursera.org/learn/big-data-essentials" target="_blank" rel="noopener">Big Data Essentials: HDFS, MapReduce and Spark RDD</a></p>
</blockquote>
<h1 id="streaming">Streaming</h1>
<p>In a Hadoop MapReduce application:</p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/undefined/2a71b2a0/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://nancyyanyu.github.io/undefined/2a71b2a0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Nancy Yan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Nancy's Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/undefined/2a71b2a0/" class="post-title-link" itemprop="url">Hadoop MapReduce Application Tuning</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-06-27 18:12:33 / Modified: 18:18:49" itemprop="dateCreated datePublished" datetime="2019-06-27T18:12:33-05:00">2019-06-27</time>
            

            
              

              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Big-Data/" itemprop="url" rel="index"><span itemprop="name">Big Data</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">7.9k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">7 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          <blockquote>
<p>This is course note of <a href="https://www.coursera.org/learn/big-data-essentials" target="_blank" rel="noopener">Big Data Essentials: HDFS, MapReduce and Spark RDD</a></p>
</blockquote>
<p><strong>The world of the efficient MapReduce is based on three whales. Combiner, Partitioner, and Comparator.</strong></p>
<h1 id="combiner">Combiner</h1>
<p>To change the usage of these IO operations and network bandwidth, you can use <strong>combiner</strong> to squash several items into one.</p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/undefined/2a71b2a0/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://nancyyanyu.github.io/undefined/2a71b2a0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Nancy Yan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Nancy's Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/undefined/2a71b2a0/" class="post-title-link" itemprop="url">Apache Spark: Advanced Topics</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-06-27 18:09:33 / Modified: 18:18:33" itemprop="dateCreated datePublished" datetime="2019-06-27T18:09:33-05:00">2019-06-27</time>
            

            
              

              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Big-Data/" itemprop="url" rel="index"><span itemprop="name">Big Data</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">8.6k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">8 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          
            <blockquote>
<p>This is course note of <a href="https://www.coursera.org/learn/big-data-essentials" target="_blank" rel="noopener">Big Data Essentials: HDFS, MapReduce and Spark RDD</a></p>
</blockquote>
<h1 id="execution-scheduling">Execution &amp; Scheduling</h1>
<p><strong>SparkContext</strong> - When creating a Spark application, the first thing you do is create a SparkContext object, which tells Sparks how to access a cluster. - The context, living in your driver program, coordinates sets of processes on the cluster to run your application.</p>
<p><img src="./images/exe1.png" width="400"></p>
<ul>
<li>The SparkContext object communicates the Cluster Manager to allocate executors.</li>
<li>The Cluster Manager is an external service for acquiring resources on a cluster. For example, YARN, Mesos or a standalone Spark cluster.</li>
<li>once the context has allocated the executors, it communicates directly with them and schedules tasks to be done.</li>
</ul>
<h2 id="jobs-stages-tasks">Jobs, stages, tasks</h2>
<ul>
<li><strong>Task</strong> is a unit of work to be done</li>
<li><strong>Tasks</strong> are created by a <strong>job scheduler</strong> during the scheduling of a job for every job stage. And every task belongs to the job stage.</li>
<li><strong>Job</strong> is spawned in response to a Spark action</li>
<li><strong>Job</strong> is divided in smaller sets of tasks called <strong>stages</strong></li>
</ul>
<h3 id="example">Example</h3>
<p>Z = X .map(lambda x: (x % 10, x / 10)) .reduceByKey(lambda x, y: x + y) .collect()</p>
<ol type="1">
<li><p>Whenever you invoke an action, the job gets spawned in the driver program. <img src="./images/exe2.png" width="400"></p></li>
<li><p>Then the driver runs a job scheduler to divide the job into smaller stages. <img src="./images/exe3.png" width="400"></p></li>
<li>Then tasks are created for every job stage.</li>
<li><p>tasks are delegated to the executors, which perform the actual work. <img src="./images/exe4.png" width="400"></p></li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bash</span><br><span class="line">All this machinery exists within the SparkContext object. It keeps track of the executors, it spawns jobs, and it runs the scheduler.</span><br></pre></td></tr></table></figure>
<p><strong>Difference between job stage and task:</strong> - <strong>Job stage</strong> is a pipelined computation spanning between materialization boundaries - job stages are defined on RDD level, thus not immediately executable - <strong>Task</strong> is a job stage bound to particular partitions - bound to a particular partitions, thus immediately executable - <strong>Materialization</strong> happens when reading, shuffling or passing data to an action - narrow dependencies allow pipelining - wide dependencies forbid it</p>
<p><strong>SparkContext – other functions</strong>: - Tracks liveness of the executors by sending heartbeat messages periodically. - required to provide fault-tolerance - Schedules multiple concurrent jobs - to control the resource allocation within the application - Performs dynamic resource allocation if the cluster manager permits. - increases cluster utilization in shared environments by proper scheduling of multiple applications according to their resource demands</p>
<h2 id="summary">Summary</h2>
<ol type="1">
<li>The SparkContext is the core of your application</li>
</ol>
<ul>
<li>allows your application to connect to a cluster and allocate resources and executors.</li>
<li>whenever you invoke an action, the SparkContext spawns a job and runs the job scheduler to divide it into stages--&gt;<strong>pipelineable</strong></li>
<li>tasks are created for every job stage and scheduled to the executors.</li>
</ul>
<ol start="2" type="1">
<li>The driver communicates directly with the executors</li>
<li><p>Execution goes as follows: <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Action -&gt; Job -&gt; Job Stages -&gt; Tasks</span><br></pre></td></tr></table></figure></p></li>
<li><p>Transformations with narrow dependencies allow pipelining</p></li>
</ol>
<h1 id="caching-persistence">Caching &amp; Persistence</h1>
<ul>
<li>RDDs are partitioned</li>
<li>Execution is build around the partitions</li>
<li>Each task processes a small number of partitions at a time, and the shuffle globally redistributes data items between the partitions, when required.</li>
<li>Spark transfers data over the network and the IO unit here is not a partition but a block.</li>
<li>Block is a unit of input and output in Spark</li>
</ul>
<h2 id="example-1">Example</h2>
<blockquote>
<p>Motivating example: load a wikipedia dump from HDFS and see how many articles there contain the words Spark</p>
</blockquote>
<p>You need to create the RDD, apply the filter transformation, and invoke the count action. <img src="./images/exe5.png" width="400"></p>
<blockquote>
<p>Motivating example: among those articles with the Spark word, you would like to see how many of them contain the word, Hadoop and how many the word MapReduce.</p>
</blockquote>
<p><img src="./images/exe6.png" width="400"></p>
<p><strong><em>Perform worse!</em></strong></p>
<ul>
<li><p>Reason: after completing the computation, Spark disposes intermediate data and those intermediate RDDs. That means, it will reload the Wikipedia dump two more times incurring extra input and output operations.</p></li>
<li><p>A better strategy:</p></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cache the preloaded dump in the memory and reuse it until you end your session.</span><br></pre></td></tr></table></figure>
<p>Spark allows you to hint which RDDs are better to be kept in memory or even on the disk. Spark does so by <strong><em>caching the blocks comprising your dataset. </em></strong></p>
<h2 id="controlling-persistence-level">Controlling persistence level</h2>
<figure class="highlight plain"><figcaption><span>mark the data set as cached by invoking a cache method on it</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"> - The cache method is just a shortcut for the memory-only persistence. </span><br><span class="line"></span><br><span class="line">```persist```: allows you to set RDDs storage to persist across operations after the first time it is computed.</span><br><span class="line"> - parameterized by a storage level</span><br><span class="line">&lt;img src=&quot;./images/exe7.png&quot; width=400&gt;</span><br><span class="line"></span><br><span class="line">## Best practices</span><br><span class="line"></span><br><span class="line">When running an **interactive shell**, cache your dataset after you&apos;ve done all the necessary preprocessing.</span><br><span class="line">- by keeping your work inside in the memory, you would get a more responsive experience. </span><br><span class="line"></span><br><span class="line">When running a **batch computation**, cache dictionaries that you join with your data. </span><br><span class="line">- Join dictionaries are often reshuffled, so it would be helpful to speed up their read times. </span><br><span class="line"></span><br><span class="line">When running an **iterative computation**, cache static data like dictionaries or input datasets</span><br><span class="line">- avoid reloading the data from the ground up on every iteration.</span><br><span class="line">- The static data tends to get evicted due to the memory pressure from the intermediate data.</span><br><span class="line"></span><br><span class="line">## Summary</span><br><span class="line">- Performance may be improved by persisting data across operations</span><br><span class="line"> - in interactive sessions, iterative computations and hot datasets</span><br><span class="line">- You can control the persistence of a dataset</span><br><span class="line"> - whether to store in the memory or on the disk</span><br><span class="line"> - how many replicas to create</span><br><span class="line"></span><br><span class="line"># Broadcast Variable </span><br><span class="line">**shared memory** is a powerful abstraction, but often misused.</span><br><span class="line">- It can make the developer&apos;s life easier</span><br><span class="line">- It can make the application performance deteriorate because of extra synchronization.</span><br><span class="line"></span><br><span class="line">```bash</span><br><span class="line">This is why in spark there are restricted forms of the shared memory.</span><br></pre></td></tr></table></figure>
<p><strong>Broadcast variable</strong> is a read-only variable that is efficiently shared among tasks</p>
<p><strong>one to many communication:</strong> When it captures a variable into the closure, it is sent to an executor together with a task specification.</p>
<p><strong>many to many communication protocol</strong>: torrent</p>
<ul>
<li>Distribution is done by a torrent-like protocol (extremely fast!)</li>
<li>Distributed efficiently compared to captured variables</li>
</ul>
<h2 id="example-2">Example</h2>
<blockquote>
<p>Motivating example: resolve IP addresses to countries from 1 terabyte access log for your website</p>
</blockquote>
<p>Idea: map-side join--distribute the database to every mapper and query it locally.</p>
<p>Distributing the database via a broadcast variable, we take slightly more than 1 gigabyte of outgoing traffic at the driver node <img src="./images/br1.png" width="400"></p>
<blockquote>
<p>Motivating example:</p>
</blockquote>
<ol type="1">
<li>setup a transformation graph to compute a dictionary</li>
<li>invoke the <figure class="highlight plain"><figcaption><span>action to load it into the driver's memory</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">3.  put it into the broadcast variable to use in further computations.</span><br><span class="line"></span><br><span class="line">Idea:  upload computations to spark executors and use the driver program as the coordinator.</span><br><span class="line">&lt;img src=&quot;./images/br2.png&quot; width=400&gt;</span><br><span class="line"></span><br><span class="line">## Summary</span><br><span class="line"></span><br><span class="line">- Broadcast variables are read-only shared variables with effective sharing mechanism</span><br><span class="line">- Useful to share dictionaries, models</span><br><span class="line"></span><br><span class="line"># Accumulator Variable </span><br><span class="line"></span><br><span class="line">**Accumulator variable** is a read-write variable that is shared among tasks</span><br><span class="line">- Writes are restricted to increments!</span><br><span class="line"> - i. e.: var += delta</span><br><span class="line"> - addition may be replaced by any associate, commutative operation</span><br><span class="line"></span><br><span class="line">```bash</span><br><span class="line">Restricting the right operations allows the framework to avoid complex synchronization thus making the accumulators efficient.</span><br></pre></td></tr></table></figure></li>
</ol>
<ul>
<li>Accumulator variable could be read only by the <strong><em>driver</em></strong> program and not by the executors.</li>
<li>cannot read the accumulated value from within a task</li>
</ul>
<h2 id="example-3">Example</h2>
<p><img src="./images/acc1.png" width="400"> <img src="./images/acc2.png" width="200"> <img src="./images/acc3.png" width="200"> <img src="./images/acc4.png" width="200"></p>
<h2 id="guarantees-on-the-updates">Guarantees on the updates</h2>
<ul>
<li>Updates generated in actions: guaranteed to be applied only once to the accumulator.</li>
<li>This is because successful actions are never re-executed and Spark can conditionally apply the update.</li>
<li>Updates generated in transformations: no guarantees when they accumulate updates. - - Transformations can be recomputed on a failure, on the memory pressure, or in another unspecified codes like a preemption.</li>
<li>Spark provides no guarantees on how many times transformation code maybe re-executed.</li>
</ul>
<h2 id="use-cases">Use cases</h2>
<ol type="1">
<li>Performance counters</li>
</ol>
<ul>
<li>number of processed records, total elapsed time, total error and so on and so forth</li>
</ul>
<ol start="2" type="1">
<li>Simple control flow</li>
</ol>
<ul>
<li>conditionals: stop on reaching a threshold for corrupted records</li>
<li>loops: decide whether to run the next iteration of an algorithm or not</li>
</ul>
<ol start="3" type="1">
<li>Monitoring</li>
</ol>
<ul>
<li>export values to the monitoring system</li>
</ul>
<ol start="4" type="1">
<li>Profiling &amp; debugging</li>
</ol>
<h2 id="summary-1">Summary</h2>
<ul>
<li>Accumulators are shared read-write variables with de-coupled read and write sides</li>
<li>could be updated from actions and transformations by using an increment.</li>
<li>can use custom associative, commutative operation for the updates</li>
<li>can read the total value only in the driver</li>
<li>Useful for the control flow, monitoring, profiling &amp; debugging</li>
</ul>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://nancyyanyu.github.io/undefined/2a71b2a0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Nancy Yan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Nancy's Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/undefined/2a71b2a0/" class="post-title-link" itemprop="url">Apache Spark: Basic Concepts</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-06-27 18:08:12 / Modified: 18:18:37" itemprop="dateCreated datePublished" datetime="2019-06-27T18:08:12-05:00">2019-06-27</time>
            

            
              

              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Big-Data/" itemprop="url" rel="index"><span itemprop="name">Big Data</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">14k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">13 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          
            <blockquote>
<p>This is course note of <a href="https://www.coursera.org/learn/big-data-essentials" target="_blank" rel="noopener">Big Data Essentials: HDFS, MapReduce and Spark RDD</a></p>
</blockquote>
<h1 id="apache-spark">Apache Spark</h1>
<p><strong>Apache Spark</strong>:a modern distributed fault tolerant computation platform.</p>
<p><strong>History of Apache Spark</strong>:</p>
<p><img src="./images/week4_1.png" width="350"> <img src="./images/week4_2.png" width="250"> <img src="./images/week4_3.png" width="350"> <img src="./images/week4_4.png" width="350"></p>
<h1 id="rdds">RDDs</h1>
<p><strong>RDD(Resilient Distributed Dataset)</strong>: a core abstraction enabling both an <em>efficient execution for a computation</em>, and a <em>flexible and convenient formalism to define computations</em>.</p>
<ul>
<li><strong>Resilient</strong> — able to withstand failures</li>
<li><strong>Distributed</strong> — spanning across multiple machines</li>
<li>Formally, RDD is a read-only, partitioned collection of records</li>
<li>To say that the dataset is an RDD, the dataset must adhere to the RDD interface.</li>
</ul>
<p><strong>The dataset must be:</strong> - able to enumerate its partitions by implementing the partition's function.</p>
<p><figure class="highlight plain"><figcaption><span>-> Array[Partition]```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> </span><br><span class="line">- The partition is an opaque object for the framework. It is passed back to the iterator function of the RDD, when the framework needs to read the data from the partition.</span><br><span class="line"></span><br><span class="line">```iterator(p: Partition, parents: Array[Iterator]) -&gt; Iterator</span><br></pre></td></tr></table></figure></p>
<ul>
<li>able to enumerate its dependencies and provide an array of dependency objects.</li>
</ul>
<p><figure class="highlight plain"><figcaption><span>-> Array[Dependency]```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br></pre></td><td class="code"><pre><span class="line"> - The dependency object maps partitions of the dataset to the dependencies that are partitions of the parent dataset. </span><br><span class="line"> - Those parent partitions are injected into the iterator call when creating a reader.</span><br><span class="line"></span><br><span class="line">- **Typed.** every item in RDD has the same, known type.</span><br><span class="line"> - typedness is an important property to catch bugs early on before the actual execution</span><br><span class="line"> - e.g. RDD[strings], or an RDD[integers].</span><br><span class="line">&lt;img src=&quot;./images/rdd1.png&quot; width=350&gt;</span><br><span class="line"></span><br><span class="line">## Why do we need a new abstraction?</span><br><span class="line"></span><br><span class="line">**Example**: iterative computations (K-means, PageRank, …)</span><br><span class="line">- relation between consequent steps is known only to the user code not the framework</span><br><span class="line"> - framework has no capabilities to optimize the whole computation</span><br><span class="line">- framework must reliably persist data between steps thus generating excessive I/O (even if it is temporary data)</span><br><span class="line"></span><br><span class="line">&gt;In this scenario,Spark is trying to keep the data in the memory, effectively eliminates an intermediate disk persistence, and thus improving the completion time. </span><br><span class="line"></span><br><span class="line">**Example**: joins</span><br><span class="line">- join operation is used in many MapReduce applications</span><br><span class="line">- not-so-easy to reuse code</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## Example: a binary file in HDFS</span><br><span class="line">&gt; Implement the necessary functions to make the binary file in RDD</span><br><span class="line"></span><br><span class="line">&lt;img src=&quot;./images/rdd2.png&quot; width=350&gt;</span><br><span class="line">- To implement the **partition**&apos;s function: lookup the blocks for NameNode, create a partition for every block, and return the partitions.</span><br><span class="line">- To implement the **iterator**&apos;s function: extract the block information from the partition, and use it to create a reader from HDFS.</span><br><span class="line">- To implement the **dependencies**&apos; function: File reading does not depend on any other RDD, nor on any other partition. So implementing the dependencies function is trivial. It returns an empty array</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## Example: a data* file in HDFS </span><br><span class="line">&gt; reading records from the file rather than a raw bytes</span><br><span class="line"></span><br><span class="line">&lt;img src=&quot;./images/rdd3.png&quot; width=350&gt;</span><br><span class="line"></span><br><span class="line">##  Example: an in-memory array</span><br><span class="line">The simplest way to make the array in RDD is to pretend that there is a single partition with the whole array. In this case, the partition object keeps a ***reference*** to the array, and the iterator function uses this reference to create an iterator.</span><br><span class="line"></span><br><span class="line">&lt;img src=&quot;./images/rdd4.png&quot; width=350&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## Example: a sliced* in-memory array</span><br><span class="line"></span><br><span class="line">&gt; slice the array into chunks to gain parallelism</span><br><span class="line"></span><br><span class="line">The partition corresponds to the chunk of the source array. </span><br><span class="line"></span><br><span class="line">*partitions are handled in parallel.*</span><br><span class="line"></span><br><span class="line">&lt;img src=&quot;./images/rdd5.png&quot; width=350&gt;</span><br><span class="line"></span><br><span class="line">## Summary</span><br><span class="line"></span><br><span class="line">- **RDD is a read-only, partitioned collection of records**</span><br><span class="line"> - The collection of the dataset must provide information about its partitions, and provide means to create an iterator over the partition. </span><br><span class="line"> - a developer can access the partitions and create iterators over them</span><br><span class="line"> - RDD tracks dependencies (to be explained in the next video)</span><br><span class="line">- Examples of RDDs</span><br><span class="line"> - Hadoop files with the proper file format</span><br><span class="line"> - In-memory arrays</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Transformation</span><br><span class="line"></span><br><span class="line">**Two ways to construct RDDs:**</span><br><span class="line">- Data in a stable storage</span><br><span class="line"> - Example: files in HDFS, objects in Amazon S3 bucket, lines in a text file, …</span><br><span class="line"> - RDD for data in a stable storage has no dependencies</span><br><span class="line">- From existing RDDs by applying a transformation </span><br><span class="line"> - Example: filtered file, grouped records, …</span><br><span class="line"> - RDD for a transformed data depends on the source data</span><br><span class="line"></span><br><span class="line">**Transformation**:</span><br><span class="line">- Allow you to create new RDDs from the existing RDDs by specifying how</span><br><span class="line">to obtain new items from the existing items</span><br><span class="line">- The transformed RDD depends implicitly on the source RDD</span><br><span class="line"></span><br><span class="line">Note: Datasets are immutable in Spark, and you cannot modify data in-place.</span><br><span class="line"></span><br><span class="line">## Example: map, flatMap</span><br><span class="line">**map**:</span><br><span class="line">- Def: map(f: T -&gt; U): RDD[T] -&gt; RDD[U]</span><br><span class="line">- returns a mapped RDD with items f(x) for every x in the source RDD</span><br><span class="line">&lt;img src=&quot;./images/trans2.png&quot; width=350&gt;</span><br><span class="line"></span><br><span class="line">**flatMap**:</span><br><span class="line">- Def: flatMap(f: T -&gt; Array[U]): RDD[T] -&gt; RDD[U]</span><br><span class="line">- same as map but flattens the result of *f*</span><br><span class="line">- generalizes map and filter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## Example: filter</span><br><span class="line"></span><br><span class="line">**filter**:</span><br><span class="line">- Def: filter(p: T -&gt; Boolean): RDD[T] -&gt; RDD[T]</span><br><span class="line">- returns a filtered RDD with items satisfying the predicate *p*</span><br><span class="line">&lt;img src=&quot;./images/trans1.png&quot; width=350&gt;</span><br><span class="line"></span><br><span class="line">**Filtered RDD**:</span><br><span class="line">- **partitions**: transformed RDD&apos;s mostly the same as source one--&gt;*reuse partitions of the source RDD* as there is no need to change the partitioning.</span><br><span class="line">- **dependencies**: every field of partition depends on the source partition. You can establish this relation by providing a dependency object that establishes one-to-one correspondence between the filtered and the source partitions.</span><br><span class="line">- **iterator**: when creating an iterator over the filter partition:</span><br><span class="line"> - Spark would inject an iterator of the source partition into the iterator function called</span><br><span class="line"> - reusing the parent iterator.</span><br><span class="line"> - When requested for the next value, you can pull values from the parent iterator until it returns you an item that satisfies the predicate. </span><br><span class="line">&lt;img src=&quot;./images/trans3.png&quot; width=350&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### Lazy iterator</span><br><span class="line"></span><br><span class="line">Actual filtering happens not at the creation time of Y, but at the access time to the iterator over a partition of Y.The filtering starts to happen only when you start to pull items from the iterator. </span><br><span class="line"></span><br><span class="line">Same holds for other transformations – they are lazy,i.e. they compute the result only when accessed.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### On closures</span><br><span class="line">&lt;img src=&quot;./images/trans4.png&quot; width=350&gt;</span><br><span class="line"></span><br><span class="line">### (Partition)Dependency graph</span><br><span class="line">Whenever you apply a transformation to the RDD, you implicitly construct a dependency graph on the RDDs</span><br><span class="line">- This graph is used by the framework to schedule jobs </span><br><span class="line"></span><br><span class="line">&lt;img src=&quot;./images/trans5.png&quot; width=350&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## Keyed Transformation</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;img src=&quot;./images/trans6.png&quot; width=400&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### Grouped RDD</span><br><span class="line">- partition the key space</span><br><span class="line"> - using the hash partition then to compute the values in the resulting partition</span><br><span class="line"></span><br><span class="line">- **Shuffle**: redistribute all the values between all the partitions. </span><br><span class="line"> - scan over the entire source RDD to select only the pairs that belong to the output partition.</span><br><span class="line"></span><br><span class="line">&gt; Differnt from flatMap like transformation, a single output partition depends on all the input partitions.</span><br><span class="line"></span><br><span class="line">&lt;img src=&quot;./images/trans9.png&quot; width=400&gt;</span><br><span class="line"></span><br><span class="line">#### Narrow &amp; Wide dependencies</span><br><span class="line">&lt;img src=&quot;./images/trans10.png&quot; width=400&gt;</span><br><span class="line"></span><br><span class="line">## Cogroup Transformation</span><br><span class="line"></span><br><span class="line">&lt;img src=&quot;./images/trans7.png&quot; width=400&gt;</span><br><span class="line"></span><br><span class="line">The cogroup transformation allows you to compute any kind of a join between two data sets.</span><br><span class="line"></span><br><span class="line">### Inner join</span><br><span class="line">That is all triples (k, x, y) where (k, x) is in X and (k, y) is in Y</span><br><span class="line"></span><br><span class="line">--&gt; apply the ```flatMap``` transformation on top of the result of the cogroup transformation. </span><br><span class="line"></span><br><span class="line">## Joins Transformation</span><br><span class="line"></span><br><span class="line">The join transformation produces the inner join of two data sets.</span><br><span class="line"></span><br><span class="line">**inner join:** If the key is present only in the one side of the join that is in one data set. Then it is omitted from the result</span><br><span class="line"></span><br><span class="line">**outer join:** one-sided keys are added to the result with appropriate null values.</span><br><span class="line"></span><br><span class="line">&lt;img src=&quot;./images/trans8.png&quot; width=400&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##  MapReduce in Spark</span><br><span class="line"></span><br><span class="line">You can express any MapReduce computation in Spark as the ```flatMap``` followed by the ```groupByKey```, followed by one more ```flatMap```. </span><br><span class="line"></span><br><span class="line">&lt;img src=&quot;./images/trans11.png&quot; width=400&gt;</span><br><span class="line"></span><br><span class="line">## Summary</span><br><span class="line">Transformation</span><br><span class="line">- is a description of how to obtain a new RDD from existing RDDs</span><br><span class="line">- is the primary way to “modify” data (given that RDDs are immutable)</span><br><span class="line"></span><br><span class="line">Properties:</span><br><span class="line">- Transformations are lazy, i.e. no work is done until data is explicitly requested</span><br><span class="line">- There are transformations with narrow and wide dependencies</span><br><span class="line">- MapReduce can be expressed with a couple of transformations</span><br><span class="line">- Complex transformations (like joins, cogroup) are available </span><br><span class="line"></span><br><span class="line"># Actions </span><br><span class="line"></span><br><span class="line">**Driver &amp; executors**:</span><br><span class="line">- When you write and invoke your Spark application, driver ***runs within the driver program.***</span><br><span class="line">- Driver program drives the execution of your Spark application</span><br><span class="line">- Driver delegates tasks to executors to use cluster resources</span><br><span class="line"> - When something must be done with the data, the driver schedules tasks to be executed by executors.</span><br><span class="line">- In *local* mode, executors are collocated with the driver</span><br><span class="line">- In *cluster* mode, executors are located on cluster machines--&gt;  allowing you to use the cluster for a computation</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">**Actions**:</span><br><span class="line">- Triggers data to be materialized and processed on the executors and</span><br><span class="line">then passes the outcome to the driver</span><br><span class="line">&gt; Actions, together with a transformation code, are executed elsewhere, not in your driver program. Your driver program receives only the outcome.</span><br><span class="line"></span><br><span class="line">```bash</span><br><span class="line">Spark does a great job of abstracting away the execution details, and that improves your developer productivity and code readability.</span><br></pre></td></tr></table></figure></p>
<ul>
<li><strong>Example</strong>: actions are used to collect, print and save data</li>
</ul>
<h2 id="frequently-used-actions">Frequently used actions</h2>
<ul>
<li><figure class="highlight plain"><figcaption><span>action collects the result into the memory of the driver program.</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"> - intended to be used when the output is small enough to fit into the driver&apos;s memory.</span><br><span class="line">- ```take``` action takes the given number of items from a data set and passes them back to the driver.</span><br><span class="line"> - tries to use only the first partition of a data set to optimize the completion time.</span><br><span class="line">&lt;img src=&quot;./images/action1.png&quot; width=400&gt;</span><br><span class="line">When the result is large enough, you may want to save it to HDFS for example. Doing so by collecting items in the driver will quickly run out of memory.</span><br><span class="line"></span><br><span class="line">There are special family of safe actions that do heavy work on the executor side and return a confirmation to the driver.</span><br><span class="line"></span><br><span class="line">- ```SaveAsText``` file is used for full debugging or for simple applications</span><br><span class="line">- ```SaveAsHadoopFile``` leverages Hadoop file formats to serialize data--&gt;common way to save data to HDFS.</span><br><span class="line">&lt;img src=&quot;./images/action2.png&quot; width=400&gt;</span><br><span class="line"></span><br><span class="line">If you need to run your own code over a data set, there are ```foreach``` and ```foreachPartition``` actions that invoke your function on the executor side.</span><br><span class="line"></span><br><span class="line">You can use this function to persist your data in your custom database for example, or to send data over the wire to an external service, or anything else.</span><br><span class="line">&lt;img src=&quot;./images/action3.png&quot; width=400&gt;</span><br><span class="line"></span><br><span class="line">## Summary</span><br><span class="line">- Actions trigger computation and processing of the dataset</span><br><span class="line">- Actions are executed on executors and they pass results back to the driver</span><br><span class="line">- Actions are used to collect, save, print and fold data </span><br><span class="line"></span><br><span class="line"># Resiliency </span><br><span class="line"></span><br><span class="line">How it is possible to continue operation despite machine failures in the cluster?</span><br><span class="line"></span><br><span class="line">## Fault-tolerance in MapReduce</span><br><span class="line">Two key aspects:</span><br><span class="line">- reliable storage for input and output data</span><br><span class="line"> -  once data is stored in HDFS it is safe</span><br><span class="line">- deterministic and side-effect free execution of mappers and reducers</span><br><span class="line"> -  if the computation is deterministic, and has no side effects, the framework can restart it multiple times, and get the same result every time</span><br><span class="line">&lt;img src=&quot;./images/resi1.png&quot; width=400&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## Fault-tolerance in Spark</span><br><span class="line"></span><br><span class="line">Same two key aspects:</span><br><span class="line">- reliable storage for input and output data</span><br><span class="line">- deterministic and side-effect free execution of</span><br><span class="line">transformations(including closures)</span><br><span class="line"> - Spark assumes that every transformation is deterministic, and free of side-effects, to be able to restart any failed computation</span><br><span class="line"></span><br><span class="line">**Determinism** — every invocation of the function results in the same returned value</span><br><span class="line">- e. g. do not use random numbers, do not depend on a hash value order</span><br><span class="line"></span><br><span class="line">**Freedom of side-effects** — an invocation of the function does not change anything in the external world</span><br><span class="line">- e. g. do not commit to a database, do not rely on global variables</span><br><span class="line"> -  If your function happens to commit to the database, there is no way to roll back changes in case of failure.  </span><br><span class="line"> </span><br><span class="line">​```bash</span><br><span class="line">Spark is allowed to restart the failed parts of the computation. To decide what to restart, Spark keeps track of the lineage.</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="fault-tolerance-transformations">Fault-tolerance &amp; transformations</h2>
<p><strong>Lineage</strong> — a dependency graph for all partitions of all RDDs involved in a computation up to the data source</p>
<p>Machine failure renders some partitions in the lineage unavailable. To cope with the failure, you must detect <em>which partitions become unavailable</em>, and decide <em>what to restart</em>. - Detection is done in the Driver Program, because the Driver Program orchestrates the execution, and already tracks the partitions. - <strong>Deciding what to restart</strong>: Given the failed partition, you look at its dependencies, and if they are alive, restart the computation. If the dependencies are failed as well, you recursively try to recover them. - <strong>Restarts</strong> are slightly more fragile in the case of wide dependencies, because to recompute an output partition, all dependencies must be alive, and there are many of them. If dependencies you evicted out of a cache for example, the restart will be expensive.</p>
<p><img src="./images/resi2.png" width="400"> <img src="./images/resi3.png" width="400"></p>
<h2 id="fault-tolerance-actions">Fault-tolerance &amp; actions</h2>
<p>Actions are side-effects in Spark (communicate with external services) - Actions have to be <strong>idempotent</strong>幂等（自己重複運算的結果等於它自己的元素） that is safe to be re-executed multiple times given the same input</p>
<p>Example: <strong>collect()</strong> - all transformations are deterministic, the final data set isn't changed in case of restarts. Therefore, even if the collect action fails, it could be safely re-executed</p>
<p>Example: <strong>saveAsTextFile()</strong> - since the data set is the same, you can safely override the output file</p>
<h2 id="summary">Summary</h2>
<p>Resiliency is implemented by - tracking lineage - assuming deterministic &amp; side-effect free execution of transformations(including closures) - all the closures pass to Spark, must be deterministic and side effect free - assuming idempotency for actions</p>
<p>These properties cannot be checked or enforced at the compile time, and may lead to obscure bugs in your application that are hard to debug and hard to reproduce.</p>
<p>​<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://nancyyanyu.github.io/undefined/2a71b2a0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Nancy Yan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Nancy's Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/undefined/2a71b2a0/" class="post-title-link" itemprop="url">DFS,HDFS,Architecture,Scaling problem</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-06-27 18:04:59 / Modified: 18:18:46" itemprop="dateCreated datePublished" datetime="2019-06-27T18:04:59-05:00">2019-06-27</time>
            

            
              

              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Big-Data/" itemprop="url" rel="index"><span itemprop="name">Big Data</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">17k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">15 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          
            <h1 id="section"></h1>
<h1 id="scaling-dfs">Scaling DFS</h1>
<h2 id="big-data-storage">Big data storage:</h2>
<ul>
<li>Scale up (vertical scaling): get yourself a bigger hard drive</li>
<li>Lower latency</li>
<li>Scale out (horizontal scaling)</li>
<li>Higher latency</li>
<li>Problem: one node get out of service 3 years averagely</li>
</ul>
<p>-&gt; Distributed file system <img src="./images/week_1.png" width="400"></p>
<h2 id="google-file-system">Google File System:</h2>
<p><strong>Keys:</strong> - components failures are a norm (→ replication) - even space utilisation: all files splited into blocks of fixed size, about 100mb - write-once-read-many: it's not allowed to modify in the middle, as it drastically simplifies API and internal implementation of a distributed file system.</p>
<p><strong>Replication:</strong> <img src="./images/week_2.png" width="400"> &gt; S.txt and B.txt. They are both split into equal sized blocks, and then distributed over a different machine, with replications. Storage machines are called <em>channel servers</em>, or <em>data nodes</em>.</p>
<p><strong>Metadata:</strong> include administrative information about creation time, access properties...</p>
<p><strong>Master node:</strong> stores all metadata in memory; enable to request metadata with minimal latency</p>
<h2 id="hadoop-distributed-file-system">Hadoop Distributed File System</h2>
<blockquote>
<p>an open source implementation of Google File System</p>
</blockquote>
<p><strong>Server roles</strong>: 1. Namenode: master node 2. Datanode</p>
<p><img src="./images/week_3.png" width="500"></p>
<p>HDFS client provides command line interface to communicate with that distributed file system -&gt;no need to write any code to access data.</p>
<h3 id="how-to-read-files-from-hdfs">How to read files from HDFS?</h3>
<p><img src="./images/week_4.png" width="400"></p>
<ol type="1">
<li><p>Request name node to get information about file blocks' locations. &gt;These blocks are distributed over different machines, but all of this complexity is hidden behind HDFS API.</p></li>
<li><p>User only sees a continuous stream of data. &gt;If at some point a datanode you retrieve data from died you get this data from another replica without bothering users about it.</p></li>
<li><p>You will get data from the closest machine.</p></li>
</ol>
<h4 id="closeness">Closeness</h4>
<blockquote>
<p>data center topology; it depends on the physical distance and unpredictable system load such as metric overutilization</p>
</blockquote>
<p><img src="./images/week_5.png" width="400"></p>
<ul>
<li>d=0: request data from HDFS, and this data is available on the same machine, then you can use data locality to read data directly from hard drive without any extra RPC codes.</li>
<li>d=2: a datanode is located in the same rack</li>
<li>d=4: read data from another rack</li>
<li>d=6: the data is allocated in another data center</li>
</ul>
<h3 id="how-to-write-files-into-hdfs">How to write files into HDFS?</h3>
<h4 id="redundancy-model">Redundancy model:</h4>
<blockquote>
<p>When you write a block of data into HDFS, Hadoop distributes replicas over the storage.</p>
</blockquote>
<p><strong>first replica</strong>: located on the same node if write data from a DataNode machin; otw, the first DataNode to put replica is chosen by random.</p>
<p><strong>second replica</strong>: placed in a different rack. If this racks goes down (power supply problems), you will access data from another rack.</p>
<p><strong>third replica</strong>: located on a different machine in the same rack as the second replica. You don't pay for extra between rack network utilization as the third replica is copied from the second data node.</p>
<p><strong>further replicas</strong>: applies on the random nodes in the cluster</p>
<p><img src="./images/week_6.png" width="150"></p>
<h4 id="data-flow-of-writing-data">Data flow of writing data</h4>
<p><img src="./images/week_7.png" width="500"></p>
<ol type="1">
<li><p>HDFS client request and name node via RPC protocol. &gt; The name node validates if you have rights to create a file and there are no naming conflicts.</p></li>
<li><p>HDFS client requests a list of datanodes to put a fraction of blocks of the file. &gt; These datanodes form a pipeline as your first client sends packets of data to the closest datanode. The later one transfers copies of packets through a datanode pipeline. As soon as packet is on all of the datanodes, datanodes send acknowledgment packets back.</p></li>
</ol>
<p><strong>If something goes wrong</strong> - then if the client closes the datanode pipeline, marks the misbehaving datanode bad and requests a replacement for the bad datanodes from a name node. So a new data node pipeline will be organized, and the process of writing the file to HDFS will continue.</p>
<h4 id="what-happens-with-failure-blocks">What happens with failure blocks?</h4>
<p><img src="./images/week_8.png" width="400"> Datanode serves a state machine for each block. Whenever a datanode recovers from its own failure, or failures of other datanodes in a pipeline, you can be sure that all the necessary replicas will be recovered. And unnecessary ones will be removed.</p>
<h1 id="block-and-replica-states">Block and Replica States</h1>
<p><strong>Replica</strong> : a physical data storage on a data node.There are usually several replicas with the same content on different data nodes.</p>
<p><strong>Block</strong>: a meta-information storage on a name node and provides information about replica's locations and their states.</p>
<p>Both replica and block have their own states. <img src="./images/week_9.png" width="250"></p>
<p><strong>Data node replica's states</strong>: Finalized, Replica Being Written to, Replica Under Recovery, Replica Waiting to be Recovered, Temporary</p>
<p><strong>Name node replica's states</strong>:</p>
<p><strong>Difference of Datanode &amp; Namenode</strong>: a block state is stored in memory, it doesn't persist on any disk.</p>
<h2 id="datanode-state-finalized">Datanode State: Finalized</h2>
<p><img src="./images/week_11.png" width="450"></p>
<p><strong>Finalized state</strong>: the content of this replica is frozen - Meaning: meta-information for this block on name node is aligned with all the corresponding replica's states and data.</p>
<ul>
<li><p><strong>Read consistency</strong>: you can safely read data from any data node and you will get exactly the same content.</p></li>
<li><strong>Generation Stamp(GS)</strong>: Each block of data has a version number called Generation Stamp. All of finalized replicas have the same GS number which can only increase over time. <img src="./images/week_10.png" width="300"></li>
<li><p>It happens during error <em>recovery process</em> or during <em>data appending to a block</em>.</p></li>
</ul>
<h2 id="datanode-state-replica-being-written">Datanode State: Replica Being Written</h2>
<p><img src="./images/week_12.png" width="450"></p>
<p><strong>RBW</strong>:the state of the last block of an open file or a file which was reopened for appending.</p>
<ul>
<li>Different data nodes can return to use a different set of bytes. In short, bytes that are acknowledged by the downstream data nodes in a pipeline are visible for a reader of this replica.</li>
<li>Data node on disk data and name node meta-information may not match during this state.</li>
<li><strong>Data Durability</strong>: In case of any failure data node will try to preserve as many bytes as possible.</li>
</ul>
<h2 id="datanode-state-replica-waiting-to-be-recovered">Datanode State: Replica Waiting to be Recovered</h2>
<p><img src="./images/week_13.png" width="450"></p>
<p><strong>RWR</strong>: a state of all Being Written replicas after data node failure and recovery after a system reboot or after Pacer.sys or BSOD,</p>
<ul>
<li>RWR replicas will not be in any data node pipeline and therefore will not receive any new data packets.</li>
<li>RWR either become <strong><em>outdated and should be discarded</em></strong>, or they will participate in a special recovery process called a <strong><em>lease recovery</em></strong> if the client also dies.</li>
</ul>
<p>HDFS client requests a <em>lease</em> from a name node to have an exclusive access to write or append data to a file. In case of HDFS client lease expiration, replica transition to a RUR state.</p>
<h2 id="datanode-state-replica-under-recovery">Datanode State: Replica Under Recovery</h2>
<p><img src="./images/week_14.png" width="450"></p>
<p><strong>RUR</strong> (Replica Under Recovery): HDFS client requests a <strong><em>lease</em></strong> from a name node to have an exclusive access to write or append data to a file. In case of HDFS client <strong><em>lease expiration</em></strong>(usually happens during the client's site failure), replica transition to a RUR state.</p>
<h2 id="datanode-state-temporary">Datanode State: Temporary</h2>
<p><img src="./images/week_15.png" width="450"></p>
<p><strong>Temporary</strong>: As data grows and different nodes are added or removed from a cluster, data can become unevenly distributed over the cluster nodes. A Hadoop administrator can spawn a process of data re-balancing or a data engineer can request increasing of the replication factor of data for the sake of durability. In these cases new generated replicas will be in a state called temporary.</p>
<ul>
<li>Similar to RBW except the fact that this data is not visible to user unless finalized.</li>
<li>In case of failure, the whole chunk of data is removed without any intermediate recovery state.</li>
</ul>
<h2 id="namenode-state-under-construction">Namenode State: Under Construction</h2>
<p><img src="./images/week_16.png" width="450"></p>
<p><strong>Under Construction</strong>: opens a file for writing, name node creates the corresponding block with the <em>under_construction</em> state; opens a file for append name node also transition this block to the state <em>under_construction</em>.</p>
<ul>
<li>always the last block of a file</li>
<li>it's length and generation stamp are mutable</li>
</ul>
<h2 id="namenode-state-under-recovery">Namenode State: Under Recovery</h2>
<p><img src="./images/week_17.png" width="450"></p>
<p>Name node block keeps track of write pipeline. It means that it contains information about all RBW and RWR replicas. Replicas transitions from RWR to recovery RUR state when the client dies. Even more generally it happens when a client's lease expires. Consequently, the corresponding block transitions from under_construction to under_recovery state. <img src="./images/week_18.png" width="450"></p>
<h2 id="namenode-state-committed">Namenode State: Committed</h2>
<p><img src="./images/week_19.png" width="450"> The under_construction block transitions to a committed state when a client successfully requests name node to close a file or to create a new consecutive block of data.</p>
<p>The committed state means that there are already some finalized replicas but not all of them. For this reason in order to serve a read request, the committed block needs to keep track of RBW replicas, until all the replicas are transitioned to the finalized state and HDFS client will be able to close the file.</p>
<h2 id="namenode-state-final-complete">Namenode State: Final Complete</h2>
<p><img src="./images/week_20.png" width="500"></p>
<p><strong>Final complete state</strong> of a block: a state where all the replicas are in the finalized state and therefore they have identical visible length and generation stamps.</p>
<ul>
<li>Only when all the blocks of a file are complete the file can be closed.</li>
</ul>
<h2 id="namenode-state-open-file">Namenode State: Open File</h2>
<p><strong>Open File state</strong>: In case of name node restart, it has to restore the open file state. All the blocks of the un-closed file are loaded as complete except the last block which is loaded as under_construction.</p>
<p>Then recovery procedures will start to work.</p>
<p><strong>Recovery</strong>: replica recovery, block recovery, lease recovery, and pipeline recovery.</p>
<h1 id="recovery-process">Recovery Process</h1>
<h2 id="block-recovery">Block Recovery</h2>
<p><strong>Goal</strong>: NameNode has to ensure that all of the corresponding replicas of a block will transition to a common state logically and physically.</p>
<p><strong>physically</strong>: all the correspondent replicas should have the same on disk content.</p>
<p>To accomplish it,</p>
<ol type="1">
<li><p><strong>primary datanode(PD)</strong>: NameNode chooses a primary datanode called PD in a design document. PD should contain a replica for the target block. <img src="./images/week_21.png" width="400"></p></li>
<li><p>PD request from a NameNode, a new generation stamp, information and location of other replicas for recovery process.</p></li>
</ol>
<p><img src="./images/week_22.png" width="400"></p>
<ol start="3" type="1">
<li>PD connects each relevant DataNodes to participate in the <strong>replica recovery process</strong>.During this phase, all the necessary information or data is propagated through the pipeline.</li>
</ol>
<p><strong>Replica recover process</strong> includes: - Aborting active clients right into a replica. - Aborting the previous replica of block recovery process, and participating in final replica size agreement process. <img src="./images/week_23.png" width="400"></p>
<ol start="4" type="1">
<li>As the last step, PD notifies NameNode about the result, success or failure. In case of failure, NameNode could retry block recovery process. <img src="./images/week_24.png" width="400"></li>
</ol>
<h2 id="lease-recovery">Lease Recovery</h2>
<blockquote>
<p>Block recovery process could happen only as a part of the lease recovery process.</p>
</blockquote>
<p>Lease manager manages all the leases at the NameNode. HDFS clients request at least every time they would like to write, or append to a file. <img src="./images/week_25.png" width="400"></p>
<h3 id="conditions-of-starting-lease-recovery-process">Conditions of starting lease recovery process</h3>
<p>Lease manager maintains a soft and a hard limit. If a current lease holder doesn't renew his lease during the soft limit timeout, then another client will be able to take over this lease. In this case and in the case of reaching a hard limit, the process of lease recovery will begin. <img src="./images/week_26.png" width="400"> <img src="./images/week_28.png" width="400"></p>
<p><strong>Necessity:</strong> to close open files for the sake of the client.</p>
<p><strong>Gurantees to be reached</strong>:</p>
<ul>
<li><p><strong>concurrency control</strong>: Even if a client is still alive, it won't be able to write data to a file</p></li>
<li><p><strong>consistency guarantee</strong>: All replicas should draw back to a consistence state to have the same on-disk data and generation stamp.</p></li>
</ul>
<p><img src="./images/week_29.png" width="400"></p>
<h3 id="lease-recovery-process">Lease recovery process</h3>
<p><img src="./images/week_30.png" width="400"></p>
<ol type="1">
<li>Lease recovery starts with a <strong>lease renew</strong>.</li>
</ol>
<p>New files lease holder should have the ability to take ownership of any other user's lease. The name of the super user is DFS. Therefore, all the other client request such as get new generation stamp, get new block, close file from other clients to this pass will be rejected.</p>
<ol start="2" type="1">
<li><p>NameNode gets the lease of DataNodes which contains the last block of a file, and sends a primary DataNode and starts a block recovery process. <img src="./images/week_30.png" width="400"></p></li>
<li><p>As soon as block recovery process finishes, the NameNode is notified by PD about the outcome. Updates blocking for, and removes the lease for a file.</p></li>
</ol>
<p><img src="./images/week_31.png" width="400"></p>
<h2 id="pipeline-recovery">Pipeline Recovery</h2>
<h3 id="pipeline">Pipeline</h3>
<p>When you write to an HDFS file, HDFS client writes data block by block. Each block is constructed through a <strong>write pipeline</strong>, as the first client breaks down block into pieces called <strong>packets</strong>. These packets are propagated to the DataNodes through the pipeline.</p>
<blockquote>
<p>Three stages: Pipeline setup, data streaming, and close</p>
</blockquote>
<ul>
<li>bold lines: data packets</li>
<li>doted lines: acknowledge messages</li>
<li>regular lines: control messages.</li>
</ul>
<p><img src="./images/week_32.png" width="400"></p>
<h4 id="setup">Setup</h4>
<p>a clients sends a setup message down to the pipeline. Each DataNode opens a replica for writing and sends ack message back upstream with the pipeline. <img src="./images/week_33.png" width="400"></p>
<h4 id="data-streaming">Data streaming</h4>
<p>Data streaming stage is defined by time range from t1 to t2, where t1 is the time when a client to receives the acknowledgement message for the top stage. And t2 is the time when the client receives the acknowledgement message for all the block packets. <img src="./images/week_34.png" width="400"></p>
<ul>
<li><p>data is buffered on the client site to form a packet, then propagated through the DataNode pipeline.</p></li>
<li><p>Next packet can be sent even before the acknowledgment of the previous packet is received.</p></li>
</ul>
<p><strong>flush</strong>: synchronous packets and used as synchronization points for the DataNode right.</p>
<p><img src="./images/week_35.png" width="400"></p>
<h4 id="close">Close</h4>
<p>Finalize replicas and shut down the pipeline.</p>
<p><img src="./images/week_36.png" width="400"></p>
<ul>
<li>All of the DataNodes in the pipeline change the replica state to the finalized.</li>
<li>Report the state to a NameNode and send the acknowledgement message upstream.</li>
</ul>
<h3 id="pipeline-recovery-process">Pipeline recovery process</h3>
<p>Pipeline recovery can be initiated during each of these stages.</p>
<h4 id="setup-failure">Setup failure</h4>
<p>A failure happens during writing to a new file, abandon DataNode pipeline and request a new one from scratch <img src="./images/week_37.png" width="400"></p>
<p>In this case, some packets can be resent but they will not be extra disk IO overhead for DataNodes that already saved this packet on disk. Once the client detects a failure during close stage, it rebuilds a pipeline with good DataNodes. Bumps generation stamp and requests to finalize replicas.</p>
<h4 id="data-streaming-failure">Data streaming failure</h4>
<p>If DataNode is not able to continue process packets appropriately,then it allots the DataNode pipeline about it, by closing all the connections.</p>
<p>When HDFS client detects a fire, it stops sending new packets to the existing pipeline, request a new generation stamp from a NameNode, and rebuilds a pipeline from good DataNodes. &gt; In this case, some packets can be resent but there will not be extra disk IO overhead for DataNodes that already saved this packet on disk.</p>
<p>All DataNodes keep track of bytes received, bytes written to a disk and bytes acknowledged. <img src="./images/week_38.png" width="400"></p>
<h4 id="close-failure">Close failure</h4>
<p>Once the client detects a failure during close stage, it rebuilds a pipeline with good DataNodes. Bumps generation stamp and requests to finalize replicas. <img src="./images/week_39.png" width="400"></p>
<h1 id="hdfs-client">HDFS Client</h1>
<h1 id="namenode-architecture">Namenode Architecture</h1>
<p><strong>NameNode</strong>: a service responsible for keeping hierarchy of folders and files.</p>
<ul>
<li>NameNode stores all of this data in memory.</li>
</ul>
<p><img src="./images/week40.png" width="300"></p>
<h2 id="capacity">Capacity</h2>
<p>For example: 10 petabytes of data.</p>
<p><strong>Capacity for data nodes:</strong> - In HDFS, all data is usually stored with replication factor three. &gt; So, you can request to buy approximately 15,000 of two terabyte hard drives.</p>
<ul>
<li>On average, 15 hard drives will die every day. &gt;So, you should request at least 1000 extra hard drives for several months of research.</li>
</ul>
<p><strong>Capacity for meta information in memory:</strong></p>
<ul>
<li><p><strong>Small files problem:</strong> Storing lot of small files which are extremely smaller than the block size cannot be efficiently handled by HDFS. Reading through small files involve lots of seeks and lots of hopping between data node to data node, which is inturn inefficient data processing.</p></li>
<li><p><strong>128 megabyte</strong> of data was once chosen as a default block size.</p></li>
</ul>
<blockquote>
<p>WHY? It is one choice to have less than one percent overhead for reading the random block of data from a hard drive, and keeping block size small at the same time.</p>
</blockquote>
<p><img src="./images/week_41.png" width="400"></p>
<p>10 petabytes data will consume at least 35 gigabyte of RAM on the NameNode.</p>
<h2 id="failure">Failure</h2>
<p><strong>NameNode server is a single point of failure.</strong> &gt; In case this service goes down, the whole HDFS storage becomes unavailable, even for read-only operations.</p>
<p>Technical tricks to make NameNode decisions durable and to speed up NameNode recovery process: - write-ahead log (WAL): strategy to persist matter information modifications. This log is called the edit log. It can be replicated to different hard drives. It is also usually replicated to an NFS storage.</p>
<ul>
<li><p>NFS storage:you will be able to tolerate full NameNode crash</p></li>
<li><p><strong>fsimage</strong>: have a snapshot of memory at some point in time from which you can replay transaction stored in the edit log.</p></li>
</ul>
<p><img src="./images/week41.png" width="400"></p>
<p><strong>secondary NameNode</strong></p>
<blockquote>
<p>tackle the problem that edit log grows fast, replay of one week of transactions from edit log will take several hours to boot a NameNode</p>
</blockquote>
<p>Secondary NameNode, or better to say, checkpoint NameNodes, compacts the edit log by creating a new fsimage.</p>
<p>New fsimage is made of all the fsimage by applying all stored transactions in edit log.</p>
<ul>
<li>secondary NameNode consumes the same amount of RAM to build a new fsimage.</li>
<li>secondary NameNode was considered a badly named service.</li>
<li>secondary NameNode <span class="math inline">\(\neq\)</span> backup NameNode</li>
</ul>
<p><img src="./images/week42.png" width="400"></p>
<h2 id="evaluate">Evaluate</h2>
<p>Evaluate how long it takes to read 10 petabytes of data from a hard drive with similar reading speed:</p>
<p><img src="./images/week43.png" width="300"></p>
<p>The amount of drives in your cluster has a linear relation to the speed of data processing</p>
<h2 id="summary">Summary</h2>
<ol type="1">
<li>explain and reason about HDFS Namenode architecture: (RAM; fsimage + edit log; block size)</li>
<li>estimate required resources for a Hadoop cluster</li>
<li>explain what small files problem is and where a bottleneck is</li>
<li>list differences between different types of Namenodes (Secondary / Checkpoint / Backup</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://nancyyanyu.github.io/undefined/b89c7c35/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Nancy Yan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Nancy's Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/undefined/b89c7c35/" class="post-title-link" itemprop="url">Machine Learning Basic Questions</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-06-17 17:49:31" itemprop="dateCreated datePublished" datetime="2019-06-17T17:49:31-05:00">2019-06-17</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-06-18 16:46:20" itemprop="dateModified" datetime="2019-06-18T16:46:20-05:00">2019-06-18</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">18k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">16 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          <h3 id="can-you-state-tom-mitchells-definition-of-learning-and-discuss-t-p-and-e">1. Can you state Tom Mitchell's definition of learning and discuss T, P and E?</h3>
<p>Mitchell (1997) provides the definition “A computer program is said to learn from <strong>experience E</strong> with respect to some class of <strong>tasks T</strong> and <strong>performance measure P</strong>, if its performance at tasks in <strong>T</strong>, as measured by <strong>P</strong>, improves with experience <strong>E</strong>.</p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/undefined/b89c7c35/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://nancyyanyu.github.io/undefined/4f6e00ef/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Nancy Yan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Nancy's Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/undefined/4f6e00ef/" class="post-title-link" itemprop="url">ESL Note: Model Averaging and Stacking</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-06-17 16:21:28" itemprop="dateCreated datePublished" datetime="2019-06-17T16:21:28-05:00">2019-06-17</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-06-18 12:33:58" itemprop="dateModified" datetime="2019-06-18T12:33:58-05:00">2019-06-18</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">1.2k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">1 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          
            <h2 id="bayesian-model-averaging">Bayesian Model Averaging</h2>
<p>We have a set of candidate models <span class="math inline">\(M_m\)</span>; m = 1,…,M for our training set <span class="math inline">\(Z\)</span>.</p>
<p><strong>Suppose</strong> <span class="math inline">\(\zeta\)</span> is some quantity of interest, for example, a prediction f(x) at some fixed feature value <span class="math inline">\(x\)</span>. The <strong><em>posterior distribution</em></strong> of <span class="math inline">\(\zeta\)</span> is <span class="math display">\[
\Pr(\zeta|\mathbf{Z})=\sum_{i=1}^M\Pr(\zeta|M_m,\mathbf{Z})\Pr(M_m| \mathbf{Z})
\]</span> with <strong><em>posterior mean</em></strong>: <span class="math display">\[
E(\zeta|\mathbf{Z})=\sum_{i=1}^ME(\zeta|M_m,\mathbf{Z})\Pr(M_m| \mathbf{Z})
\]</span> This Bayesian prediction is a weighted average of the individual predictions, with weights proportional to the posterior probability of each model.</p>
<h3 id="frequentist-viewpoint">Frequentist Viewpoint</h3>
<p>Given predictions <span class="math inline">\(\hat{f}_1(x); \hat{f}_2(x),…, \hat{f}_M(x)\)</span>, under squared-error loss, we can seek the weights $w = (w_1, w_2,…, w_M) $ such that <span class="math display">\[
\hat{w}=\arg \min_w E_P[Y-\sum_{i=1}^Mw_m\hat{f}_m(x)]^2
\]</span> Here the input value <span class="math inline">\(x\)</span> is fixed and the <span class="math inline">\(N\)</span> observations in the dataset <span class="math inline">\(Z\)</span> (and the target <span class="math inline">\(Y\)</span> ) are distributed according to <span class="math inline">\(P\)</span>. The solution is the population linear regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(\hat{F}(x)^T=[\hat{f}_1(x); \hat{f}_2(x),…, \hat{f}_M(x)]\)</span> : <span class="math display">\[
\hat{w}=E_P[\hat{F}(x)\hat{F}(x)^T]^{-1}E_P[\hat{F}(x)Y]
\]</span></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://nancyyanyu.github.io/undefined/c7bd9d66/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Nancy Yan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Nancy's Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/undefined/c7bd9d66/" class="post-title-link" itemprop="url">Deep Learning Questions Part I: UAT, Motivation</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-06-17 15:22:02" itemprop="dateCreated datePublished" datetime="2019-06-17T15:22:02-05:00">2019-06-17</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-06-18 16:46:15" itemprop="dateModified" datetime="2019-06-18T16:46:15-05:00">2019-06-18</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">3.5k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">3 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          <h2 id="universal-approximation-of-neural-networks">Universal Approximation of neural networks</h2>
<h3 id="state-the-universal-approximation-theorem-what-is-the-technique-used-to-prove-that">1. State the universal approximation theorem? What is the technique used to prove that?</h3>
<p><strong>Universal approximation theorem</strong> (Hornik et al., 1989; Cybenko, 1989) states that a feedforward network with a linear output layer and at least one hidden layer with any “squashing” activation function (such as the logistic sigmoid activation function) can approximate any Borel measurable function from one finite-dimensional space to another with any desired non-zero amount of error, provided that the network is given enough hidden units.</p>
<p>The universal approximation theorem means that <strong>regardless of what function we are trying to learn, we know that a large MLP will be able to represent this function.</strong></p>
<p>However, we are not guaranteed that the training algorithm will be able to learn that function. Even if the MLP is able to represent the function, learning can fail for two different reasons.</p>
<ol type="1">
<li>The <strong>optimization algorithm</strong> used for training may not be able to find the value of the parameters that corresponds to the desired function.</li>
<li>The training algorithm might <strong>choose the wrong function due to overfitting</strong></li>
</ol>
<p>The universal approximation theorem says that there exists a network large enough to achieve any degree of accuracy we desire, but the theorem does not say how large this network will be.</p>
<h3 id="what-is-a-borel-measurable-function">2. What is a Borel measurable function?</h3>
<p>Any continuous function on a closed and bounded subset of <span class="math inline">\(R^n\)</span> is Borel measurable and therefore may be approximated by a neural network.</p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/undefined/c7bd9d66/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://nancyyanyu.github.io/undefined/25b6d1fa/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Nancy Yan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Nancy's Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/undefined/25b6d1fa/" class="post-title-link" itemprop="url">Machine Learning Questions - Part IV: Clustering & Bayesian</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-06-17 15:22:00" itemprop="dateCreated datePublished" datetime="2019-06-17T15:22:00-05:00">2019-06-17</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-06-18 16:45:53" itemprop="dateModified" datetime="2019-06-18T16:45:53-05:00">2019-06-18</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">8.3k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">8 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          <h2 id="clustering">Clustering</h2>
<h3 id="describe-the-k-means-algorithm.">1. Describe the k-means algorithm.</h3>
<p><strong>K-means clustering</strong> is a simple and elegant approach for partitioning a data set into K distinct, <strong><em>non-overlapping</em></strong> clusters.</p>
<p>The idea behind <strong>K-means clustering</strong> is that a <em>good</em> clustering is one for which the <strong><em>within-cluster</em></strong> <strong><em>variation</em></strong> is as small as possible.</p>
<p>The <strong>within-cluster variation</strong> for cluster <span class="math inline">\(C_k\)</span> is a measure <span class="math inline">\(W(C_k)\)</span> of the amount by which the observations within a cluster differ from each other.</p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/undefined/25b6d1fa/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Nancy Yan</p>
              <div class="site-description motion-element" itemprop="description"></div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">42</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">3</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">28</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                  
                    
                  
                  <a href="https://github.com/nancyyanyu" title="GitHub &rarr; https://github.com/nancyyanyu" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i></a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                  
                    
                  
                  <a href="mailto:yy2799@columbia.edu" title="E-Mail &rarr; mailto:yy2799@columbia.edu" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i></a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                  
                    
                  
                  <a href="https://www.linkedin.com/in/nancy-yanyu-yan" title="LinkedIn &rarr; https://www.linkedin.com/in/nancy-yanyu-yan" rel="noopener" target="_blank"><i class="fa fa-fw fa-linkedin"></i></a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                  
                    
                  
                  <a href="https://instagram.com/nancy_yanyu_yan" title="Instagram &rarr; https://instagram.com/nancy_yanyu_yan" rel="noopener" target="_blank"><i class="fa fa-fw fa-instagram"></i></a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Nancy Yan</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
    <span title="Symbols count total">310k</span>
  

  
</div>









        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>





  



  






  



  
    
    
      
    
  
  <script color="0,0,255" opacity="0.5" zindex="-1" count="99" src="//cdn.bootcss.com/canvas-nest.js/1.0.1/canvas-nest.min.js"></script>



  
  



  
  



  
  



  
  
  <script id="ribbon" size="300" alpha="0.6" zindex="-1" src="/lib/canvas-ribbon/canvas-ribbon.js"></script>





  
  <script src="//cdn.jsdelivr.net/jquery/2.1.3/jquery.min.js"></script>

  
  <script src="//cdn.jsdelivr.net/fastclick/1.0.6/fastclick.min.js"></script>

  
  <script src="//cdn.jsdelivr.net/npm/jquery-lazyload@1/jquery.lazyload.min.js"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script src="/lib/fancybox/source/jquery.fancybox.pack.js"></script>

  
  <script src="/lib/three/three.min.js"></script>

  
  <script src="/lib/three/three-waves.min.js"></script>

  
  <script src="/lib/three/canvas_lines.min.js"></script>

  
  <script src="/lib/three/canvas_sphere.min.js"></script>


  


  <script src="/js/utils.js?v=7.1.2"></script>

  <script src="/js/motion.js?v=7.1.2"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.1.2"></script>




  

  


  <script src="/js/next-boot.js?v=7.1.2"></script>


  

  

  

  





  




  

  

  
  

  
  
    
      
        
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

    
  


  

  

  

  

  

  

  

  

  

  

  

</body>
</html>

<script type="text/javascript" src="/js/src/dynamic_bg.js"></script>

