<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang>
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">

<script>
    (function(){
        if(''){
            if (prompt('Show me your password') !== ''){
                alert('Blah, wrong.');
                history.back();
            }
        }
    })();
</script>


<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">






  
  
  <link rel="stylesheet" href="/lib/Han/dist/han.min.css?v=3.3">













  
  
  <link rel="stylesheet" href="/lib/fancybox/source/jquery.fancybox.css">







  

<link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">

<link rel="stylesheet" href="/css/main.css?v=7.1.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.2">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.1.2">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.1.2">


  <link rel="mask-icon" href="/images/logo.svg?v=7.1.2" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.1.2',
    sidebar: {"position":"right","display":"hide","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: true,
    fastclick: true,
    lazyload: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta property="og:type" content="website">
<meta property="og:title" content="Nancy&#39;s Notes">
<meta property="og:url" content="https://nancyyanyu.github.io/index.html">
<meta property="og:site_name" content="Nancy&#39;s Notes">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Nancy&#39;s Notes">



  <link rel="alternate" href="/atom.xml" title="Nancy's Notes" type="application/atom+xml">



  
  
  <link rel="canonical" href="https://nancyyanyu.github.io/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Nancy's Notes</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-right 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Nancy's Notes</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">Code changes world!</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home menu-item-active">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-resumé">

    
    
    
      
    

    

    <a href="/resume/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>Resumé</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-machine-learning">

    
    
    
      
    

    

    <a href="/categories/Machine-Learning" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Machine Learning</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-journal">

    
    
    
      
    

    

    <a href="/categories/Journal/" rel="section"><i class="menu-item-icon fa fa-fw fa-coffee"></i> <br>Journal</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-手帳">

    
    
    
      
    

    

    <a href="/techou/" rel="section"><i class="menu-item-icon fa fa-fw fa-heart"></i> <br>手帳</a>

  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://nancyyanyu.github.io/undefined/a2f8a358/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Nancy Yan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Nancy's Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/undefined/a2f8a358/" class="post-title-link" itemprop="url">Machine Learning Questions - Part II</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-06-15 05:53:38 / Modified: 17:30:41" itemprop="dateCreated datePublished" datetime="2019-06-15T05:53:38-05:00">2019-06-15</time>
            

            
              

              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">4.8k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">4 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          
            <h2 id="curse-of-dimensionality">Curse of dimensionality</h2>
<h3 id="describe-the-curse-of-dimensionality-with-examples.">1. Describe the curse of dimensionality with examples.</h3>
<p><strong><em>Curse of dimensionality:</em></strong> as the dimensionality of the features space increases, the number configurations can grow exponentially, and thus the number of configurations covered by an observation decreases.</p>
<p><strong>As the number of feature or dimensions grows, the amount of data we need to generalise accurately grows exponentially.</strong></p>
<p>（fun example: It's easy to hunt a dog and maybe catch it if it were running around on the plain (two dimensions). It's much harder to hunt birds, which now have an extra dimension they can move in. If we pretend that ghosts are higher-dimensional beings ）</p>
<h3 id="what-is-local-constancy-or-smoothness-prior-or-regularization">2. What is local constancy or smoothness prior or regularization?</h3>
<p>(See DL Book 5.11.2)</p>
<p><strong>Smoothness prior</strong> or <strong>local constancy prior</strong>: This prior states that the function we learn should not change very much within a small region.</p>
<p>Many simpler algorithms rely exclusively on this prior to generalize well, and as a result they fail to scale to the statistical challenges involved in solving AIlevel tasks.</p>
<ul>
<li>KNN, decision trees, local kernel</li>
</ul>
<p>All of these different methods are designed to encourage the learning process to learn a function <span class="math inline">\(f^*\)</span> that satisfies the condition <span class="math display">\[
f^*(x)\approx f^*(x+\epsilon)
\]</span> In other words, if we know a good answer for an input x (for example, if x is a labeled training example) then that answer is probably good in the neighborhood of x.</p>
<p>Assuming only <strong>smoothness</strong> of the underlying function will not allow a learnerto represent a complex function that has many more regions to be distinguished than the number of training examples</p>
<h2 id="regularization">Regularization</h2>
<h3 id="what-is-l1-regularization">1. What is L1 regularization?</h3>
<p>L1 lasso penalty: <span class="math inline">\(\sum_{j=1}^p |\beta_j|\)</span></p>
<p>A type of regularization that penalizes weights in proportion to the <strong>sum of the absolute values</strong> of the weights. In models relying on <strong>sparse features</strong>, L1 regularization helps drive the weights of irrelevant or barely relevant features to exactly 0, which removes those features from the model.</p>
<h3 id="what-is-l2-regularization">2. What is L2 regularization?</h3>
<p>L2 ridge penalty : <span class="math inline">\(\sum_{j=1}^p\beta_j^2\)</span></p>
<p>A type of regularization that penalizes weights in proportion to the sum of the squares of the weights. L2 regularization helps drive outlier weights (those with high positive or low negative values) closer to 0 but not quite to 0. L2 regularization always improves generalization in linear models.</p>
<h3 id="compare-l1-and-l2-regularization.">3. Compare L1 and L2 regularization.</h3>
<p><strong>SAME</strong>: Ridge &amp; Lasso all can yield a reduction in variance at the expense of a small increase in bias, and consequently can generate more accurate predictions.</p>
<p><strong>DIFFERENCES</strong>:</p>
<ul>
<li>Unlike ridge regression, the <strong>lasso performs variable selection</strong>, and hence results in models that are easier to interpret.</li>
<li>ridge regression outperforms the lasso in terms of prediction error in this setting</li>
</ul>
<p><strong>Suitable setting</strong>:</p>
<ul>
<li><strong>Lasso</strong>: perform better in a setting where a relatively small number of predictors have substantial coefficients, and the remaining predictors have coefficients that are very small or that equal zero.</li>
<li><strong>Ridge regression</strong>: perform better when the response is a function of many predictors, all with coefficients of roughly equal size.</li>
<li>The number of predictors that is related to the response is never known a <strong>priori</strong> for real data sets. Cross-validation can be used in order to determine which approach is better on a particular data set.</li>
</ul>
<h3 id="why-does-l1-regularization-result-in-sparse-models">4. Why does L1 regularization result in sparse models?</h3>
<p>The lasso and ridge regression coefficient estimates are given by the first point at which an ellipse contacts the constraint region.</p>
<p><strong>Ridge regression</strong>: <strong>circular</strong> constraint with no sharp points, so the ridge regression coefficient estimates will be exclusively non-zero.</p>
<p><strong>The lasso</strong>: constraint has <strong>corners</strong> at each of the axes, and so the ellipse will often intersect the constraint region at an axis.</p>
<ul>
<li>The <span class="math inline">\(l_1\)</span> penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter λ is sufficiently large.</li>
<li>Hence, much like best subset selection, the lasso performs <strong>variable selection</strong></li>
</ul>
<blockquote>
<p>Lasso yields <strong>sparse</strong> models</p>
</blockquote>
<p><img src="./1.png" width="600"></p>
<h2 id="evaluation-of-machine-learning-systems">Evaluation of Machine Learning systems</h2>
<h3 id="what-are-accuracy-sensitivity-specificity-roc">1. What are accuracy, sensitivity, specificity, ROC?</h3>
<h3 id="what-are-precision-and-recall">2. What are precision and recall?</h3>
<h3 id="describe-t-test-in-the-context-of-machine-learning.">3. Describe t-test in the context of Machine Learning.</h3>
<h2 id="dimensionality-reduction">Dimensionality Reduction</h2>
<ol type="1">
<li>Why do we need dimensionality reduction techniques? (data compression, speeds up learning algorithm and visualizing data)</li>
<li>What do we need PCA and what does it do? (PCA tries to find a lower dimensional surface such the sum of the squared projection error is minimized)</li>
<li>What is the difference between logistic regression and PCA?</li>
<li>What are the two pre-processing steps that should be applied before doing PCA? (mean normalization and feature scaling)</li>
</ol>
<p><strong>Ref</strong>:</p>
<p><a href="https://github.com/Sroy20/machine-learning-interview-questions" target="_blank" rel="noopener">machine-learning-interview-questions</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://nancyyanyu.github.io/undefined/6bd38994/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Nancy Yan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Nancy's Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/undefined/6bd38994/" class="post-title-link" itemprop="url">Machine Learning Questions - Part I</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-06-14 22:57:50" itemprop="dateCreated datePublished" datetime="2019-06-14T22:57:50-05:00">2019-06-14</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-06-15 16:45:42" itemprop="dateModified" datetime="2019-06-15T16:45:42-05:00">2019-06-15</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">20k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">18 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          
            <h2 id="learning-theory">Learning Theory</h2>
<h3 id="describe-bias-and-variance-with-examples.">1. Describe bias and variance with examples.</h3>
<p><strong><em>Variance</em></strong>: refers to the amount by which <span class="math inline">\(\hat{f}\)</span> would change if we estimated it using a different training data set. <strong><em>more flexible statistical methods have higher variance</em></strong></p>
<ul>
<li>Explanation: different training data sets will result in a different <span class="math inline">\(\hat{f}\)</span>. But ideally the estimate for f should not vary too much between training sets. However, if a method has high variance then small changes in the training data can result in large changes in <span class="math inline">\(\hat{f}\)</span></li>
</ul>
<p><strong><em>Bias</em></strong>: refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model.</p>
<ul>
<li>Explanation: As we increase the flexibility of a class of methods, the bias tends to initially decrease faster than the variance increases. Consequently, the expected test MSE declines. However, at some point increasing flexibility has little impact on the bias but starts to significantly increase the variance. When this happens the test MSE increases.</li>
</ul>
<p><strong><em>Decomposition</em></strong>：The expected test MSE, for a given value <span class="math inline">\(x_0\)</span> can always be decomposed into the sum of three fundamental quantities: <strong>the variance of <span class="math inline">\(\hat{f}(x_0)\)</span>, the squared bias of <span class="math inline">\(\hat{f}(x_0)\)</span>, and the variance of the error variance terms <span class="math inline">\(\epsilon\)</span>.</strong> <span class="math display">\[
\begin{align}
E(y_0-\hat{f}(x_0))^2=Var(\hat{f}(x_0))+[Bias(\hat{f}(x_0))]^2+Var(\epsilon)
\end{align}
\]</span> The overall expected test MSE can be computed by averaging <span class="math inline">\(E(y_0-\hat{f}(x_0))^2\)</span> over all possible values of x0 in the test set.</p>
<p><img src="./2.png" width="600"></p>
<h3 id="what-is-empirical-risk-minimization">2. What is Empirical Risk Minimization?</h3>
<p><strong><em>ERM</em></strong>: the function that minimizes loss on the training set.</p>
<p>In many machine learning task, we have data <span class="math inline">\(Z\)</span> from some distribution <span class="math inline">\(p\)</span> and the task is to minimize the risk: <span class="math display">\[
R_(f)=E_{Z \sim p}[loss(f(Z),Z)]
\]</span> <strong>Loss function</strong>: In classification <span class="math inline">\(Z = (X, Y )\)</span> and we use 0/1 loss <span class="math inline">\(loss(f(Z), Z) = I_{f(X) \neq Y}\)</span> , in regression $Z = (X, Y ) $and we use squared error <span class="math inline">\(loss(f(Z), Z) = (f(X) − Y )^2\)</span> and in density estimation <span class="math inline">\(Z = X\)</span> and we use negative log likelihood loss <span class="math inline">\(loss(f(Z), Z) = − \log f(X)\)</span>. We are interested in finding the optimal predictor <span class="math display">\[
f^*=\arg\min_f R(f)
\]</span> In practice, we compute the empirical risk: <span class="math display">\[
\hat{R}(f)=\frac{1}{n} \sum_{i=1}^n loss(f(X_i),Y_I)
\]</span> We choose the <span class="math inline">\(\hat{f}\)</span> that minimizes the empirical risk over some class <span class="math inline">\(F\)</span>, such as parametric models, histogram classifiers, decision trees or linear/polynomial functions, etc. <span class="math display">\[
\hat{f}^{ERM}=\arg \min_{f \in F} \hat{R}(f)
\]</span></p>
<h3 id="write-the-formulae-for-training-error-and-generalization-error.-point-out-the-differences.">3. Write the formulae for training error and generalization error. Point out the differences.</h3>
<p><strong><em>Generalization error</em></strong> is defined as the expected value of the error on a new input. Here the expectation is taken across different possible inputs, drawn from the distribution of inputs we expect the system to encounter in practice. [DL Book]</p>
<p><strong><em>Training error:</em></strong> when training a machine learning model, we have access to a training set, we can compute some error measure on the training set called the training error, and we reduce this training error.</p>
<p>In our linear regression example, we trained the model by minimizing the <em>training error</em>: <span class="math display">\[
\frac{1}{m^{(train)}}||\mathbf{X}^{(train)}\mathbf{w}-\mathbf{y}^{(train)}||^2_2
\]</span> but we actually care about the <em>test error</em>: <span class="math display">\[
\frac{1}{m^{(test)}}||\mathbf{X}^{(test)}\mathbf{w}-\mathbf{y}^{(test)}||^2_2
\]</span> <strong>Error v.s. Loss v.s. Risk</strong>:</p>
<p><strong><em>Error</em></strong> is the difference between the actual / true value (𝜃) and the predicted / estimated value (𝜃̂ ) <span class="math display">\[
Error=\theta-\hat{\theta}
\]</span> <strong><em>Loss (𝐿)</em></strong> is a measurement of how well our model perform against your <strong>training data</strong>.</p>
<ul>
<li><p><strong>Regression Losses</strong></p>
<ul>
<li><strong>Mean Square Error/Quadratic Loss/L2 Loss</strong></li>
</ul>
<p><span class="math display">\[
MSE=\frac{\sum_{i=1}^n(y_i-\hat{y}_i)^2}{n}
\]</span></p>
<ul>
<li><strong>Mean Absolute Error/L1 Loss</strong></li>
</ul>
<p><span class="math display">\[
MAE=\frac{\sum_{i=1}^n|y_i-\hat{y}_i|}{n}
\]</span></p>
<ul>
<li><strong>Mean Bias Error</strong> <span class="math display">\[
MBE=\frac{\sum_{i=1}^n(y_i-\hat{y}_i)}{n}
\]</span></li>
</ul></li>
<li><p><strong>Classification Losses</strong></p>
<ul>
<li><strong>Hinge Loss/Multi class SVM Loss</strong> <span class="math display">\[
SVM Loss=\sum_{j \neq y_i} \max(0,s_j-s_{y_i}+1) \\
L(\mathbf{X},\mathbf{y},\beta)=\sum_{i=1}^n \max[0,1-y_i(\beta_0+\beta_1x_{i1}+,,,+\beta_px_{ip})]
\]</span></li>
</ul></li>
<li><p><strong>Cross Entropy Loss/Negative Log Likelihood</strong></p></li>
</ul>
<p><span class="math display">\[
CrossEntropyLoss=-\left[y_i\log(\hat{y_i})+(1-y_i)\log (1-y_i)\right]
\]</span></p>
<p><strong><em>Risk</em></strong> is the average measure of loss, or expected loss, across your whole data distribution. <span class="math display">\[
R(\theta,\hat{\theta})=E(L(\theta,\hat{\theta}))
\]</span></p>
<p><strong><em>Empirical Risk:</em></strong> when we train our model, we do not have the full distribution of the data. This may be because some of our data is used for validation and testing, or that new data points are produced in real-time. The best we can do is to pick our training data in a random way and assume that our training data is representative of the real data.</p>
<p>Therefore, because we don't have all the data, the best we can do is to minimize the empirical risk, from data that we do have (our training data), and use regularization techniques to generalize (i.e. avoid overfitting). This is why minimizing loss and minimizing empirical risk are roughly the same thing.</p>
<h3 id="what-is-the-bias-variance-trade-off-theorem">4. What is the bias-variance trade-off theorem?</h3>
<p><strong><em>Bias-variance trade-off</em></strong>: If our model is too simple and has very few parameters then it may have <strong>high bias and low variance</strong>. On the other hand if our model has large number of parameters then it’s going to have <strong>high variance and low bias</strong>. So we need to find the right/good balance without overfitting and underfitting the data.</p>
<p><img src="./1.png" width="600"></p>
<p><em>Bias and variance using bulls-eye diagram:</em></p>
<p><img src="./3.png" width="300"></p>
<p><strong><em>Underfitting</em></strong> happens when a model unable to capture the underlying pattern of the data <span class="math inline">\(\Rightarrow\)</span> high bias, low variance.</p>
<p><strong><em>Overfitting</em></strong> happens when our model captures the noise along with the underlying pattern in data. It happens when we train our model a lot over noisy dataset <span class="math inline">\(\Rightarrow\)</span> low bias and high variance.</p>
<p><img src="./5.png" width="600"></p>
<p>State the uniform convergence theorem and derive it.</p>
<p>What is sample complexity bound of uniform convergence theorem?</p>
<p>What is error bound of uniform convergence theorem?</p>
<p>From the bias-variance trade-off, can you derive the bound on training set size?What is the VC dimension?What does the training set size depend on for a finite and infinite hypothesis set? Compare and contrast.What is the VC dimension for an n-dimensional linear classifier?How is the VC dimension of a SVM bounded although it is projected to an infinite dimension?Considering that Empirical Risk Minimization is a NP-hard problem, how does logistic regression and SVM loss work?</p>
<h2 id="model-and-feature-selection">Model and feature selection</h2>
<h3 id="why-are-model-selection-methods-needed">1. Why are model selection methods needed?</h3>
<ul>
<li><p><strong><em>Model Interpretability</em></strong>：Irrelevant variables leads to unnecessary complexity in the resulting model. By removing these variables—that is, by setting the corresponding coefficient estimates to zero—we can obtain a model that is more easily interpreted.</p></li>
<li>We want to estimate the generalization performance, the predictive performance of our model on future (unseen) data.</li>
<li>We want to increase the Prediction Accuracy by tweaking the learning algorithm and selecting the best performing model from a given hypothesis space.</li>
<li>We want to identify the machine learning algorithm that is best-suited for the problem at hand; thus, we want to compare different algorithms, selecting the best-performing one as well as the best performing model from the algorithm’s hypothesis space.</li>
<li>Fast (to train and test)</li>
<li><p>Scalable (it can be applied to a large dataset)</p></li>
</ul>
<p>The idea of model selection method is intuitive. It answers the following question:</p>
<blockquote>
<p><strong><em>How to select the right input variables for an optimal model?</em></strong></p>
</blockquote>
<p><strong>An Optimal model is a model that fits the data with best values for the evaluation metrics.</strong></p>
<p>(Feature selection simplifies a machine learning problem by choosing which subset of the available features should be used.)</p>
<h3 id="how-do-you-do-a-trade-off-between-bias-and-variance">2. How do you do a trade-off between bias and variance?</h3>
<p><strong>Shrinkage methods:</strong> By <strong>constraining</strong> or <strong>shrinking</strong> the estimated coefficients, we can often substantially reduce the variance at the cost of a negligible increase in bias.</p>
<p><strong>Bagging</strong> and other <strong>resampling techniques</strong> can be used to reduce the variance in model predictions.</p>
<h3 id="why-is-cross-validation-required">3. Why is cross-validation required?</h3>
<p><strong>Cross-validation</strong>: can be used to estimate the test error associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility.</p>
<p>Cross Validation is <strong>a very useful technique for assessing the effectiveness of your model, particularly in cases where you need to mitigate overfitting.</strong> You need some kind of <strong>assurance that your model has got most of the patterns from the data correct, and its not picking up too much on the noise, or in other words its low on bias and variance.</strong></p>
<p>Evaluation of residuals only gives us an idea about how well our model does on data used to train it. So, the problem with this evaluation technique is that <strong>it does not give an indication of how well the learner will generalize to an independent/ unseen data set.</strong> Getting this idea about our model is known as Cross Validation.</p>
<p>It is <strong>also of use in determining the hyper parameters of your model</strong>, in the sense that which parameters will result in lowest test error.</p>
<h3 id="describe-different-cross-validation-techniques.">4. Describe different cross-validation techniques.</h3>
<h4 id="what-is-hold-out-cross-validation-what-are-its-advantages-and-disadvantages">What is hold-out cross validation? What are its advantages and disadvantages?</h4>
<p><strong>Hold-out cross validation</strong>:</p>
<ol type="1">
<li>Randomly dividing the available set of observations into two parts, a <strong>training set</strong> and a <strong>validation set</strong> or hold-out set.</li>
<li>The model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set.</li>
<li>The resulting validation set error rate—typically assessed using MSE in the case of a quantitative response—provides an estimate of the test error rate.</li>
</ol>
<p><strong>Advantage</strong>: this method doesn’t take any overhead to compute and is better than traditional validation</p>
<p><strong>Disadvantage</strong>:</p>
<ul>
<li><strong>High variance</strong>: because it is not certain which data points will end up in the validation set and the result might be entirely different for different sets.</li>
<li>Removing a part of it for validation poses a problem of <strong>underfitting</strong></li>
</ul>
<h4 id="what-is-k-fold-cross-validation-what-are-its-advantages-and-disadvantages">What is k-fold cross validation? What are its advantages and disadvantages?</h4>
<p><strong>K-fold cross validation</strong>:</p>
<ol type="1">
<li>Randomly k-fold CV dividing the set of observations into k groups, or folds, of approximately equal size.</li>
<li>The first fold is treated as a validation set, and the method is fit on the remaining k − 1 folds.</li>
<li>The mean squared error, MSE1, is then computed on the observations in the held-out fold. This procedure is repeated k times; each time, a different group of observations is treated as a validation set.</li>
<li>This process results in k estimates of the test error, MSE1,MSE2, . . . ,MSEk.</li>
<li>The k-fold CV estimate is computed by averaging these values,</li>
</ol>
<p><span class="math display">\[
\begin{align}
CV_{(k)}=\frac{1}{k}\sum_{i=1}^kMSE_i
\end{align}
\]</span></p>
<p><strong>Advantage</strong>: significantly <strong><em>reduces bias</em></strong> as we are using most of the data for fitting, and also significantly <strong><em>reduces variance</em></strong> as most of the data is also being used in validation set.</p>
<p><strong>Disadvantage</strong>: the training algorithm has to be rerun from scratch <em>k</em> times, which means it takes <em>k</em> times as much computation to make an evaluation.</p>
<h4 id="what-is-leave-one-out-cross-validation-what-are-its-advantages-and-disadvantages">What is leave-one-out cross validation? What are its advantages and disadvantages?</h4>
<p><strong>Leave-one-out cross validation</strong>: leaves 1 data points out of training data as validation set. The statistical learning method is fit on the n − 1 training observations, and a prediction <span class="math inline">\(y_1\)</span> is made for the excluded observation, using its value <span class="math inline">\(x_1\)</span>. and then the error is averaged for all trials, to give overall effectiveness.</p>
<p>Since $ (x_1, y_1)$ was not used in the fitting process, <span class="math inline">\(MSE_1 =(y_1 − \hat{y}_1)^2\)</span> provides an approximately <strong><em>unbiased</em></strong> estimate for the test error.But even though <span class="math inline">\(MSE_1\)</span> is unbiased for the test error, it is a poor estimate because it is <strong><em>highly variable</em></strong>, since it is based upon a single observation $ (x_1, y_1)$.</p>
<p><strong>Advantages and disadvantages</strong>:</p>
<ul>
<li><p>k-Fold more biased than LOOCV; k-Fold less variance than LOOCV</p></li>
<li>When we perform LOOCV, we are in effect averaging the outputs of n fitted models, each of which is trained on an almost identical set of observations; therefore, these outputs are highly (positively) correlated with each other.</li>
<li><p>very expensive to compute</p></li>
</ul>
<h3 id="why-is-feature-selection-required">5. Why is feature selection required?</h3>
<ul>
<li><p><strong>Reduces Overfitting</strong>: Less redundant data means less opportunity to make decisions based on noise.</p></li>
<li><p><strong>Improves Accuracy:</strong> Less misleading data means modeling accuracy improves.</p></li>
<li><p><strong>Reduces Training Time</strong>: fewer data points reduce algorithm complexity and algorithms train faster.</p></li>
</ul>
<h3 id="describe-some-feature-selection-methods.">6. Describe some feature selection methods.</h3>
<ul>
<li><strong>Filter Methods:</strong> apply a statistical measure to assign a scoring to each feature. The features are ranked by the score and either selected to be kept or removed from the dataset.
<ul>
<li>distance metrics, correlation, mutual information, and consistency metrics</li>
</ul></li>
<li><strong>Wrapper Methods:</strong> consider the selection of a set of features as a search problem, where different combinations are prepared, evaluated and compared to other combinations. A predictive model is used to evaluate a combination of features and assign a score based on model accuracy.
<ul>
<li>Best Subset, forward and backward, recursive feature elimination</li>
</ul></li>
<li><strong>Embedded Methods:</strong> learn which features best contribute to the accuracy of the model while the model is being created.
<ul>
<li>regularization methods: LASSO, Elastic Net and Ridge Regression</li>
</ul></li>
</ul>
<h3 id="what-is-forward-feature-selection-method-what-are-its-advantages-and-disadvantages">7. What is forward feature selection method? What are its advantages and disadvantages?</h3>
<p><strong>Forward stepwise selection</strong> starts with the intercept, and then sequentially adds into the model the predictor that most improves the ﬁt.</p>
<p><strong>Advantages and Disadvantages</strong>:</p>
<ul>
<li><strong>Computational</strong>: for large p we cannot compute the best subset sequence, but we can always compute the forward stepwise sequence</li>
<li><strong>Statistical</strong>: a price is paid in variance for selecting the best subset of each size; forward stepwise is a more constrained search, and will have lower variance, but perhaps more bias</li>
</ul>
<p><img src="./6.png" width="600"></p>
<h3 id="what-is-backward-feature-selection-method-what-are-its-advantages-and-disadvantages">8. What is backward feature selection method? What are its advantages and disadvantages?</h3>
<p><strong>Backward-stepwise selection</strong> starts with the full model, and sequentially deletes the predictor that has the least impact on the ﬁt. The candidate for dropping is the variable with the smallest Z-score</p>
<p><strong>Advantages and Disadvantages</strong>:</p>
<ul>
<li><p>Backward-stepwise selection can only be used when N&gt;p, while forward stepwise can always be used.</p></li>
<li><p>Like forward stepwise selection, backward stepwise selection is not guaranteed to yield the best model containing a subset of the p predictors.</p></li>
</ul>
<p><img src="./7.png" width="600"></p>
<h3 id="what-is-filter-feature-selection-method-and-describe-two-of-them">9. What is filter feature selection method and describe two of them?</h3>
<p><strong>Filter Methods:</strong> apply a statistical measure to assign a scoring to each feature. The features are ranked by the score and either selected to be kept or removed from the dataset.</p>
<ul>
<li><p><a href="http://nlp.stanford.edu/IR-book/html/htmledition/feature-selectionchi2-feature-selection-1.html" target="_blank" rel="noopener"><strong>Chi-squared</strong></a> <em>:</em>Chi-square test is used for categorical features in a dataset. We calculate Chi-square between each feature and the target and select the desired number of features with best Chi-square scores. to evaluate how likely it is that any observed difference between the sets arose by chance or if the association between two categorical variables of the sample would reflect their real association in the population.</p></li>
<li><p><a href="http://www.ime.unicamp.br/~wanderson/Artigos/correlation_based_feature_selection.pdf" target="_blank" rel="noopener"><strong>Correlation</strong></a> <strong>:</strong> The Correlation Feature Selection (CFS) measure evaluates subsets of features on the basis of the following hypothesis: &quot;Good feature subsets contain features highly correlated with the classification, yet uncorrelated to each other</p></li>
<li><p><a href="https://hal.archives-ouvertes.fr/hal-00617969/document" target="_blank" rel="noopener"><strong>Entropy</strong></a> <strong>:</strong> Entropy measures the amount of information in a random variable; it’s the average length of the message needed to transmit an outcome of that variable using the optimal code.</p>
<ul>
<li><p><strong>Information content</strong>: <span class="math inline">\(I(E)=-\log [\Pr(E)]=-\log (P)\)</span></p></li>
<li><p>Define the <strong>entropy</strong> as the expected value of information: <span class="math display">\[
H(X)=E[I(X)]=E[-\log (P(X))]=-\sum_{i=1}^nP(x_i)\log (P(x_i))
\]</span></p></li>
</ul></li>
<li><p><a href="http://www.saedsayad.com/oner.htm" target="_blank" rel="noopener"><strong>One-attribute-rule(OneR)</strong></a> <strong>:</strong> The idea of the OneR (one-attribute-rule) algorithm is to find the one attribute to use that makes fewest prediction errors.</p></li>
</ul>
<h3 id="what-is-cross-entropy-and-kl-divergencedescribe-kl-divergence-intuitively.">10. What is Cross Entropy and KL divergence?Describe KL divergence intuitively.</h3>
<p><strong>Cross entropy</strong> is, at its core, a way of measuring the “distance” between two probability distributions P and Q. As you observed, <strong>entropy</strong> on its own is just a measure of a single probability distribution. As such, if we are trying to find a way to model a <strong>true probability distribution, P,</strong> using, say, a neural network to produce an <strong>approximate probability distribution Q</strong>, then there is the need for some sort of distance or difference measure which can be minimized.</p>
<p><strong>Cross entropy function</strong>: <span class="math display">\[
H(p,q)=H(p)+D_{KL}(p||q)
\]</span></p>
<ul>
<li>The first term, the <strong>entropy</strong> of the true probability distribution <em>p</em>, during optimization is <strong><em>fixed</em></strong> – it reduces to an additive constant during optimization.</li>
<li>Only the parameters of the second, approximation distribution, <em>q</em> that can be varied during optimization – and hence <em>the core of the cross entropy measure</em> of distance is the <strong>KL divergence</strong> function.</li>
</ul>
<p><strong>KL divergence</strong> is an expression of “surprise” – under the assumption that <em>P</em> and <em>Q</em> are <em>close,</em> it is <em>surprising</em> if it turns out that they are NOT CLOSE, hence in those cases the KL divergence will be high. If they are CLOSE together, then the KL divergence will be low.</p>
<p><strong>KL divergence</strong> is the i<em>nformation gained</em> when we move from a prior distribution <em>Q</em> to a posterior distribution <em>P</em>.</p>
<p><strong>Derivation of KL divergence</strong>:</p>
<p>The expression for KL divergence can also be derived by using a likelihood ratio approach.</p>
<p><strong>The likelihood ratio</strong> <span class="math display">\[
LR=\frac{p(x)}{q(x)}
\]</span></p>
<ul>
<li><strong>Interpretation</strong>: if a value x is sampled from some unknown distribution, the likelihood ratio expresses how much more likely the sample has come from distribution p than from distribution q. If it is more likely from p, the LR &gt; 1, otherwise if it is more likely from q, the LR &lt; 1.</li>
</ul>
<p>Let’s say we have lots of independent samples and we want to estimate the likelihood function taking into account all this evidence – it then becomes: <span class="math display">\[
LR=\prod_{i=1}^n\frac{p(x_i)}{q(x_i)}
\]</span> If we convert the ratio to <span class="math inline">\(log\)</span>, it’s possible to turn the product in the above definition to a summation: <span class="math display">\[
LR=\sum_{i=1}^n\log \frac{p(x_i)}{q(x_i)}
\]</span> So now we have the likelihood ratio as a <em>summation</em>. Let’s say we want to answer the question of how much, on average, each sample gives evidence of <span class="math inline">\(p(x)\)</span> over <span class="math inline">\(q(x)\)</span> . To do this, we can take the <em>expected value</em> of the likelihood ratio and arrive at: <span class="math display">\[
D_{KL}(P||Q)=\sum_{i=1}^n p(x_i)\log \frac{p(x_i)}{q(x_i)}
\]</span> The expression above is the definition of KL divergence. It is basically the expected value of the likelihood ratio – where the likelihood ratio expresses how much more likely the sampled data is from distribution P instead of distribution Q. Another way of expressing the above definition is as follows (using log rules): <span class="math display">\[
D_{KL}(P||Q)=\sum_{i=1}^n p(x_i)\log p(x_i)-\sum_{i=1}^n p(x_i)\log q(x_i)
\]</span></p>
<ul>
<li>The first term in the above equation is the <strong><em>entropy</em></strong> of the distribution P. As you can recall it is the expected value of the <strong><em>information content</em></strong> of P.</li>
<li>The second term is the information content of Q, but instead weighted by the distribution P.</li>
<li>This yields the <em>interpretation of the KL divergence</em> to be something like the following: if P is the “true” distribution, then the KL divergence is the amount of <strong>information “lost”</strong> when expressing it via Q.</li>
</ul>
<p><strong>Cross entropy</strong>: <span class="math display">\[
\begin{align}
H(p,q)&amp; =H(p)+D_{KL}(p||q) \\
&amp;= -\sum_{i=1}^nP(x_i)\log (P(x_i))+\sum_{i=1}^n p(x_i)\log p(x_i)-\sum_{i=1}^n p(x_i)\log q(x_i) \\
&amp;= -\sum_{i=1}^n p(x_i)\log q(x_i)
\end{align}
\]</span></p>
<p><strong>Ref:</strong></p>
<p><a href="https://github.com/Sroy20/machine-learning-interview-questions" target="_blank" rel="noopener">machine-learning-interview-questions</a></p>
<p><a href="https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23" target="_blank" rel="noopener">Common Loss functions in machine learning</a></p>
<p><a href="https://datascience.stackexchange.com/questions/35928/whats-the-difference-between-error-risk-and-loss" target="_blank" rel="noopener">What's the difference between Error, Risk and Loss?</a></p>
<p><a href="https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229" target="_blank" rel="noopener">Understanding the Bias-Variance Tradeoff</a></p>
<p><a href="https://adventuresinmachinelearning.com/cross-entropy-kl-divergence/" target="_blank" rel="noopener">An introduction to entropy, cross entropy and KL divergence in machine learning</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://nancyyanyu.github.io/undefined/a1139410/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Nancy Yan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Nancy's Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/undefined/a1139410/" class="post-title-link" itemprop="url">ISLR Note - SVM: Support Vector Machines</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-06-11 18:03:07" itemprop="dateCreated datePublished" datetime="2019-06-11T18:03:07-05:00">2019-06-11</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-06-14 23:27:11" itemprop="dateModified" datetime="2019-06-14T23:27:11-05:00">2019-06-14</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">5.1k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">5 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          <h1 id="svms-with-kernel">SVMs with Kernel</h1>
<p>The <strong><em>support vector machine (SVM)</em></strong> is an extension of the support vector classifier that results from enlarging the feature space using <strong>kernels</strong>.</p>
<p>The <strong><em>solution to the support vector classifier problem</em></strong> involves only the <strong><em>inner products</em></strong> of the observations: <span class="math display">\[
\langle x_i,x_{i^{&#39;}} \rangle =\sum_{j=1}^px_{ij}x_{i^{&#39;}j}
\]</span> (Details won't be discussed in this note)</p>
<p>The <strong>linear support vector classifier</strong> can be represented as <span class="math display">\[
f(x)=\beta_0+\sum_{i=1}^n \alpha_i \langle x,x_i \rangle
\]</span></p>
<ul>
<li><span class="math inline">\(α_i\)</span> is nonzero only for the support vectors in the solution—that is, if a training observation is not a support vector, then its <span class="math inline">\(α_i\)</span>equals zero.</li>
</ul>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/undefined/a1139410/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://nancyyanyu.github.io/undefined/e6eddd64/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Nancy Yan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Nancy's Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/undefined/e6eddd64/" class="post-title-link" itemprop="url">ISLR Note - SVM: Support Vector Classifiers</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-06-11 17:29:11 / Modified: 21:45:59" itemprop="dateCreated datePublished" datetime="2019-06-11T17:29:11-05:00">2019-06-11</time>
            

            
              

              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">2.9k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">3 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          <h1 id="overview-of-the-support-vector-classifier">Overview of the Support Vector Classifier</h1>
<p><strong>Maximal margin hyperplane</strong> is extremely sensitive to a change in a single observation suggests that it may have <strong><em>overfit</em></strong> the training data.</p>
<p>In this case, we might be willing to consider a classifier based on a hyperplane that does not perfectly separate the two classes, in the interest of</p>
<ul>
<li>Greater <em>robustness</em> to individual observations, and</li>
<li>Better classification of most of the training observations.</li>
</ul>
<p><strong><em>Support Vector Classifier (Soft Margin Classifier)</em></strong>: Rather than seeking the largest possible margin that every observation is not only on the correct side of the hyperplane but also on the correct side of the margin, we instead allow some observationsto be on the incorrect side of the margin, or even the incorrect side ofthe hyperplane.</p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/undefined/e6eddd64/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://nancyyanyu.github.io/undefined/bc53b72b/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Nancy Yan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Nancy's Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/undefined/bc53b72b/" class="post-title-link" itemprop="url">ISLR Note - SVM: Maximal Margin Classifier</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-06-11 16:48:50 / Modified: 22:01:10" itemprop="dateCreated datePublished" datetime="2019-06-11T16:48:50-05:00">2019-06-11</time>
            

            
              

              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">3k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">3 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          <h1 id="what-is-a-hyperplane">What Is a Hyperplane?</h1>
<p><strong><em>Hyperplane </em></strong>: In a p-dimensional space, a hyperplane is a flat affine subspace of dimension <span class="math inline">\(p − 1\)</span>.</p>
<ul>
<li>e.g. in two dimensions, a hyperplane is a flat one-dimensional subspace—in other words, a line.</li>
</ul>
<p><strong>Mathematical definition of a hyperplane</strong>: <span class="math display">\[
\beta_0+\beta_1X_1+\beta_2X_2,...+\beta_pX_p=0, \quad (9.1)
\]</span></p>
<ul>
<li>Any <span class="math inline">\(X = (X1,X2,…X_p)^T\)</span> for which (9.1) holds is a point on the hyperplane.</li>
</ul>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/undefined/bc53b72b/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://nancyyanyu.github.io/undefined/babf682e/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Nancy Yan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Nancy's Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/undefined/babf682e/" class="post-title-link" itemprop="url">离开纽约前的一些想法</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-06-11 13:17:57 / Modified: 12:19:31" itemprop="dateCreated datePublished" datetime="2019-06-11T13:17:57-05:00">2019-06-11</time>
            

            
              

              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Journal/" itemprop="url" rel="index"><span itemprop="name">Journal</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">1</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">1 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          
            

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://nancyyanyu.github.io/undefined/9c99c8b6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Nancy Yan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Nancy's Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/undefined/9c99c8b6/" class="post-title-link" itemprop="url">ISLR Note - Clustering</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-06-11 12:52:06 / Modified: 22:02:07" itemprop="dateCreated datePublished" datetime="2019-06-11T12:52:06-05:00">2019-06-11</time>
            

            
              

              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">6.8k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">6 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          <h1 id="clustering-methods">Clustering Methods</h1>
<p><strong>Clustering</strong> refers to a very broad set of techniques for finding subgroups, or clusters, in a data set.</p>
<ul>
<li><p>When we cluster the observations of a data set, we seek to partition them into distinct groups so that the observations within each group are quite similar to each other</p></li>
<li><p>This is an unsupervised problem because we are trying to discover structure</p></li>
</ul>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/undefined/9c99c8b6/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://nancyyanyu.github.io/undefined/9b1a6524/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Nancy Yan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Nancy's Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/undefined/9b1a6524/" class="post-title-link" itemprop="url">ISLR Note - PCA</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-06-09 00:40:07" itemprop="dateCreated datePublished" datetime="2019-06-09T00:40:07-05:00">2019-06-09</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-06-15 04:23:21" itemprop="dateModified" datetime="2019-06-15T04:23:21-05:00">2019-06-15</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">7.9k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">7 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          <h1 id="principal-components-analysis">Principal Components Analysis</h1>
<p><strong>Principal component analysis (PCA)</strong> refers to the process by which principal components are computed, and the subsequent use of these components in understanding the data. - PCA also serves as a tool for data visualization (visualization of the observations or visualization of the variables).</p>
<h2 id="what-are-principal-components">What Are Principal Components?</h2>
<p><strong>PCA</strong> :finds a low-dimensional representation of a data set that contains as much as possible of the <strong>variation</strong></p>
<p>Each of the dimensions found by PCA is a linear combination of the <span class="math inline">\(p\)</span> features.</p>
<p><strong><em>The first principal component</em></strong> of a set of features <span class="math inline">\(X_1,X_2, . . . , X_p\)</span> is the normalized linear combination of the features</p>
<p><span class="math display">\[
\begin{align}
Z_1=\phi_{11}X_1+\phi_{21}X_2+,,,+\phi_{p1}X_p
\end{align}
\]</span></p>
<p>that has the <strong>largest variance</strong>.</p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/undefined/9b1a6524/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://nancyyanyu.github.io/undefined/e9c631e9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Nancy Yan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Nancy's Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/undefined/e9c631e9/" class="post-title-link" itemprop="url">ISLR Note - Bagging,Random_Forest,Boosting</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-06-09 00:36:15" itemprop="dateCreated datePublished" datetime="2019-06-09T00:36:15-05:00">2019-06-09</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-06-15 04:18:37" itemprop="dateModified" datetime="2019-06-15T04:18:37-05:00">2019-06-15</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">7.1k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">6 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          <h1 id="bagging">Bagging</h1>
<p><strong>Bootstrap aggregation</strong>, or <strong>bagging</strong>, is a general-purpose procedure for reducing the variance of a statistical learning method, frequently used in the context of decision trees.</p>
<p><strong>Averaging a set of observations reduces variance</strong>: Recall that given a set of n independent observations Z1, . . . , Zn, each with variance <span class="math inline">\(σ^2\)</span>, the variance of the mean <span class="math inline">\(\bar{Z}\)</span> of the observations is given by <span class="math inline">\(σ^2/n\)</span>. - A natural way to reduce the variance and hence increase the prediction accuracy of a statistical learning method is to <strong>take many training sets from the population</strong>, build a separate prediction model using each training set, and average the resulting predictions.</p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/undefined/e9c631e9/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://nancyyanyu.github.io/undefined/6b588a86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Nancy Yan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Nancy's Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/undefined/6b588a86/" class="post-title-link" itemprop="url">ISLR Note - The Basics of Decision Trees</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-06-09 00:36:08" itemprop="dateCreated datePublished" datetime="2019-06-09T00:36:08-05:00">2019-06-09</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-06-15 04:26:35" itemprop="dateModified" datetime="2019-06-15T04:26:35-05:00">2019-06-15</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">9.3k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">8 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          <h1 id="regression-trees">Regression Trees</h1>
<h2 id="predicting-baseball-players-salaries-using-regression-trees">Predicting Baseball Players’ Salaries Using Regression Trees</h2>
<p><strong>Terminal nodes</strong>: The regions R1, R2, and R3 are known as terminal nodes or leaves of the tree.</p>
<p><strong>Internal nodes</strong>: The points along the tree where the predictor space is split are referred to as internal nodes.</p>
<p><strong>Branches</strong>: The segments of the trees that connect the nodes as branches</p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/undefined/6b588a86/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Nancy Yan</p>
              <div class="site-description motion-element" itemprop="description"></div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">31</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">2</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">23</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                  
                    
                  
                  <a href="https://github.com/nancyyanyu" title="GitHub &rarr; https://github.com/nancyyanyu" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i></a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                  
                    
                  
                  <a href="mailto:yy2799@columbia.edu" title="E-Mail &rarr; mailto:yy2799@columbia.edu" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i></a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                  
                    
                  
                  <a href="https://www.linkedin.com/in/nancy-yanyu-yan" title="LinkedIn &rarr; https://www.linkedin.com/in/nancy-yanyu-yan" rel="noopener" target="_blank"><i class="fa fa-fw fa-linkedin"></i></a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                  
                    
                  
                  <a href="https://instagram.com/nancy_yanyu_yan" title="Instagram &rarr; https://instagram.com/nancy_yanyu_yan" rel="noopener" target="_blank"><i class="fa fa-fw fa-instagram"></i></a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Nancy Yan</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
    <span title="Symbols count total">189k</span>
  

  
</div>









        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>





  



  






  



  
    
    
      
    
  
  <script color="0,0,255" opacity="0.5" zindex="-1" count="99" src="//cdn.bootcss.com/canvas-nest.js/1.0.1/canvas-nest.min.js"></script>



  
  



  
  



  
  



  
  
  <script id="ribbon" size="300" alpha="0.6" zindex="-1" src="/lib/canvas-ribbon/canvas-ribbon.js"></script>





  
  <script src="//cdn.jsdelivr.net/jquery/2.1.3/jquery.min.js"></script>

  
  <script src="//cdn.jsdelivr.net/fastclick/1.0.6/fastclick.min.js"></script>

  
  <script src="//cdn.jsdelivr.net/npm/jquery-lazyload@1/jquery.lazyload.min.js"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script src="/lib/fancybox/source/jquery.fancybox.pack.js"></script>

  
  <script src="/lib/three/three.min.js"></script>

  
  <script src="/lib/three/three-waves.min.js"></script>

  
  <script src="/lib/three/canvas_lines.min.js"></script>

  
  <script src="/lib/three/canvas_sphere.min.js"></script>


  


  <script src="/js/utils.js?v=7.1.2"></script>

  <script src="/js/motion.js?v=7.1.2"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.1.2"></script>




  

  


  <script src="/js/next-boot.js?v=7.1.2"></script>


  

  

  

  





  




  

  

  
  

  
  
    
      
        
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

    
  


  

  

  

  

  

  

  

  

  

  

  

</body>
</html>

<script type="text/javascript" src="/js/src/dynamic_bg.js"></script>

