<!DOCTYPE html><html class="theme-next mist use-motion"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><script></script><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><link rel="stylesheet" href="/lib/Han/dist/han.min.css?v=3.3"><link rel="stylesheet" href="/lib/fancybox/source/jquery.fancybox.css"><link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"><link rel="stylesheet" href="/css/main.css?v=7.1.2"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.2"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.1.2"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.1.2"><link rel="mask-icon" href="/images/logo.svg?v=7.1.2" color="#222"><script id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",version:"7.1.2",sidebar:{position:"right",display:"hide",offset:12,onmobile:!1,dimmer:!1},back2top:!0,back2top_sidebar:!1,fancybox:!0,fastclick:!0,lazyload:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><meta name="description" content="Linear Regression Models and Least SquaresLinear Regression ModelForm of the linear regression model: \(f(X)=\beta_{0}+\sum_{j=1}^{p}X_{j}\beta_{j}\).Training data: (x1,y1) ... (xN,yN). Each \(x_{i} ="><meta property="og:type" content="article"><meta property="og:title" content="ISLR Note - Comparison of Classification Methods"><meta property="og:url" content="https://nancyyanyu.github.io/undefined/9b5af8e1/index.html"><meta property="og:site_name" content="Nancy&#39;s Notes"><meta property="og:description" content="Linear Regression Models and Least SquaresLinear Regression ModelForm of the linear regression model: \(f(X)=\beta_{0}+\sum_{j=1}^{p}X_{j}\beta_{j}\).Training data: (x1,y1) ... (xN,yN). Each \(x_{i} ="><meta property="og:locale" content="default"><meta property="og:updated_time" content="2019-06-09T19:22:31.513Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="ISLR Note - Comparison of Classification Methods"><meta name="twitter:description" content="Linear Regression Models and Least SquaresLinear Regression ModelForm of the linear regression model: \(f(X)=\beta_{0}+\sum_{j=1}^{p}X_{j}\beta_{j}\).Training data: (x1,y1) ... (xN,yN). Each \(x_{i} ="><link rel="alternate" href="/atom.xml" title="Nancy's Notes" type="application/atom+xml"><link rel="canonical" href="https://nancyyanyu.github.io/undefined/9b5af8e1/"><script id="page.configurations">CONFIG.page={sidebar:""}</script><title>ISLR Note - Comparison of Classification Methods | Nancy's Notes</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-title,.use-motion .comments,.use-motion .menu-item,.use-motion .motion-element,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage" lang="default"><div class="container sidebar-position-right page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Nancy's Notes</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Code changes world!</p></div><div class="site-nav-toggle"><button aria-label="Toggle navigation bar"><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-resumé"><a href="/resume/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>Resumé</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-journal"><a href="/journal/" rel="section"><i class="menu-item-icon fa fa-fw fa-coffee"></i><br>Journal</a></li><li class="menu-item menu-item-手帳"><a href="/techou/" rel="section"><i class="menu-item-icon fa fa-fw fa-heart"></i><br>手帳</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://nancyyanyu.github.io/undefined/9b5af8e1/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Nancy Yan"><meta itemprop="description" content><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Nancy's Notes"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">ISLR Note - Comparison of Classification Methods</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2019-06-08 23:11:26" itemprop="dateCreated datePublished" datetime="2019-06-08T23:11:26-05:00">2019-06-08</time> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">Edited on</span> <time title="Modified: 2019-06-09 14:22:31" itemprop="dateModified" datetime="2019-06-09T14:22:31-05:00">2019-06-09</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">Symbols count in article: </span><span title="Symbols count in article">14k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">Reading time &asymp;</span> <span title="Reading time">12 mins.</span></div></div></header><div class="post-body han-init-context" itemprop="articleBody"><h1 id="linear-regression-models-and-least-squares">Linear Regression Models and Least Squares</h1><h3 id="linear-regression-model">Linear Regression Model</h3><ul><li><p>Form of the linear regression model: <em><span class="math inline">\(f(X)=\beta_{0}+\sum_{j=1}^{p}X_{j}\beta_{j}\)</span></em>.</p></li><li><p>Training data: (x1,y1) ... (xN,yN). Each <span class="math inline">\(x_{i} =(x_{i1},x_{i2},...,x_{ip})^{T}\)</span> is a vector of feature measurements for the ith case.</p></li><li><p>Goal: estimate the parameters β</p></li><li><p>Estimation method: <strong>Least Squares</strong>, we pick the coeﬃcients <span class="math inline">\(β =(β0,β1,...,βp)^{T}\)</span> to minimize the <strong>residual sum of squares</strong></p></li></ul><p><strong>Assumptions:</strong></p><ul><li>Observations <span class="math inline">\(y_i\)</span> are uncorrelated and have constant variance <span class="math inline">\(\sigma^2\)</span>;</li><li><span class="math inline">\(x_i\)</span> are ﬁxed (non random)</li><li>The regression function E(Y |X) is linear, or the linear model is a reasonable approximation.</li></ul><a id="more"></a><h3 id="residual-sum-of-squares">Residual Sum of Squares</h3><p><span class="math display">\[\begin{align} RSS(\beta)=\sum_{i=1}^{N}(y_{i}-f(x_{i}))^2=\sum_{i=1}^{N}(y_{i}-\beta_{0}-\sum_{j=1}^{p}X_{ij}\beta_{j})^2 \end{align}\]</span></p><h4 id="solution">Solution</h4><p>Denote by X the N × (p + 1) matrix with each row an input vector (with a 1 in the ﬁrst position), and similarly let y be the N-vector of outputs in the training set.</p><p><span class="math display">\[\begin{align} \min RSS(\beta)= (y-\mathbf{X}\beta)^T(y-\mathbf{X}\beta) \end{align}\]</span></p><p>A quadratic function in the p + 1 parameters</p><p>Taking derivatives:</p><p><span class="math display">\[\begin{align} \frac{\partial RSS}{\partial \beta}=-2\mathbf{X}^T(y-\mathbf{X}\beta) \end{align}\]</span></p><p><span class="math display">\[\begin{align} \frac{\partial^2 RSS}{\partial \beta \partial \beta^T}=2\mathbf{X}^T\mathbf{X} \end{align}\]</span></p><p>Assuming (for the moment) that <span class="math inline">\(\mathbf{X}\)</span> has full column rank, and hence <span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> is positive deﬁnite, we set the ﬁrst derivative to zero: <span class="math inline">\(\mathbf{X}^T(y-\mathbf{X}\beta)=0\)</span></p><p><span class="math display">\[\begin{align}\Rightarrow \hat{\beta}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^Ty \end{align}\]</span></p><p>Fitted values at the training inputs: <span class="math inline">\(\hat{y}=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^Ty\)</span></p><p>Hat matrix: <span class="math inline">\(H=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\)</span></p><h3 id="sampling-properties-of-beta">Sampling Properties of <span class="math inline">\(\beta\)</span></h3><p>The variance–covariance matrix of the least squares parameter estimates: <span class="math display">\[\begin{align} Var(\hat{\beta})=(\mathbf{X}^T\mathbf{X})^{-1}\sigma^2) \end{align}\]</span></p><p>Unbiased estimate of <span class="math inline">\(\sigma^2\)</span>: <span class="math display">\[\begin{align} \hat{\sigma^2}=\frac{1}{N-p-1}\sum^{N}_{i=1}(y_i-\hat{y_i})^2 \end{align}\]</span></p><p>Assume the deviations of <span class="math inline">\(\mathbf{Y}\)</span> around its expectation are additive and Gaussian: <span class="math display">\[\begin{align} Y=E(Y|X_1,...,X_p)+\epsilon=\beta_0+\sum_{j=1}^{p}X_j\beta_j+\epsilon \end{align}\]</span></p><p>where <span class="math inline">\(\epsilon \sim N(0,\sigma^2)\)</span></p><p>Thus, <span class="math inline">\(\beta\)</span> follows multivariate normal distribution with mean vector and variance–covariance matrix: <span class="math display">\[\begin{align}\hat{\beta} \sim N(\beta,(\mathbf{X}^T\mathbf{X})^{-1}\sigma^2 ) \end{align}\]</span></p><p>Also, a chi-squared distribution with N −p−1 degrees of freedom: <span class="math display">\[\begin{align} (N-p-1)\hat{\sigma^2} \sim \sigma^2 \chi_{N-p-1}^{2} \end{align}\]</span></p><p>(<span class="math inline">\(\hat{\beta}\)</span> and <span class="math inline">\(\hat{\sigma^2}\)</span> are indep.)</p><p>We use these distributional properties to form tests of hypothesis and conﬁdence intervals for the parameters <span class="math inline">\(\beta_j\)</span>:</p><h3 id="inference">Inference</h3><h4 id="test-hypothesis">Test hypothesis</h4><p>To test the hypothesis that a particular coeﬃcient <span class="math inline">\(\beta_j= 0\)</span>, we form thestandardized coeﬃcient or Z-score: <span class="math display">\[\begin{align} z_j=\frac{\hat{\beta_j}}{\hat{\sigma}\sqrt{\upsilon_j}} \end{align}\]</span></p><p>where <span class="math inline">\(\upsilon_j\)</span> is the jth diagonal element of <span class="math inline">\((\mathbf{X}^T\mathbf{X})^{-1}\)</span></p><p>Under the null hypothesis that <span class="math inline">\(\beta_j= 0\)</span>, <span class="math inline">\(z_j\)</span> is distributed as <span class="math inline">\(t_{N-p-1}\)</span>, and hence a large (absolute) value of zj will lead to rejection of this null hypothesis. If <span class="math inline">\(\hat{\sigma}\)</span> is replaced by a known value σ, then <span class="math inline">\(z_j\)</span> would have a standard normal distribution.</p><h4 id="test-the-signiﬁcance-of-groups-of-coeﬃcients">Test the signiﬁcance of groups of coeﬃcients</h4><p>To test if a categorical variable with k levels can be excluded from a model, we need to test whether the coeﬃcients of the dummy variables used to represent the levels can all be set to zero. Here we use the F statistic：</p><p><span class="math display">\[\begin{align} F=\frac{(RSS_0-RSS_1)/(p_1-p_0)}{RSS_1/(N-p_1-1)} \end{align}\]</span></p><ul><li>RSS1 is the residual sum-of-squares for the least squares ﬁt of the bigger model with p1+1 parameters;</li><li>RSS0 the same for the nested smaller model with p0 +1 parameters, having p1 −p0 parameters constrained to be zero.</li></ul><p>The F statistic measures the change in residual sum-of-squares per additional parameter in the bigger model, and it is normalized by an estimate of <span class="math inline">\(\sigma^2\)</span>.</p><p>Under the Gaussian assumptions, and the null hypothesis that the smaller model is correct, the F statistic will have a <span class="math inline">\(F_{p_1-p_0,N-p_1-1}\)</span> distribution.</p><h3 id="confidence-interval">Confidence Interval</h3><p><span class="math inline">\(1-2\alpha\)</span> conﬁdence interval for <span class="math inline">\(\beta_j\)</span>: <span class="math display">\[\begin{align} (\hat{\beta_j}-z^{(1-\alpha)}\upsilon^{0.5}_j \hat{\sigma},\hat{\beta_j}+z^{(1-\alpha)}\upsilon^{0.5}_j \hat{\sigma}) \end{align}\]</span> where <span class="math inline">\(z^{(1-\alpha)}\)</span> is the 1 − α percentile of the normal distribution. <span class="math inline">\(\alpha\)</span> could be 0.025, 0.5, etc.</p><p>In a similar fashion we can obtain an approximate confidence set for the entire parameter vector <span class="math inline">\(\beta\)</span>, namely</p><p><span class="math display">\[\begin{equation} C_\beta = \left\{ \beta \big| (\hat\beta-\beta)^T\mathbf{X}^T\mathbf{X}(\hat\beta-\beta) \le \hat\sigma^2{\chi^2_{p+1}}^{(1-\alpha)}\right\}, \end{equation}\]</span></p><p>This condifence set for <span class="math inline">\(\beta\)</span> generates a corresponding confidence set for the true function <span class="math inline">\(f(x) = x^T\beta\)</span>: <span class="math inline">\(\left\{ x^T\beta \big| \beta \in C_\beta \right\}\)</span></p><h2 id="example-prostate-cancer">3.2.1 Example: Prostate Cancer</h2><p>See other notebook</p><h2 id="the-gaussmarkov-theorem">3.2.2 The Gauss–Markov Theorem</h2><p>We focus on estimation of any linear combination of the parameters <span class="math inline">\(\theta=\alpha^T\beta\)</span>, for example, predictions <span class="math inline">\(f(x_0)=x^T_0\beta\)</span> are of this form.The least squares estimate of <span class="math inline">\(\alpha^T\beta\)</span> is: <span class="math display">\[\begin{equation} \hat{\theta}=\alpha^T\hat{\beta}=\alpha^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y} \end{equation}\]</span></p><p>Considering <span class="math inline">\(\mathbf{X}\)</span> to be ﬁxed, this is a linear function <span class="math inline">\(\mathbf{c}^T_0\mathbf{y}\)</span> of the response vector <span class="math inline">\(\mathbf{y}\)</span>.If we assume that the linear model is correct, <span class="math inline">\(\alpha^T\hat{\beta}\)</span> is unbiased.</p><p>The Gauss–Markov theorem states that if we have any other linear estimator <span class="math inline">\(\tilde{\theta}=\mathbf{c}^T\mathbf{y}\)</span> that is unbiased for <span class="math inline">\(\alpha^T\beta\)</span>, that is, <span class="math inline">\(E(\mathbf{c}^T\mathbf{y})= \alpha^T\beta\)</span>, then: <span class="math display">\[\begin{equation} Var(\alpha^T\hat{\beta})\leq Var(\mathbf{c}^T\mathbf{y}) \end{equation}\]</span></p><h3 id="variance-bias">Variance &amp; Bias</h3><p>Consider the mean squared error of an estimator <span class="math inline">\(\tilde{\theta}\)</span> in estimating <span class="math inline">\(\theta\)</span>:</p><p><span class="math display">\[\begin{align} MSE(\tilde{\theta})&amp;= E(\tilde{\theta}-\theta)^2 \\ &amp;= E(\tilde{\theta^2}+\theta^2-2\theta\tilde{\theta}) \\ &amp;= E(\tilde{\theta^2})-E^2(\tilde{\theta})+E^2(\tilde{\theta})+E(\theta^2-2\theta\tilde{\theta})\\ &amp;= Var(\tilde{\theta})+[E(\tilde{\theta})-\theta]^2 \end{align}\]</span></p><p>The ﬁrst term is the <strong>variance</strong>, while the second term is the <strong>squared bias</strong>.</p><p>The <strong>Gauss-Markov theorem</strong> implies that the <em>least squares estimator</em> has the smallest mean squared error of all linear estimators with no bias. However, there may well exist a biased estimator with smaller mean squared error. <font color="red">Such an estimator would trade a little bias for a larger reduction in variance.</font></p><p>From a more pragmatic point of view, most models are distortions of the truth, and hence are biased; picking the right model amounts to creating the right balance between bias and variance.</p><p>Mean squared error is intimately related to prediction accuracy. Consider the prediction of the new response at input <span class="math inline">\(x_0\)</span>, <span class="math display">\[\begin{equation} y_0=f(x_0)+\epsilon_0 \end{equation}\]</span></p><h3 id="prediction-error-mse">Prediction error &amp; MSE</h3><p>The expected prediction error of an estimate <span class="math inline">\(\tilde{f}(x_0)=x_0^T\tilde{\beta}\)</span>:</p><p><span class="math display">\[\begin{align} E(y_0-\tilde{f}(x_0))^2 &amp;=E(f(x_0)+\epsilon_0-x_0^T\tilde{\beta})^2 \\ &amp;=E(\epsilon_0^2)+E(f(x_0)-x_0^T\tilde{\beta})^2-2E(\epsilon_0(f(x_0)-x_0^T\tilde{\beta})) \\ &amp;=\sigma^2+E(f(x_0)-x_0^T\tilde{\beta})^2 \\ &amp;=\sigma^2+MSE(\tilde{f}(x_0)) \end{align}\]</span></p><p>Therefore, expected prediction error and mean squared error diﬀer only by the constant <span class="math inline">\(\sigma^2\)</span>, representing the variance of the new observation <span class="math inline">\(y_0\)</span>.</p><h2 id="multiple-regression-from-simple-univariate-regression">Multiple Regression from Simple Univariate Regression</h2><p>Gram-Schmidt正交化</p><h3 id="simple-univariate-regression">Simple Univariate Regression</h3><p>Suppose ﬁrst that we have a univariate model with no intercept: <span class="math display">\[\begin{align} \mathbf{Y}=\mathbf{X}\beta+\epsilon \end{align}\]</span> The least squares estimate and residuals are: <span class="math display">\[\begin{align} \hat{\beta}&amp;=\frac{\sum_1^Nx_iy_i}{\sum_1^Nx_i^2} \\ r_i&amp;=y_i-x_i\hat{\beta} \end{align}\]</span></p><p>Let <span class="math inline">\(\mathbf{y}=(y_1,...,y_N)^T\)</span>,<span class="math inline">\(\mathbf{x}=(x_1,...,x_N)^T\)</span></p><p>Define the <strong>inner product</strong> between <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>: <span class="math display">\[\begin{align} &lt;\mathbf{x},\mathbf{y}&gt;=\sum_1^Nx_iy_i=\mathbf{x}^T\mathbf{y} \end{align}\]</span></p><p>Then, <span class="math display">\[\begin{align} \hat{\beta}&amp;=\frac{&lt;\mathbf{x},\mathbf{y}&gt;}{&lt;\mathbf{x},\mathbf{x}&gt;} \\ \mathbf{r}&amp;=\mathbf{y}-\mathbf{x}\hat{\beta} \end{align}\]</span></p><p>Suppose the inputs x1, x2,..., xp (the columns of the data matrix <span class="math inline">\(\mathbf{X}\)</span>) are orthogonal; that is <span class="math inline">\(&lt;\mathbf{x_k},\mathbf{x_j}&gt;=0\)</span>. Then the multiple least squares estimates <span class="math inline">\(\hat{\beta_j}\)</span> are equal to <span class="math inline">\(\frac{&lt;\mathbf{x_j},\mathbf{y}&gt;}{&lt;\mathbf{x_j},\mathbf{x_j}&gt;}\)</span>—the univariate estimates. In other words, <font color="red">when the inputs are orthogonal, they have no eﬀect on each other’s parameter estimates in the model.</font></p><h3 id="orthogonalization">Orthogonalization</h3><p><span class="math display">\[\begin{align} \hat{\beta}_1&amp;=\frac{&lt;\mathbf{x}-\bar{x}\mathbf{1},\mathbf{y}&gt;}{&lt;\mathbf{x}-\bar{x}\mathbf{1},\mathbf{x}-\bar{x}\mathbf{1}&gt;} \\ \end{align}\]</span></p><ul><li><span class="math inline">\(\bar{x}=\sum_ix_i/N\)</span>;</li><li><span class="math inline">\(\mathbf{1}\)</span>, the vector of N ones;</li></ul><p><strong>Steps:</strong> 1. regress <span class="math inline">\(\mathbf{x}\)</span> on <span class="math inline">\(\mathbf{1}\)</span> to produce the residual <span class="math inline">\(\mathbf{z}=\mathbf{x}-\bar{x}\mathbf{1}\)</span>; 2. regress <span class="math inline">\(\mathbf{y}\)</span> on the residual <span class="math inline">\(\mathbf{z}\)</span> to give the coeﬃcient <span class="math inline">\(\hat{\beta}_1\)</span></p><blockquote><p>regress <span class="math inline">\(\mathbf{a}\)</span> on <span class="math inline">\(\mathbf{b}\)</span>: (<span class="math inline">\(\mathbf{b}\)</span> is adjusted for <span class="math inline">\(\mathbf{a}\)</span>),(or <span class="math inline">\(\mathbf{b}\)</span> is <strong>“orthogonalized”</strong> with respect to <span class="math inline">\(\mathbf{a}\)</span>); a simple univariate regression of <span class="math inline">\(\mathbf{b}\)</span> on a with no intercept, producing coeﬃcient <span class="math inline">\(\hat{\lambda}=\frac{&lt;\mathbf{a},\mathbf{b}&gt;}{&lt;\mathbf{a},\mathbf{a}&gt;}\)</span> and residual vector $ - $.</p></blockquote><p>The orthogonalization does not change the subspace spanned by x1 and x2, it simply produces an <strong>orthogonal basis</strong> for representing it.</p><h3 id="gramschmidt-procedure-for-multiple-regression">Gram–Schmidt procedure for multiple regression</h3><p><strong>ALGORITHM 3.1 Regression by Successive Orthogonalization</strong></p><ol type="1"><li>Initialize <span class="math inline">\(\mathbf{z_0}=\mathbf{x_0}=\mathbf{1}\)</span>.</li><li>For j=1,2,...,1,,...,p,<br>Regress <span class="math inline">\(\mathbf{x_j}\)</span> on <span class="math inline">\(\mathbf{z_0},\mathbf{z_1},...,\mathbf{z_{j-1}}\)</span> to produce coeﬃcients <span class="math inline">\(\hat{\lambda}_{l,j}=\frac{&lt;\mathbf{z_l},\mathbf{x_j}&gt;}{&lt;\mathbf{z_l},\mathbf{z_l}&gt;}\)</span>, l=0,1,...,j-1, and residual vector <span class="math inline">\(\mathbf{z_j}=\mathbf{x_j}-\sum_{k=0}^{j-1}\hat{\lambda_{kj}}\mathbf{z_k}\)</span></li><li>Regress <span class="math inline">\(\mathbf{y}\)</span> on the residual <span class="math inline">\(\mathbf{z_p}\)</span> to give the estimate <span class="math inline">\(\hat{\beta_p}=\frac{&lt;\mathbf{z_p},\mathbf{y}&gt;}{&lt;\mathbf{z_p},\mathbf{z_p}&gt;}\)</span>.</li></ol><p>Note: - Each of the <span class="math inline">\(\mathbf{x}_j\)</span> is a linear combination of the <span class="math inline">\(\mathbf{z}_k\)</span>,k ≤ j. - Since th e<span class="math inline">\(\mathbf{z}_j\)</span> are all orthogonal, they form a basis for the column space of <span class="math inline">\(\mathbf{X}\)</span>, and hence the least squares projection onto this subspace is <span class="math inline">\(\mathbf{\hat{y}}\)</span>. - By rearranging the xj , any one of them could be in the last position, and a similar results holds. - The multiple regression coeﬃcient <span class="math inline">\(\mathbf{x}_j\)</span> represents the additional contribution of <span class="math inline">\(\mathbf{x}_j\)</span> on <span class="math inline">\(\mathbf{y}\)</span>, after <span class="math inline">\(\mathbf{x}_j\)</span> has been adjusted for x0, x1,..., xj−1,xj+1,..., xp.</p><h3 id="precision-of-coefficient-estimation">Precision of Coefficient Estimation</h3><p>If <span class="math inline">\(\mathbf{x}_p\)</span> is highly correlated with some of the other <span class="math inline">\(\mathbf{x}_k\)</span>’s, the residual vector <span class="math inline">\(\mathbf{z}_p\)</span> will be close to zero, and the coeﬃcient <span class="math inline">\(\mathbf{x}_j\)</span> will be very unstable.</p><p>From <span class="math inline">\(\hat{\beta_p}=\frac{&lt;\mathbf{z_p},\mathbf{y}&gt;}{&lt;\mathbf{z_p},\mathbf{z_p}&gt;}\)</span>, we also obtain an alternate formula for the variance estimates:</p><p><span class="math display">\[\begin{align}Var(\hat{\beta}_p)=\frac{\sigma^2}{&lt;\mathbf{z_p},\mathbf{z_p}&gt;}=\frac{\sigma^2}{||\mathbf{z_p}||^2} \end{align}\]</span></p><p>The precision with which we can estimate <span class="math inline">\(\hat{\beta_p}\)</span> depends on the length of the residual vector <span class="math inline">\(\mathbf{z_p}\)</span>; this represents how much of <span class="math inline">\(\mathbf{x_p}\)</span> is unexplained by the other <span class="math inline">\(\mathbf{x_k}\)</span>’s</p><h3 id="qr-decomposition">QR decomposition</h3><p>We can represent step 2 of Algorithm 3.1 in matrix form:</p><p><span class="math display">\[\begin{align} \mathbf{X}=\mathbf{Z}\mathbf{Γ} \end{align}\]</span></p><ul><li><span class="math inline">\(\mathbf{Z}\)</span> has as columns the <span class="math inline">\(\mathbf{z_j}\)</span> (in order)</li><li><span class="math inline">\(\mathbf{Γ}\)</span> is the upper triangular matrix with entries <span class="math inline">\(\hat{\lambda}_{kj}\)</span></li></ul><p>Introducing the diagonal matrix D with jth diagonal entry <span class="math inline">\(D_{jj} = ||\mathbf{z_j}||\)</span>, we get</p><p><strong>QR decomposition of X</strong>:</p><p><span class="math display">\[\begin{align} \mathbf{X}=\mathbf{Z}\mathbf{D}^{-1}\mathbf{D}\mathbf{Γ}=\mathbf{Q}\mathbf{R} \end{align}\]</span></p><ul><li>Q is an N ×(p+1) orthogonal matrix, <span class="math inline">\(Q^TQ=I\)</span>;</li><li>R is a (p +1) × (p + 1) upper triangular matrix.</li></ul><p>least squares solution:</p><p><span class="math display">\[\begin{align} \hat{\beta}&amp;=R^{-1}Q^T\mathbf{y} \\ \mathbf{\hat{y}}&amp;=QQ^T\mathbf{y} \end{align}\]</span></p><h2 id="multiple-outputs">3.2.4 Multiple Outputs</h2><p>Suppose we have multiple outputs Y1,Y2,...,YK that we wish to predict from our inputs X0,X1,X2,...,Xp. We assume a linear model for each output:</p><p><span class="math display">\[\begin{align} Y_k&amp;=\beta_{0k}+\sum_{j=1}^pX_j\beta_{jk}+\epsilon_k \\ &amp;=f_k(X)+\epsilon_k \end{align}\]</span></p><p>With N training cases we can write the model in matrix notation:</p><p><span class="math display">\[\begin{align} Y=XB+E \end{align}\]</span></p><ul><li>Y: N×K response matrix</li><li>X: N×(p+1) input matrix</li><li>B: (p+1)× K matrix of parameters</li><li>E: N×K matrix of errors</li></ul><p>A straightforward generalization of the univariate loss function:</p><p><span class="math display">\[\begin{align} RSS(B)&amp;=\sum_{k=1}^K\sum_{i=1}^N(y_{ik}-f_k(x_i))^2 \\ &amp;=tr[(Y-XB)^T(Y-XB)] \end{align}\]</span></p><p>The least squares estimates have exactly the same form as before: <span class="math display">\[\begin{align} \hat{B}=(X^TX)^{-1}X^Ty \end{align}\]</span></p><p>If the errors <span class="math inline">\(\epsilon =(\epsilon_1,...,\epsilon_K)\)</span> in are correlated, suppose <span class="math inline">\(Cov(\epsilon)= Σ\)</span>, then the multivariate weighted criterion: <span class="math display">\[\begin{align} RSS(B;Σ)&amp;=\sum_{i=1}^N(y_{ik}-f_k(x_i))^TΣ^{-1}(y_{ik}-f_k(x_i)) \end{align}\]</span></p></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/undefined/4df00c7b/" rel="next" title="ISLR Note - Potential Problems"><i class="fa fa-chevron-left"></i> ISLR Note - Potential Problems</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/undefined/2ce36d51/" rel="prev" title="ESL Note - Linear Regression Example Prostate Cancer">ESL Note - Linear Regression Example Prostate Cancer <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">Table of Contents</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">Overview</li></ul><div class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">Nancy Yan</p><div class="site-description motion-element" itemprop="description"></div></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">24</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">1</span> <span class="site-state-item-name">categories</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/nancyyanyu" title="GitHub &rarr; https://github.com/nancyyanyu" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i></a> </span><span class="links-of-author-item"><a href="mailto:yy2799@columbia.edu" title="E-Mail &rarr; mailto:yy2799@columbia.edu" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i></a> </span><span class="links-of-author-item"><a href="https://www.linkedin.com/in/nancy-yanyu-yan" title="LinkedIn &rarr; https://www.linkedin.com/in/nancy-yanyu-yan" rel="noopener" target="_blank"><i class="fa fa-fw fa-linkedin"></i></a> </span><span class="links-of-author-item"><a href="https://weibo.com/u/1780432094" title="Weibo &rarr; https://weibo.com/u/1780432094" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i></a></span></div></div></div><div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#linear-regression-models-and-least-squares"><span class="nav-number">1.</span> <span class="nav-text">Linear Regression Models and Least Squares</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#linear-regression-model"><span class="nav-number">1.0.1.</span> <span class="nav-text">Linear Regression Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#residual-sum-of-squares"><span class="nav-number">1.0.2.</span> <span class="nav-text">Residual Sum of Squares</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#solution"><span class="nav-number">1.0.2.1.</span> <span class="nav-text">Solution</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sampling-properties-of-beta"><span class="nav-number">1.0.3.</span> <span class="nav-text">Sampling Properties of \(\beta\)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#inference"><span class="nav-number">1.0.4.</span> <span class="nav-text">Inference</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#test-hypothesis"><span class="nav-number">1.0.4.1.</span> <span class="nav-text">Test hypothesis</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#test-the-signiﬁcance-of-groups-of-coeﬃcients"><span class="nav-number">1.0.4.2.</span> <span class="nav-text">Test the signiﬁcance of groups of coeﬃcients</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#confidence-interval"><span class="nav-number">1.0.5.</span> <span class="nav-text">Confidence Interval</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#example-prostate-cancer"><span class="nav-number">1.1.</span> <span class="nav-text">3.2.1 Example: Prostate Cancer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#the-gaussmarkov-theorem"><span class="nav-number">1.2.</span> <span class="nav-text">3.2.2 The Gauss–Markov Theorem</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#variance-bias"><span class="nav-number">1.2.1.</span> <span class="nav-text">Variance &amp; Bias</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#prediction-error-mse"><span class="nav-number">1.2.2.</span> <span class="nav-text">Prediction error &amp; MSE</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#multiple-regression-from-simple-univariate-regression"><span class="nav-number">1.3.</span> <span class="nav-text">Multiple Regression from Simple Univariate Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#simple-univariate-regression"><span class="nav-number">1.3.1.</span> <span class="nav-text">Simple Univariate Regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#orthogonalization"><span class="nav-number">1.3.2.</span> <span class="nav-text">Orthogonalization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gramschmidt-procedure-for-multiple-regression"><span class="nav-number">1.3.3.</span> <span class="nav-text">Gram–Schmidt procedure for multiple regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#precision-of-coefficient-estimation"><span class="nav-number">1.3.4.</span> <span class="nav-text">Precision of Coefficient Estimation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#qr-decomposition"><span class="nav-number">1.3.5.</span> <span class="nav-text">QR decomposition</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#multiple-outputs"><span class="nav-number">1.4.</span> <span class="nav-text">3.2.4 Multiple Outputs</span></a></li></ol></div></div></div></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2019</span> <span class="with-love" id="animate"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">Nancy Yan</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span title="Symbols count total">143k</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script>"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script color="0,0,255" opacity="0.5" zindex="-1" count="99" src="//cdn.bootcss.com/canvas-nest.js/1.0.1/canvas-nest.min.js"></script><script id="ribbon" size="300" alpha="0.6" zindex="-1" src="/lib/canvas-ribbon/canvas-ribbon.js"></script><script src="//cdn.jsdelivr.net/jquery/2.1.3/jquery.min.js"></script><script src="//cdn.jsdelivr.net/fastclick/1.0.6/fastclick.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery-lazyload@1/jquery.lazyload.min.js"></script><script src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script src="/lib/fancybox/source/jquery.fancybox.pack.js"></script><script src="/lib/three/three.min.js"></script><script src="/lib/three/three-waves.min.js"></script><script src="/lib/three/canvas_lines.min.js"></script><script src="/lib/three/canvas_sphere.min.js"></script><script src="/js/utils.js?v=7.1.2"></script><script src="/js/motion.js?v=7.1.2"></script><script src="/js/schemes/muse.js?v=7.1.2"></script><script src="/js/scrollspy.js?v=7.1.2"></script><script src="/js/post-details.js?v=7.1.2"></script><script src="/js/next-boot.js?v=7.1.2"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });</script><script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html><script type="text/javascript" src="/js/src/dynamic_bg.js"></script>